{"/SridharBlog/posts/add-new-blog/":{"data":{"blog-post-creation-workflow#Blog Post Creation Workflow":"Blog Post Creation WorkflowThis document outlines the process for creating new blog posts. It will be updated regularly as new techniques and workflow improvements are implemented.","prerequisites#Prerequisites":"Git Bash: Used as the primary shell. Git Extensions: Preferred tool for managing Git operations. Hugo: Static site generator.","procedure#Procedure":"Navigate to the Repository:\nOpen Git Bash. Navigate to the blog’s repository directory: ~/source/Blog/static-app. Create a Branch:\nUsing Git Extensions, create a new branch for the blog post. This isolates changes and facilitates easier review. Create a New Blog Post:\nExecute the following command to create a new Markdown file for the blog post:\nhugo new posts/{blog-post-file-name}.md This command creates an empty blog post file in the C:\\Users\\sridh\\source\\Blog\\static-app\\content\\posts\\ directory. Replace {blog-post-file-name} with the desired file name for the blog post. Edit the Blog Post:\nOpen the newly created Markdown file and add the content of your blog post. Test the Draft:\nPreview the blog post in draft mode using the following command:\nhugo server -D The -D flag enables draft content. Review the changes in your local browser.\nPublish the Blog Post:\nOnce satisfied with the draft, open the Markdown file and change the draft status: Replace draft = true with draft = false. Final Test:\nTest the published version of the blog post using the following command:\nhugo server Review the changes in your local browser.\nCommit and Deploy:\nUsing Git Extensions, commit the changes to the branch. Push the branch to the remote repository. Merge the branch to main branch. The changes will be automatically deployed to the Azure static website."},"title":"Add New Blog"},"/SridharBlog/posts/aspire-getting-started/":{"data":{"":"In this comprehensive guide, we will explore the foundational steps involved in writing your very first Aspire project. Additionally, the post will provide insightful instructions on how to smoothly transition and migrate your existing projects to the Aspire platform. This tutorial aims to equip beginners with the necessary knowledge and confidence to start developing with Aspire effectively.\nMicrosoft has excellent documentation for getting started with Aspire, but I am preparing this document for my personal reference and easy access.","add-net-aspire-to-the-store-web-app#Add .NET Aspire to the Store web app":"Now, let’s enroll the Store project, which implements the web user interface, in .NET Aspire orchestration:\nIn Visual Studio, in the Solution Explorer, right-click the Store project, select Add, and then select .NET Aspire Orchestrator Support.\nIn the Add .NET Aspire Orchestrator Support dialog, select Ok. You should now have two new projects, both of which have been added to the solution.\neShopLite.AppHost is an orchestrator project designed to connect and configure the different projects and services of your application. The orchestrator is set as the Startup project, and it depends on the eShopLite.\nServiceDefaults: This shared project is used to manage configurations that are reused across an orchestrator project designed to connect and configure the various projects and services within eShopLite.Store project.\neshopLite.ServiceDefaults: This shared project is used to manage configurations that are reused across the projects in your solution related to resilience, service discovery, and telemetry.\nIf you would examine the AppHost.cs file, you’ll find the following line of code, which registers the Store project in the .NET Aspire orchestration:\nbuilder.AddProject(\"store\"); To add the Products project to the .NET Aspire:\nIn Visual Studio, in the Solution Explorer, right-click the Products project, select Add, and then select .NET Aspire Orchestrator Support.\nA dialog indicating that the .NET Aspire Orchestrator project already exists, select OK. In the eShopLite.AppHost project, open AppHost.cs file. Notice this line of code, which registers the Products project in the .NET Aspire orchestration:\nbuilder.AddProject(\"products\"); Also notice that the eShopLite.AppHost project, now depends on both the Store and Products projects.","create-your-first-aspire-project#Create your first Aspire Project.":"Start Visual Studio and create a new project. In the Create New Project template, select to create a new “Aspire Starter App”. In the following window, select ‘AspireSample’ as the project name and click Next.\nIn the next dialog box, ensure .NET 9.0 is selected, ensure Redis for caching is checked, and select Create.\nVisual Studio creates a new solution that is structured to use .NET Aspire.","ensure-net-aspire-templates-are-installed#Ensure .NET Aspire templates are installed":"To develop .NET Aspire projects, we need to ensure that Aspire templates are installed on our computer. We can install and list the Aspire templates using the following commands:\ndotnet new install Aspire.ProjectTemplates This will install Aspire, and we can check the installed packages and version using the following command:\ndotnet new list aspire","explore-the-enrolled-application#Explore the enrolled application":"Let’s start by examining the new behavior that .NET Aspire provides.\nNote\nNotice that the eShopLite.AppHost project is the new statup project.\nIn Visual Studio, to start debugging, press F5 to build and launch the project.\nIf the Start Docker Desktop dialog appears, select Yes. Visual Studio starts the Docker engine and creates the necessary containers. When the deployment is complete, the .NET Aspire dashboard is displayed.\nIn the dashboard, select the endpoint for the Products project.\nIn the dashboard, select the endpoint for the products project. A new browser tab appears and displays the product catalog in JSON format.\nIn the menu on the left, select Products. The product catalog is displayed.\nTo stop debugging, close the browser.\nCongratulations, you added .NET Aspire orchestration to your preexisting web app. You can now add .NET Aspire integrations and use the .NET Aspire tooling to streamline your cloud native web app development.","explore-the-sample-application#Explore the sample application":"Data Entities: This project serves as an example class library. It defines the Product class used in the Web App and Web API.\nProducts: This example Web API returns a list of products in the catalog, along with their properties.\nStore: This example Blazor Web App shows the product catalog to visitors on the website.","get-started#Get started":"We will work on an existing application created by Microsoft for this tutorial. We’ll use the following command to clone an existing repository:\ngit clone https://github.com/MicrosoftDocs/mslearn-dotnet-cloudnative-devops.git eShopLite","moving-an-existing-net-application-to-aspire#Moving an existing .NET application to Aspire":"If you have an existing microservice and .NET web app, you can add .NET Aspire to it and get all the included features and benefits.","prerequisites#Prerequisites":".NET 9 Though .NET 8 will be suitable for Aspire, we’ll go with version 9 for Aspire 9.x support. An OCI-compliant container runtime, such as Podman. Visual Studio 2022","run-the-sample-application#Run the sample application":"Load the solution in Visual Studio and in the Solution Explorer right-click the eShopLite solution, and then select Configure Startup Projects.\nSelect Multiple startup projects.\nIn the Action column, Select Start for both the Products and Store projects.\nSelect Ok.\nStart debugging the solution by pressing the F5key.\nTwo pages open in the browser:\nA page displays products in JSON format from a call to the Products Web API.\nA page displays the website’s homepage. In the menu on the left, select “Products” to view the catalog retrieved from the Web API.\nRegardless of the tool employed, initiating multiple projects manually or establishing the necessary connections between them can be a laborious and time-consuming task. Furthermore, the Store project mandates explicit configuration of endpoints for the Products API, which can be both cumbersome and susceptible to errors. In such scenarios, .NET Aspire offers an effective solution by simplifying and streamlining these processes.","service-discovery#Service Discovery":"At this point, both projects are part of .NET Aspire orchestration, but the Store project needs to rely on the Products backend address through .NET Aspire’s service discovery. To enable service discovery, open the AppHost.cs file in the eShopLite.AppHost project and update the code so that the builder adds a reference to the Products project:\nvar builder = DistributedApplication.CreateBuilder(args); var products = builder.AddProject(\"products\"); builder.AddProject(\"store\") .WithExternalHttpEndpoints() .WithReference(products) .WaitFor(products); builder.Build().Run(); The preceding code expresses that the Store project depends on the Products project. This reference is used to discover the address of the Products project at run time. Additionally, the Store project is configured to use external HTTP endpoints. If you later choose to deploy this application, you’d need the call to WithExternalHttpEndpoints to ensure that it’s public to the outside world. Finally, the WaitFor API ensures that the Store app waits for the Products app to be ready to serve requests.\nNext, update the appsettings.json in the Store project with the following JSON:\n{ \"DetailedErrors\": true, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } }, \"AllowedHosts\": \"*\", \"ProductEndpoint\": \"http://products\", \"ProductEndpointHttps\": \"https://products\" } Change to note is ProductEndpoint and ProductEndpointHttps. The destination addresses were hardcoded, but now use the “products” name that was added to the orchestrator in the AppHost. These names are used to discover the address of the Products project.\nNote\nNotice that the eShopLite.AppHost project is the new startup project.","testing-the-application#Testing the application":"The sample application includes a frontend Blazor app that communicates with a Minimal API project. The API project is used to provide weather data to the frontend. The frontend app is configured to use service discovery to connect to the API project. The API project is configured to use output caching with Redis. The sample app is now ready for testing. When we deploy the application, we can verify the following conditions:\nWeather data is retrieved from the API project using service discovery and displayed on the weather page. Subsequent requests are handled via the output caching configured by the .NET Aspire Redis integration. In Visual Studio, set the AspireSample.AppHost project as the startup project if it isn’t set as startup project. When you start debugging/running the application:\nThe application displays the .NET Aspire dashboard in the browser.\nLaunch the Blazor frontend by clicking on the launch button in Aspire dashboard.\nNavigate from the home page to the weather page. Make a mental note of some of the values represented in the forecast table. Continue occasionally refreshing the page. If it is refreshed within 10 seconds, the cached data is returned. Eventually, the cache data will expire, and a new set of values will be displayed.\nCongratulations! You have created and run your first .NET Aspire solution! Below, we will outline the steps to migrate your existing solution to Aspire."},"title":"Aspire Getting Started"},"/SridharBlog/posts/aws-saa-c02-study-guide/":{"data":{"amazon-fsx-for-lustre#Amazon FSx for Lustre":"","amazon-fsx-for-lustre-key-details#Amazon FSx for Lustre Key Details:":"FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu. Since the Lustre file system is designed for high-performance computing workloads that typically run on compute clusters, choose EFS for normal Linux file system if your requirements don’t match this use case. FSx Lustre has the ability to store and retrieve data directly on S3 on its own.","amazon-fsx-for-lustre-simplified#Amazon FSx for Lustre Simplified:":"Amazon FSx for Lustre makes it easy and cost effective to launch and run the open source Lustre file system for high-performance computing applications. With FSx for Lustre, you can launch and run a file system that can process massive data sets at up to hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.","amazon-fsx-for-windows#Amazon FSx for Windows":"","amazon-fsx-for-windows-key-details#Amazon FSx for Windows Key Details:":"With FSx for Windows, you can easily move your Windows-based applications that require file storage in AWS. It is built on Windows Server and exists solely for Microsoft-based applications so if you need SMB-based file storage then choose FSx. FSx for Windows also permits connectivity between on-premise servers and AWS so those same on-premise servers can make use of Amazon FSx too. You can use Microsoft Active Directory to authenticate into the file system. Amazon FSx for Windows provides multiple levels of security and compliance to help ensure your data is protected. Amazon FSx automatically encrypts your data at-rest and in-transit. You can access Amazon FSx for Windows from a variety of compute resources, not just EC2. You can deploy your Amazon FSx for Windows in a single AZ or in a Multi-AZ configuration. You can use SSD or HDD for the storage device depending on your requirements. FSx for Windows support daily automated backups and admins take take backups when needed as well. FSx for Windows removes duplicated content and compresses common content By default, all data is encrypted at rest.","amazon-fsx-for-windows-simplified#Amazon FSx for Windows Simplified:":"Amazon FSx for Windows File Server provides a fully managed native Microsoft File System.","api-gateway#API Gateway":"","api-gateway-key-details#API Gateway Key Details:":"Amazon API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Amazon API Gateway has no minimum fees or startup costs. You pay only for the API calls you receive and the amount of data transferred out. API Gateway does the following for your APIs: Exposes HTTP(S) endpoints for RESTful functionality Uses serverless functionality to connect to Lambda \u0026 DynamoDB Can send each API endpoint to a different target Runs cheaply and efficiently Scales readily and effortlessly Can throttle requests to prevent attacks Track and control usage via an API key Can be version controlled Can be connected to CloudWatch for monitoring and observability Since API Gateway can function with AWS Lambda, you can run your APIs and code without needing to maintain servers. Amazon API Gateway provides throttling at multiple levels including global and by a service call. In software, a throttling process, or a throttling controller as it is sometimes called, is a process responsible for regulating the rate at which application processing is conducted, either statically or dynamically. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage. You can enable API caching for improving latency and reducing I/O for your endpoint. When caching for a particular API stage (version controlled version), you cache responses for a particular TTL in seconds. API Gateway supports AWS Certificate Manager and can make use of free TLS/SSL certificates. With API Gateway, there are two kinds of API calls: Calls to the API Gateway API to create, modify, delete, or deploy REST APIs. These are logged in CloudTrail. API calls set up by the developers to deliver their custom functionality: These are not logged in CloudTrail.","api-gateway-simplified#API Gateway Simplified:":"API Gateway is a fully managed service for developers that makes it easy to build, publish, manage, and secure entire APIs. With a few clicks in the AWS Management Console, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as workloads running on EC2) code running on AWS Lambda, or any web application.","aurora#Aurora":"","aurora-cluster-endpoints#Aurora Cluster Endpoints:":"Using cluster endpoints, you map each connection to the appropriate instance or group of instances based on your use case. You can connect to cluster endpoints associated with different roles or jobs across your Aurora DB. This is because different instances or groups of instances perform different functions. For example, to perform DDL statements you can connect to the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas behind the reader endpoint. For diagnosis or tuning, you can connect to a different endpoint to examine details. Since the entryway for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention for any of your endpoints.","aurora-key-details#Aurora Key Details:":"In case of an infrastructure failure, Aurora performs an automatic failover to to a replica of its own. Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hard code all the host names or write your own logic for load-balancing and rerouting connections when some DB instances aren’t available. By default, there are 2 copies in a minimum of 3 availability zones for 6 copies total for all of your Aurora data. This makes it possible for it to handle the potential loss of up to 2 copies of your data without impacting write availability and up to 3 copies of your data without impacting read availability. Aurora storage is self-healing and data blocks and disks are continuously scanned for errors. If any are found, those errors are repaired automatically. Aurora replication differs from RDS replicas in the sense that it is possible for Aurora’s replicas to be be both a standby as part of a multi-AZ configuration as well as a target for read traffic. In RDS, the multi-AZ standby cannot be configured to be a read endpoint and only read replicas can serve that function. With Aurora replication, you can have up to fifteen copies. If you want downstream MySQL or PostgreSQL as you replicated copies, then you can only have 5 or 1. Automated failover is only possible with Aurora read replication For more on the differences between RDS replication and Aurora Replication, please consult the following: Automated backups are always enabled on Aurora instances and backups don’t impact DB performance. You can also take snapshots which also don’t impact performance. Your snapshots can be shared across AWS accounts. A common tactic for migrating RDS DBs into Aurora RDs is to create a read replica of a RDS MariaDB/MySQL DB as an Aurora DB. Then simply promote the Aurora DB into a production instance and delete the old MariaDB/MySQL DB. Aurora starts w/ 10GB and scales per 10GB all the way to 64 TB via storage autoscaling. Aurora’s computing power scales up to 32vCPUs and 244GB memory","aurora-reader-endpoints#Aurora Reader Endpoints:":"Aurora Reader endpoints are a subset of the above idea of cluster endpoints. Use the reader endpoint for read operations, such as queries. By processing those statements on the read-only Aurora Replicas, this endpoint reduces the overhead on the primary instance. There are up 15 Aurora Read Replicas because a Reader Endpoint to help handle read-only query traffic. It also helps the cluster to scale the capacity to handle simultaneous SELECT queries, proportional to the number of Aurora Replicas in the cluster. Each Aurora DB cluster has one reader endpoint. If the cluster contains one or more Aurora Replicas, the reader endpoint load-balances each connection request among the Aurora Replicas. In that case, you can only perform read-only statements such as SELECT in that session. If the cluster only contains a primary instance and no Aurora Replicas, the reader endpoint connects to the primary instance directly. In that case, you can perform write operations through the endpoint.","aurora-serverless#Aurora Serverless:":"Aurora Serverless is a simple, on-demand, autoscaling configuration for the MySQL/PostgreSQl-compatible editions of Aurora. With Aurora Serverless, your instance automatically scales up or down and starts on or off based on your application usage. The use cases for this service are infrequent, intermittent, and unpredictable workloads. This also makes it possible cheaper because you only pay per invocation With Aurora Serverless, you simply create a database endpoint, optionally specify the desired database capacity range, and connect your applications. It removes the complexity of managing database instances and capacity. The database will automatically start up, shut down, and scale to match your application’s needs. It will seamlessly scale compute and memory capacity as needed, with no disruption to client connections.","aurora-simplified#Aurora Simplified:":"Aurora is the AWS flagship DB known to combine the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. It is a MySQL/PostgreSQL-compatible RDBMS that provides the security, availability, and reliability of commercial databases at 1/10th the cost of competitors. It is far more effective as an AWS database due to the 5x and 3x performance multipliers for MySQL and PostgreSQL respectively.","auto-scaling#Auto Scaling":"","auto-scaling-cooldown-period#Auto Scaling Cooldown Period:":"The cooldown period is a configurable setting for your Auto Scaling Group that helps to ensure that it doesn’t launch or terminate additional instances before the previous scaling activity takes effect. After the Auto Scaling Group scales using a policy, it waits for the cooldown period to complete before resuming further scaling activities if needed. The default waiting period is 300 seconds, but this can be modified.","auto-scaling-default-termination-policy#Auto Scaling Default Termination Policy:":"The default termination policy for an Auto Scaling Group is to automatically terminate a stopped instance, so unless you’ve configured it to do otherwise, stopping an instance will result in termination regardless if you wanted that to happen or not. A new instance will be spun up in its place. The default termination policy will spare instances that you tell it in case some servers are running critical systems or applications. These critical servers are protected from “scale in”, which is just the deletion process of instances deemed superfluous to requirements. The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows: If there are instances in multiple Availability Zones, it will terminate an instance from the Availability Zone with the most instances. If there is more than one Availability Zone with the same max number of instances, it will choose the Availability Zone where instances use the oldest launch configuration. It will then determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, it will terminate it. If there are multiple instances to terminate, it will determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there are some instances that match this criteria, they will be terminated. This flow chart can provide further clarity on how the default Auto Scaling policy decides which instances to delete:","auto-scaling-key-details#Auto Scaling Key Details:":"Auto Scaling is a major benefit from the cloud’s economies of scale so if you ever have a requirement for scaling, automatically think of using the Auto Scaling service. Auto Scaling has three components: Groups: These are logical components. A webserver group of EC2 instances, a database group of RDS instances, etc. Configuration Templates: Groups use a template to configure and launch new instances to better match the scaling needs. You can specify information for the new instances like the AMI to use, the instance type, security groups, block devices to associate with the instances, and more. Scaling Options: Scaling Options provides several ways for you to scale your Auto Scaling groups. You can base the scaling trigger on the occurrence of a specified condition or on a schedule. The following image highlights the state of an Auto scaling group. The orange squares represent active instances. The dotted squares represent potential instances that can and will be spun up whenever necessary. The minimum number, the maximum number, and the desired capacity of instances are all entirely configurable. When you use Auto Scaling, your applications gain the following benefits: Better fault tolerance: Auto Scaling can detect when an instance is unhealthy, terminate it, and launch an instance to replace it. You can also configure Auto Scaling to use multiple Availability Zones. If one Availability Zone becomes unavailable, Auto Scaling can launch instances in another one to compensate. Better availability: Auto Scaling can help you ensure that your application always has the right amount of capacity to handle the current traffic demands. When it comes to actually scale your instance groups, the Auto Scaling service is flexible and can be done in various ways: Auto Scaling can scale based on the demand placed on your instances. This option automates the scaling process by specifying certain thresholds that, when reached, will trigger the scaling. This is the most popular implementation of Auto Scaling. Auto Scaling can ensure the current number of instances at all times. This option will always maintain the number of servers you want running even when they fail. Auto Scaling can scale only with manual intervention. If want to control all of the scaling yourself, this option makes sense. Auto Scaling can scale based on a schedule. If you can reliably predict spikes in traffic, this option makes sense. Auto Scaling based off of predictive scaling. This option lets AWS AI/ML learn more about your environment in order to predict the best time to scale for both performance improvements and cost-savings. In maintaining the current running instance, Auto Scaling will perform occasional health checks on the running instances to ensure that they are all healthy. When the service detects that an instance is unhealthy, it will terminate that instance and then bring up a new one online. When designing HA for your Auto Scaling, use multiple AZs and multiple regions wherever you can. Auto Scaling allows you to suspend and then resume one or more of the Auto Scaling processes in your Auto Scaling Group. This can be very useful when you want to investigate a problem in your application without triggering the Auto Scaling process when making changes. You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time. You cannot modify a launch configuration after you’ve created it. If you want to change the launch configuration for an Auto Scaling group, you must create a new launch configuration and update your Auto Scaling group to inherit this new launch configuration.","auto-scaling-simplified#Auto Scaling Simplified:":"AWS Auto Scaling lets you build scaling plans that automate how groups of different resources respond to changes in demand. You can optimize availability, costs, or a balance of both. AWS Auto Scaling automatically creates all of the scaling policies and sets targets for you based on your preference.","aws-directconnect#AWS DirectConnect:":"Direct Connect is an AWS service that establishes a dedicated network connection between your premises and AWS. You can create this private connectivity to reduce network costs, increase bandwidth, and provide more consistent network experience compared to regular internet-based connections. The use case for Direct Connect is high throughput workloads or if you need a stable or reliable connection VPN connects to your on-prem over the internet and DirectConnect connects to your on-prem off through a private tunnel. The steps for setting up an AWS DirectConnect connection: Create a virtual interface in the DirectConnect console. This is a public virtual interface. Go to the VPC console and then VPN connections. Create a customer gateway for your on-premise. Create a virtual private gateway and attach it to the desired VPC environment. Select VPN connections and create a new VPN connection. Select both the customer gateway and the virtual private gateway. Once the VPN connection is available, set up the VPN either on the customer gateway or the on-prem firewall itself Data flow into AWS via DirectConnect looks like the following: On-prem router -\u003e dedicated line -\u003e your own cage / DMZ -\u003e cross connect line -\u003e AWS Direct Connect Router -\u003e AWS backbone -\u003e AWS Cloud Summary: DirectConnect connects your on-prem with your VPC through a non-public tunnel.","aws-global-accelerator#AWS Global Accelerator:":"AWS Global Accelerator accelerates connectivity to improve performance and availability for users. Global Accelerator sits on top of the AWS backbone and directs traffic to optimal endpoints worldwide. By default, Global Accelerator provides you two static IP addresses that you can make use of.\nGlobal Accelerator helps reduce the number of hops to get to your AWS resources. Your users just need to make it to an edge location and once there, everything will remain internal to the AWS global network. Normally, it takes many networks to reach the application in full and paths to and from the application may vary. With each hop, there is risk involved either in security or in failure.\nIn summary, Global Accelerator is a fast/reliable pipeline between user and application. It’s like going on a trip (web traffic) and stopping to ask for directions in possibly unsafe parts of town (multiple networks are visited which can increase security risks) as opposed to having a GPS (global accelerator) that leads you directly where you want to go (endpoint) without having to make unnecessary stops. It can be confused with Cloudfront, but CloudFront is a cache for content stemming from a distant origin server. While CloudFront simply caches static content to the closest AWS Point Of Presence (POP) location, Global accelerator will use the same Amazon POP to accept initial requests and routes them directly to the services. Route53’s latency based routing might also appear similar to Global Accelerator, but Route 53 is for simply helping choose which region for the user to use. Route53 has nothing to do with actually providing a fast network path. Global Accelerator also provides fast regional failover.","aws-organizations#AWS Organizations":"","aws-organizations-key-details#AWS Organizations Key Details:":"Best practices is to use the root account to manage billing only with separate accounts used to deploy resources. The point of AWS Organizations is to deploy permissions to the separate accounts underneath the root account and have those policies trickle down. AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. You can use organizational units (OUs) to group similar accounts together to administer as a single unit. This greatly simplifies the management of your accounts. You can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. So if your company’s developers all have their own sandbox AWS account, they can be treated as a single unit and be restricted by the same policies. With AWS Organizations, we can enable or disable services using Service Control Policies (SCPs) broadly on organizational units or more specifically on individual accounts Use SCPs with AWS Organizations to establish access controls so that all IAM principals (users and roles) adhere to them. With SCPs, you can specify Conditions, Resources, and NotAction to deny access across accounts in your organization or organizational unit. For example, you can use SCPs to restrict access to specific AWS Regions, or prevent deleting common resources, such as an IAM role used for your central administrators.","aws-organizations-simplified#AWS Organizations Simplified:":"AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.","aws-privatelink#AWS PrivateLink:":"AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between different VPCs, AWS services, and on-premises applications, securely on the Amazon network. It’s similar to the AWS Direct Connect service in that it establishes private connections to the AWS cloud, except Direct Connect links on-premises environments to AWS. PrivateLink, on the other hand, secures traffic from VPC environments which are already in AWS. This is useful because different AWS services often talk to each other over the internet. If you do not want that behavior and instead want AWS services to only communicate within the AWS network, use AWS PrivateLink. By not traversing the Internet, PrivateLink reduces the exposure to threat vectors such as brute force and distributed denial-of-service attacks. PrivateLink allows you to publish an “endpoint” that others can connect with from their own VPC. It’s similar to a normal VPC Endpoint, but instead of connecting to an AWS service, people can connect to your endpoint. Further, you’d want to use private IP connectivity and security groups so that your services function as though they were hosted directly on your private network. Remember that AWS PrivateLink applies to Applications/Services communicating with each other within the AWS network. For VPCs to communicate with each other within the AWS network, use VPC Peering. Summary: AWS PrivateLink connects your AWS services with other AWS services through a non-public tunnel.","aws-saa-c02-study-guide#AWS SAA-C02 Study Guide":"This study guide will help you pass the newer AWS Certified Solutions Architect - Associate exam. Ideally, you should reference this guide while working through the following material:\nStephane Maarek’s Ultimate AWS Certified Solutions Architect Associate 2021 course (permanent discount available through this link) or A Cloud Guru’s AWS Certified Solutions Architect Associate SAA-C02 course The FAQs for the most critical services, included in the recommended reading list below Tutorials Dojo’s AWS Certified Solutions Architect Associate Practice Exams Andrew Brown’s AWS Certified Solutions Architect - Associate 2020 (PASS THE EXAM!) | Ad-Free Course Notes: If at any point you find yourself feeling uncertain of your progress and in need of more time, you can postpone your AWS exam date. Be sure to also keep up with the ongoing discussions in r/AWSCertifications as you will find relevant exam tips, studying material, and advice from other exam takers. Before experimenting with AWS, it’s very important to be sure that you know what is free and what isn’t. Relevant Free Tier FAQs can be found here. Finally, Udemy often has their courses go on sale from time to time. It might be worth waiting to purchase either the Tutorial Dojo practice exam or Stephane Maarek’s course depending on how urgently you need the content.","bastion-hosts#Bastion Hosts:":"Bastion Hosts are special purpose computers designed and configured to withstand attacks. This server generally runs a single program and is stripped beyond this purpose in order to reduce attack vectors. The purpose of Bastion Hosts are to remotely access the instances behind the private subnet for system administration purposes without exposing the host via an internet gateway. The best way to implement a Bastion Host is to create a small EC2 instance that only has a security group rule for a single IP address. This ensures maximum security. It is perfectly fine to use a small instance rather than a large one because the instance will only be used as a jump server that connects different servers to each other. If you are going to RDP or SSH into the instances of your private subnet, use a Bastion Host. If you are going to be providing internet traffic into the instances of your private subnet, use a NAT. Similar to NAT Gateways and NAT Instances, Bastion Hosts live within a public-facing subnet. There are pre-baked Bastion Host AMIs.","cloudformation#CloudFormation":"","cloudformation-key-details#CloudFormation Key Details:":"The main use case for CloudFormation is for advanced setups and production environments as it is complex and has many robust features. CloudFormation templates can be used to create, update, and delete infrastructure. The templates are written in YAML or JSON A full CloudFormation setup is called a stack. Once a template is created, AWS will make the corresponding stack. This is the living and active representation of said template. One template can create an infinite number of stacks. The Resources field is the only mandatory field when creating a CloudFormation template Rollback triggers allow you to monitor the creation of the stack as it’s built. If an error occurs, you can trigger a rollback as the name implies. AWS Quick Starts is composed of many high-quality CloudFormation stacks designed by AWS engineers. An example template that would spin up an EC2 instance: For any Logical Resources in the stack, CloudFormation will make a corresponding Physical Resources in your AWS account. It is CloudFormation’s job to keep the logical and physical resources in sync. A template can be updated and then used to update the same stack.","cloudformation-simplified#CloudFormation Simplified:":"CloudFormation is an automated tool for provisioning entire cloud-based environments. It is similar to Terraform where you codify the instructions for what you want to have inside your application setup (X many web servers of Y type with a Z type DB on the backend, etc). It makes it a lot easier to just describe what you want in markup and have AWS do the actual provisioning work involved.","cloudfront#CloudFront":"","cloudfront-key-details#CloudFront Key Details:":"When content is cached, it is done for a certain time limit called the Time To Live, or TTL, which is always in seconds If needed, CloudFront can serve up entire websites including dynamic, static, streaming and interactive content. Requests are always routed and cached in the nearest edge location for the user, thus propagating the CDN nodes and guaranteeing best performance for future requests. There are two different types of distributions: Web Distribution: web sites, normal cached items, etc RTMP: streaming content, adobe, etc Edge locations are not just read only. They can be written to which will then return the write value back to the origin. Cached content can be manually invalidated or cleared beyond the TTL, but this does incur a cost. You can invalidate the distribution of certain objects or entire directories so that content is loaded directly from the origin every time. Invalidating content is also helpful when debugging if content pulled from the origin seems correct, but pulling that same content from an edge location seems incorrect. You can set up a failover for the origin by creating an origin group with two origins inside. One origin will act as the primary and the other as the secondary. CloudFront will automatically switch between the two when the primary origin fails. Amazon CloudFront delivers your content from each edge location and offers a Dedicated IP Custom SSL feature. SNI Custom SSL works with most modern browsers. If you run PCI or HIPAA-compliant workloads and need to log usage data, you can do the following: Enable CloudFront access logs. Capture requests that are sent to the CloudFront API. An Origin Access Identity (OAI) is used for sharing private content via CloudFront. The OAI is a virtual user that will be used to give your CloudFront distribution permission to fetch a private object from your origin (e.g. S3 bucket).","cloudfront-signed-urls-and-signed-cookies#CloudFront Signed URLs and Signed Cookies:":"CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. These features exist because many companies that distribute content via the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users. As an example, users who have paid a fee should be able to access private content that users on the free tier shouldn’t. If you want to serve private content through CloudFront and you’re trying to decide whether to use signed URLs or signed cookies, consider the following: Use signed URLs for the following cases: You want to use an RTMP distribution. Signed cookies aren’t supported for RTMP distributions. You want to restrict access to individual files, for example, an installation download for your application. Your users are using a client (for example, a custom HTTP client) that doesn’t support cookies. Use signed cookies for the following cases: You want to provide access to multiple restricted files. For example, all of the files for a video in HLS format or all of the files in the paid users’ area of a website. You don’t want to change your current URLs.","cloudfront-simplified#CloudFront Simplified:":"The AWS CDN service is called CloudFront. It serves up cached content and assets for the increased global performance of your application. The main components of CloudFront are the edge locations (cache endpoints), the origin (original source of truth to be cached such as an EC2 instance, an S3 bucket, an Elastic Load Balancer or a Route 53 config), and the distribution (the arrangement of edge locations from the origin or basically the network itself). More info on CloudFront’s features","cloudtrail#CloudTrail":"","cloudtrail-key-details#CloudTrail Key Details:":"CloudTrail Events logs API calls or activities. CloudTrail Events stores the last 90 days of events in its Event History. This is enabled by default and is no additional cost. This event history simplifies security analysis, resource change tracking, and troubleshooting. There are two types of events that can be logged in CloudTrail: management events and data events. Management events provide information about management operations that are performed on resources in your AWS account. Think of Management events as things normally done by people when they are in AWS. Examples: a user sign in a policy changed a newly created security configuration a logging rule deletion Data events provide information about the resource operations performed on or in a resource. Think of Data events as things normally done by software when hitting various AWS endpoints. Examples: S3 object-level API activity Lambda function execution activity By default, CloudTrail logs management events, but not data events. By default, CloudTrail Events log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key. As these logs are stored in S3, you can define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about log file delivery and validation, you can set up Amazon SNS notifications.","cloudtrail-simplified#CloudTrail Simplified:":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With it, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, API calls, and other AWS services. It is a regional service, but you can configure CloudTrail to collect trails in all regions.","cloudwatch#CloudWatch":"","cloudwatch-alarms#CloudWatch Alarms:":"CloudWatch alarms send notifications or automatically make changes to the resources you are monitoring based on rules that you define. For example, you can create custom CloudWatch alarms which will trigger notifications such as surpassing a set billing threshold. CloudWatch alarms have two states of either ok or alarm","cloudwatch-dashboards#CloudWatch Dashboards:":"CloudWatch dashboards are customizable home pages in the CloudWatch console that you can use to monitor your resources in a single view These dashboards integrate with CloudWatch Metrics and CloudWatch Alarms to create customized views of the metrics and alarms for your AWS resources.","cloudwatch-events#CloudWatch Events:":"Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. You can use events to trigger lambdas for example while using alarms to inform you that something went wrong.","cloudwatch-key-details#CloudWatch Key Details:":"CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly. Within the compute domain, CloudWatch can inform you about the health of EC2 instances, Autoscaling Groups, Elastic Load Balancers, and Route53 Health Checks. Within the storage and content delivery domains, CloudWatch can inform you about the health of EBS Volumes, Storage Gateways, and CloudFront. With regards to EC2, CloudWatch can only monitor host level metrics such as CPU, network, disk, and status checks for insights like the health of the underlying hypervisor. CloudWatch is NOT CloudTrail so it is important to know that only CloudTrail can monitor AWS access for security and auditing reasons. CloudWatch is all about performance. CloudTrail is all about auditing. CloudWatch with EC2 will monitor events every 5 minutes by default, but you can have 1 minute intervals if you use Detailed Monitoring. You can customize your CloudWatch dashboards for insights. There is a multi-platform CloudWatch agent which can be installed on both Linux and Windows-based instances. This agent enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. You can use this single agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The following metrics are not collected from EC2 instances via CloudWatch: Memory utilization Disk swap utilization Disk space utilization Page file utilization Log collection If you need the above information, then you can retrieve it via the official CloudWatch agent or you can create a custom metric and send the data on your own via a custom script. CloudWatch’s key purpose: Collect metrics Collect logs Collect events Create alarms Create dashboards","cloudwatch-logs#CloudWatch Logs:":"You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon EC2 instances, AWS CloudTrail, Amazon Route 53, and other sources. You can then retrieve the associated log data from CloudWatch Logs. It helps you centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can create log groups so that you join logical units of CloudWatch Logs together. You can stream custom log files for further insights.","cloudwatch-metrics#CloudWatch Metrics:":"CloudWatch Metrics represent a time-ordered set of data points. These basically are a variable you can monitor over time to help tell if everything is okay, e.g. Hourly CPU Utilization. CloudWatch Metrics allows you to track high resolution metrics at sub-minute intervals all the way down to per second.","cloudwatch-simplified#CloudWatch Simplified:":"Amazon CloudWatch is a monitoring and observability service. It provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.","cross-origin-resource-sharing#Cross Origin Resource Sharing:":"In computing, the same-origin policy is an important concept where a web browser permits scripts contained in one page to access data from another page, but only if both pages have the same origin. This behavior is enforced by browsers, but is ignored by tools like cURL and PostMan. Cross-origin resource sharing (CORS) is one way the server at the origin can relax the same-origin policy. CORS allows sharing of restricted resources like fonts to be requested from another domain outside the original domain of where the first resource was shared from. CORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. If you ever come across an error that mentions that an origin policy cannot be read at the remote resource, then you need to enable CORS on API Gateway. CORS is enforced on the client (web browser) side. A common example of this issue is if you are using a site with Javascript/AJAX for multiple domains under API Gateway. You would need to ensure that CORS is enabled. CORS does not prevent XSS attacks, but does protect against CSRF attacks. What it does is controls who can use the data served by your endpoint. So if you have a weather website with callbacks to an API that checks the forecast, you could stop someone from writing a website that serves JavaScript calls into your API when they navigate to your website. When someone attempts the malicious calls, your browser will read the CORS headers and it will not allow the request to take place thus protecting you from the attack.","dynamodb#DynamoDB":"","dynamodb-accelerator-dax#DynamoDB Accelerator (DAX):":"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache that can reduce Amazon DynamoDB response times from milliseconds to microseconds, even at millions of requests per second. With DAX, your applications remain fast and responsive, even when unprecedented request volumes come your way. There is no tuning required. DAX lets you scale on-demand out to a ten-node cluster, giving you millions of requests per second. DAX does more than just increase read performance by having write through cache. This improves write performance as well. Just like DynamoDB, DAX is fully managed. You no longer need to worry about management tasks such as hardware or software provisioning, setup and configuration, software patching, operating a reliable, distributed cache cluster, or replicating data over multiple instances as you scale. This means there is no need for developers to manage the caching logic. DAX is completely compatible with existing DynamoDB API calls. DAX enables you to provision one DAX cluster for multiple DynamoDB tables, multiple DAX clusters for a single DynamoDB table or somewhere in between giving you maximal flexibility. DAX is designed for HA so in the event of a failure of one AZ, it will fail over to one of its replicas in another AZ. This is also managed automatically.","dynamodb-global-tables#DynamoDB Global Tables":"Global Tables is a multi-region, multi-master replication solution for fast local performance of globally distributed apps. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions. It is based on DynamoDB streams and is multi-region redundant for data recovery or high availability purposes. Application failover is as simple as redirecting your application’s DynamoDB calls to another AWS region. Global Tables eliminates the difficult work of replicating data between regions and resolving update conflicts, enabling you to focus on your application’s business logic. You do not need to rewrite your applications to make use of Global Tables. Replication latency with Global Tables is typically under one second.","dynamodb-key-details#DynamoDB Key Details:":"The main components of DynamoDB are: a collection which serves as the foundational table a document which is equivalent to a row in a SQL database key-value pairs which are the fields within the document or row The convenience of non-relational DBs is that each row can look entirely different based on your use case. There doesn’t need to be uniformity. For example, if you need a new column for a particular entry you don’t also need to ensure that that column exists for the other entries. DynamoDB supports both document and key-value based models. It is a great fit for mobile, web, gaming, ad-tech, IoT, etc. DynamoDB is stored via SSD which is why it is so fast. It is spread across 3 geographically distinct data centers. The default consistency model is Eventually Consistent Reads, but there are also Strongly Consistent Reads. The difference between the two consistency models is the one second rule. With Eventual Consistent Reads, all copies of data are usually reached within one second. A repeated read after a short period of time should return the updated data. However, if you need to read updated data within or less than a second and this needs to be a guarantee, then strongly consistent reads are your best bet. If you face a scenario that requires the schema, or the structure of your data, to change frequently, then you have to pick a database which provides a non-rigid and flexible way of adding or removing new types of data. This is a classic example of choosing between a relational database and non-relational (NoSQL) database. In this scenario, pick DynamoDB. A relational database system does not scale well for the following reasons: It normalizes data and stores it on multiple tables that require multiple queries to write to disk. It generally incurs the performance costs of an ACID-compliant transaction system. It uses expensive joins to reassemble required views of query results. High cardinality is good for DynamoDB I/O performance. The more distinct your partition key values are, the better. It makes it so that the requests sent will be spread across the partitioned space. DynamoDB makes use of parallel processing to achieve predictable performance. You can visualize each partition or node as an independent DB server of fixed size with each partition or node responsible for a defined block of data. In SQL terminology, this concept is known as sharding but of course DynamoDB is not a SQL-based DB. With DynamoDB, data is stored on Solid State Drives (SSD).","dynamodb-simplified#DynamoDB Simplified:":"Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It’s a fully managed, multiregion, multimaster, durable non-SQL database. It comes with built-in security, backup and restore, and in-memory caching for internet-scale applications.","dynamodb-streams#DynamoDB Streams:":"A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. Immediately after an item in the table is modified, a new record appears in the table’s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. With triggers, you can build applications that react to data modifications in DynamoDB tables. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the “before” and “after” images of modified items.","ebs-encryption#EBS Encryption:":"EBS encryption offers a straight-forward encryption solution for EBS resources that doesn’t require you to build, maintain, and secure your own key management infrastructure. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. You can encrypt both the root device and secondary volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted: Data at rest inside the volume All data moving between the volume and the instance All snapshots created from the volume All volumes created from those snapshots EBS encrypts your volume with a data key using the AES-256 algorithm. Snapshots of encrypted volumes are naturally encrypted as well. Volumes restored from encrypted snapshots are also encrypted. You can only share unencrypted snapshots. The old way of encrypting a root device was to create a snapshot of a provisioned EC2 instance. While making a copy of that snapshot, you then enabled encryption during the copy’s creation. Finally, once the copy was encrypted, you then created an AMI from the encrypted copy and used to have an EC2 instance with encryption on the root device. Because of how complex this is, you can now simply encrypt root devices as part of the EC2 provisioning options.","ebs-key-details#EBS Key Details:":"EBS volumes persist independently from the running life of an EC2 instance. Each EBS volume is automatically replicated within its Availability Zone to protect from both component failure and disaster recovery (similar to Standard S3). There are five different types of EBS Storage: General Purpose (SSD) Provisioned IOPS (SSD, built for speed) Throughput Optimized Hard Disk Drive (magnetic, built for larger data loads) Cold Hard Disk Drive (magnetic, built for less frequently accessed workloads) Magnetic EBS Volumes offer 99.999% SLA. Wherever your EC2 instance is, your volume for it is going to be in the same availability zone An EBS volume can only be attached to one EC2 instance at a time. After you create a volume, you can attach it to any EC2 instance in the same availability zone. Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to S3, where it is stored redundantly in multiple Availability Zones An EBS snapshot reflects the contents of the volume during a concrete instant in time. An image (AMI) is the same thing, but includes an operating system and a boot loader so it can be used to boot an instance. AMIs can also be thought of as pre-baked, launchable servers. AMIs are always used when launching an instance. When you provision an EC2 instance, an AMI is actually the first thing you are asked to specify. You can choose a pre-made AMI or choose your own made from an EBS snapshot. You can also use the following criteria to help pick your AMI: Operating System Architecture (32-bit or 64-bit) Region Launch permissions Root Device Storage (more in the relevant section below) You can copy AMIs into entirely new regions. When copying AMIs to new regions, Amazon won’t copy launch permissions, user-defined tags, or Amazon S3 bucket permissions from the source AMI to the new AMI. You must ensure those details are properly set for the instances in the new region. You can change EBS volumes on the fly, including the size and storage type","ebs-root-device-storage#EBS Root Device Storage:":"All AMI root volumes (where the EC2’s OS is installed) are of two types: EBS-backed or Instance Store-backed When you delete an EC2 instance that was using an Instance Store-backed root volume, your root volume will also be deleted. Any additional or secondary volumes will persist however. If you use an EBS-backed root volume, the root volume will not be terminated with its EC2 instance when the instance is brought offline. EBS-backed volumes are not temporary storage devices like Instance Store-backed volumes. EBS-backed Volumes are launched from an AWS EBS snapshot, as the name implies Instance Store-backed Volumes are launched from an AWS S3 stored template. They are ephemeral, so be careful when shutting down an instance! Secondary instance stores for an instance-store backed root device must be installed during the original provisioning of the server. You cannot add more after the fact. However, you can add EBS volumes to the same instance after the server’s creation. With these drawbacks of Instance Store volumes, why pick one? Because they have a very high IOPS rate. So while an Instance Store can’t provide data persistence, it can provide much higher IOPS compared to network attached storage like EBS. Further, Instance stores are ideal for temporary storage of information that changes frequently such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. When to use one over the other? Use EBS for DB data, critical logs, and application configs. Use instance storage for in-process data, noncritical logs, and transient application state. Use S3 for data shared between systems like input datasets and processed results, or for static data needed by each new system when launched.","ebs-simplified#EBS Simplified:":"An Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. You can think of EBS as a cloud-based virtual hard disk. You can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans.","ebs-snapshots#EBS Snapshots:":"EBS Snapshots are point in time copies of volumes. You can think of Snapshots as photographs of the disk’s current state and the state of everything within it. A snapshot is constrained to the region where it was created. Snapshots only capture the state of change from when the last snapshot was taken. This is what is recorded in each new snapshot, not the entire state of the server. Because of this, it may take some time for your first snapshot to be created. This is because the very first snapshot’s change of state is the entire new volume. Only afterwards will the delta be captured because there will then be something previous to compare against. EBS snapshots occur asynchronously which means that a volume can be used as normal while a snapshot is taking place. When creating a snapshot for a future root device, it is considered best practices to stop the running instance where the original device is before taking the snapshot. The easiest way to move an EC2 instance and a volume to another availability zone is to take a snapshot. When creating an image from a snapshot, if you want to deploy a different volume type for the new image (e.g. General Purpose SSD -\u003e Throughput Optimized HDD) then you must make sure that the virtualization for the new image is hardware-assisted. A short summary for creating copies of EC2 instances: Old instance -\u003e Snapshot -\u003e Image (AMI) -\u003e New instance You cannot delete a snapshot of an EBS Volume that is used as the root device of a registered AMI. If the original snapshot was deleted, then the AMI would not be able to use it as the basis to create new instances. For this reason, AWS protects you from accidentally deleting the EBS Snapshot, since it could be critical to your systems. To delete an EBS Snapshot attached to a registered AMI, first remove the AMI, then the snapshot can be deleted.","ec2-instance-lifecycle#EC2 Instance Lifecycle:":"The following table highlights the many instance states that a VM can be in at a given time.\nInstance state Description Billing pending The instance is preparing to enter the running state. An instance enters the pending state when it launches for the first time, or when it is started after being in the stopped state. Not billed running The instance is running and ready for use. Billed stopping The instance is preparing to be stopped or stop-hibernated. Not billed if preparing to stop. Billed if preparing to hibernate stopped The instance is shut down and cannot be used. The instance can be started at any time. Not billed shutting-down The instance is preparing to be terminated. Not billed terminated The instance has been permanently deleted and cannot be started. Not billed Note: Reserved Instances that are terminated are billed until the end of their term.","ec2-instance-pricing#EC2 Instance Pricing:":"On-Demand instances are based on a fixed rate by the hour or second. As the name implies, you can start an On-Demand instance whenever you need one and can stop it when you no longer need it. There is no requirement for a long-term commitment. Reserved instances ensure that you keep exclusive use of an instance on 1 or 3 year contract terms. The long-term commitment provides significantly reduced discounts at the hourly rate. Spot instances take advantage of Amazon’s excess capacity and work in an interesting manner. In order to use them, you must financially bid for access. Because Spot instances are only available when Amazon has excess capacity, this option makes sense only if your app has flexible start and end times. You won’t be charged if your instance stops due to a price change (e.g., someone else just bid a higher price for the access) and so consequently your workload doesn’t complete. However, if you terminate the instance yourself you will be charged for any hour the instance ran. Spot instances are normally used in batch processing jobs.","ec2-key-details#EC2 Key Details:":"You can launch different types of instances from a single AMI. An instance type essentially determines the hardware of the host computer used for your instance. Each instance type offers different compute and memory capabilities. You should select an instance type based on the amount of memory and computing power that you need for the application or software that you plan to run on top of the instance. You can launch multiple instances of an AMI, as shown in the following figure: You have the option of using dedicated tenancy with your instance. This means that within an AWS data center, you have exclusive access to physical hardware. Naturally, this option incurs a high cost, but it makes sense if you work with technology that has a strict licensing policy. With EC2 VM Import, you can import existing VMs into AWS as long as those hosts use VMware ESX, VMware Workstation, Microsoft Hyper-V, or Citrix Xen virtualization formats. When you launch a new EC2 instance, EC2 attempts to place the instance in such a way that all of your VMs are spread out across different hardware to limit failure to a single location. You can use placement groups to influence the placement of a group of interdependent instances that meet the needs of your workload. There is an explanation about placement groups in a section below. When you launch an instance in Amazon EC2, you have the option of passing user data to the instance when the instance starts. This user data can be used to run common automated configuration tasks or scripts. For example, you can pass a bash script that ensures htop is installed on the new EC2 host and is always active. By default, the public IP address of an EC2 Instance is released when the instance is stopped even if its stopped temporarily. Therefore, it is best to refer to an instance by its external DNS hostname. If you require a persistent public IP address that can be associated to the same instance, use an Elastic IP address which is basically a static IP address instead. If you have requirements to self-manage a SQL database, EC2 can be a solid alternative to RDS. To ensure high availability, remember to have at least one other EC2 Instance in a separate Availability zone so even if a DB instance goes down, the other(s) will still be available. A golden image is simply an AMI that you have fully customized to your liking with all necessary software/data/configuration details set and ready to go once. This personal AMI can then be the source from which you launch new instances. Instance status checks check the health of the running EC2 server, systems status check monitor the health of the underlying hypervisor. If you ever notice a systems status issue, just stop the instance and start it again (no need to reboot) as the VM will start up again on a new hypervisor.","ec2-placement-groups#EC2 Placement Groups:":"Placement groups balance the tradeoff between risk tolerance and network performance when it comes to your fleet of EC2 instances. The more you care about risk, the more isolated you want your instances to be from each other. The more you care about performance, the more conjoined you want your instances to be with each other.\nThere are three different types of EC2 placement groups:\n1.) Clustered Placement Groups\nClustered Placement Grouping is when you put all of your EC2 instances in a single availability zone. This is recommended for applications that need the lowest latency possible and require the highest network throughput. Only certain instances can be launched into this group (compute optimized, GPU optimized, storage optimized, and memory optimized). 2.) Spread Placement Groups\nSpread Placement Grouping is when you put each individual EC2 instance on top of its own distinct hardware so that failure is isolated. Your VMs live on separate racks, with separate network inputs and separate power requirements. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. 3.) Partitioned Placement Groups\nPartitioned Placement Grouping is similar to Spread placement grouping, but differs because you can have multiple EC2 instances within a single partition. Failure instead is isolated to a partition (say 3 or 4 instances instead of 1), yet you enjoy the benefits of close proximity for improved network performance. With this placement group, you have multiple instances living together on the same hardware inside of different availability zones across one or more regions. If you would like a balance of risk tolerance and network performance, use Partitioned Placement Groups. Each placement group name within your AWS must be unique\nYou can move an existing instance into a placement group guaranteed that it is in a stopped state. You can move the instance via the CLI or an AWS SDK, but not the console. You can also take a snapshot of the existing instance, convert it into an AMI, and launch it into the placement group where you desire it to be.","ec2-security#EC2 Security:":"When you deploy an Amazon EC2 instance, you are responsible for management of the guest operating system (including updates and security patches), any application software or utilities installed on the instances, and the configuration of the AWS-provided firewall (called a security group) on each instance. With EC2, termination protection of the instance is disabled by default. This means that you do not have a safe-guard in place from accidentally terminating your instance. You must turn this feature on if you want that extra bit of protection. Amazon EC2 uses public–key cryptography to encrypt and decrypt login information. Public–key cryptography uses a public key to encrypt a piece of data, such as a password, and the recipient uses their private key to decrypt the data. The public and private keys are known as a key pair. You can encrypt your root device volume which is where you install the underlying OS. You can do this during creation time of the instance or with third-party tools like bit locker. Of course, additional or secondary EBS volumes are also encryptable as well. By default, an EC2 instance with an attached AWS Elastic Block Store (EBS) root volume will be deleted together when the instance is terminated. However, any additional or secondary EBS volume that is also attached to the same instance will be preserved. This is because the root EBS volume is for OS installations and other low-level settings. This rule can be modified, but it is usually easier to boot a new instance with a fresh root device volume than make use of an old one.","ec2-simplified#EC2 Simplified:":"EC2 spins up resizable server instances that can scale up and down quickly. An instance is a virtual server in the cloud. With Amazon EC2, you can set up and configure the operating system and applications that run on your instance. Its configuration at launch is a live copy of the Amazon Machine Image (AMI) that you specify when you launched the instance. EC2 has an extremely reduced time frame for provisioning and booting new instances and EC2 ensures that you pay as you go, pay for what you use, pay less as you use more, and pay even less when you reserve capacity. When your EC2 instance is running, you are charged on CPU, memory, storage, and networking. When it is stopped, you are only charged for EBS storage.","efs-key-details#EFS Key Details:":"In EFS, storage capacity is elastic (grows and shrinks automatically) and its size changes based on adding or removing files. While EBS mounts one EBS volume to one instance, you can attach one EFS volume across multiple EC2 instances. The EC2 instances communicate to the remote file system using the NFSv4 protocol. This makes it required to open up the NFS port for our security group (EC2 firewall rules) to allow inbound traffic on that port. Within an EFS volume, the mount target state will let you know what instances are available for mounting With EFS, you only pay for the storage that you use so you pay as you go. No pre-provisioning required. EFS can scale up to the petabytes and can support thousands of concurrent NFS connections. Data is stored across multiple AZs in a region and EFS ensures read after write consistency. It is best for file storage that is accessed by a fleet of servers rather than just one server","efs-simplified#EFS Simplified:":"EFS provides a simple and fully managed elastic NFS file system for use within AWS. EFS automatically and instantly scales your file system storage capacity up or down as you add or remove files without disrupting your application.","elastic-block-store-ebs#Elastic Block Store (EBS)":"","elastic-compute-cloud-ec2#Elastic Compute Cloud (EC2)":"","elastic-file-system-efs#Elastic File System (EFS)":"","elastic-load-balancers-elb#Elastic Load Balancers (ELB)":"","elastic-network-interfaces-eni#Elastic Network Interfaces (ENI)":"","elasticache#ElastiCache":"","elasticache-key-details#ElastiCache Key Details:":"The service is great for improving the performance of web applications by allowing you to receive information locally instead of relying solely on relatively distant DBs.\nAmazon ElastiCache offers fully managed Redis and Memcached for the most demanding applications that require sub-millisecond response times.\nFor data that doesn’t change frequently and is often asked for, it makes a lot of sense to cache said data rather than querying it from the database.\nCommon configurations that improve DB performance include introducing read replicas of a DB primary and inserting a caching layer into the storage architecture.\nMemcacheD is for simple caching purposes with horizontal scaling and multi-threaded performance, but if you require more complexity for your caching environment then choose Redis.\nA further comparison between MemcacheD and Redis for ElastiCache: Another advantage of using ElastiCache is that by caching query results, you pay the price of the DB query only once without having to re-execute the query unless the data changes.\nAmazon ElastiCache can scale-out, scale-in, and scale-up to meet fluctuating application demands. Write and memory scaling is supported with sharding. Replicas provide read scaling.","elasticache-simplified#ElastiCache Simplified:":"The ElastiCache service makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It helps you boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores.","elasticbeanstalk#ElasticBeanstalk":"","elasticbeanstalk-key-details#ElasticBeanstalk Key Details:":"Just upload your application and ElasticBeanstalk will take care of the underlying infrastructure. ElasticBeanstalk has capacity provisioning, meaning you can use it with autoscaling from the get-go. ElasticBeanstalk applies updates to your application by having a duplicate ready with the already updated version. This duplicate is then swapped with the original. This is done as a preventative measure in case your updated application fails. If the app does fail, ElasticBeanstalk will switch back to the original copy with the older version and there will be no downtime experienced by the users who are using your application. You can use ElasticBeanstalk to even host Docker as Elastic Beanstalk supports the deployment of web applications from containers. With Docker containers, you can define your own runtime environment, your own platform, programming language, and any application dependencies (such as package managers or tools) that aren’t supported by other platforms. ElasticBeanstalk makes it easy to deploy Docker as Docker containers are already self-contained and include all the configuration information and software required to run.","elasticbeanstalk-simplified#ElasticBeanstalk Simplified:":"ElasticBeanstalk is another way to script out your provisioning process by deploying existing applications to the cloud. ElasticBeanstalk is aimed toward developers who know very little about the cloud and want the simplest way of deploying their code.","elb-advanced-features#ELB Advanced Features:":"To enable IPv6 DNS resolution, you need to create a second DNS resource record so that the ALIAS AAAA record resolves to the load balancer along with the IPv4 record. The X-Forwarded-For header, via the Proxy Protocol, is simply the idea for load balancers to forward the requester’s IP address along with the actual request for information from the servers behind the LBs. Normally, the servers behind the LBs only see that the IP sending it traffic belongs to the Load Balancer. They usually have no idea about the true origin of the request as they only know about the computer (the LB) that asks them to do something. But sometimes we may want to route the original IP to the backend servers for specific use cases and have the LB’s IP address ignored. The X-Forwarded-For header makes this possible. Sticky Sessions bind a given user to a specific instance throughout the duration of their stay on the application or website. This means all of their interactions with the application will be directed to the same host each time. If you need local disk for your application to work, sticky sessions are great as users are guaranteed consistent access to the same ephemeral storage on a particular instance. The downside of sticky sessions is that, if done improperly, it can defeat the purpose of load balancing. All traffic could hypothetically be bound to the same instance instead of being evenly distributed. Path Patterns create a listener with rules to forward requests based on the URL path set within those user requests. This method, known as path-based routing, ensures that traffic can be specifically directed to multiple back-end services. For example, with Path Patterns you can route general requests to one target group and requests to render images to another target group. So the URL, “www.example.com/” will be forwarded to a server that is used for general content while “www.example.com/photos” will be forwarded to another server that renders images.","elb-cross-zone-load-balancing#ELB Cross Zone Load Balancing:":"Cross Zone load balancing guarantees even distribution across AZs rather than just within a single AZ. If Cross Zone load balancing is disabled, each load balancer node distributes requests evenly across the registered instances in its Availability Zone only. Cross Zone load balancing reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone, and improves your application’s ability to handle the loss of one or more instances. However, it is still recommend that you maintain approximately equivalent numbers of instances in each enabled Availability Zone for higher fault tolerance. For environments where clients cache DNS lookups, incoming requests might favor one of the Availability Zones. Using Cross Zone load balancing, this imbalance in the request load is spread across all available instances in the region instead.","elb-key-details#ELB Key Details:":"Load balancers can be internet facing or application internal. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an Alias record that points to your load balancer. An Alias record is preferable over a CName, but both can work. ELBs do not have predefined IPv4 addresses; you must resolve them with DNS instead. Your load balancer will never have its own IP by default, but you can create a static IP for a network load balancer because network LBs are for high performance purposes. Instances behind the ELB are reported as InService or OutOfService. When an EC2 instance behind an ELB fails a health check, the ELB stops sending traffic to that instance. A dual stack configuration for a load balancer means load balancing over IPv4 and IPv6 In AWS, there are three types of LBs: Application LBs Network LBs Classic LBs. Application LBs are best suited for HTTP(S) traffic and they balance load on layer 7. They are intelligent enough to be application aware and Application Load Balancers also support path-based routing, host-based routing and support for containerized applications. As an example, if you change your web browser’s language into French, an Application LB has visibility of the metadata it receives from your browser which contains details about the language you use. To optimize your browsing experience, it will then route you to the French-language servers on the backend behind the LB. You can also create advanced request routing, moving traffic into specific servers based on rules that you set yourself for specific cases. Network LBs are best suited for TCP traffic where performance is required and they balance load on layer 4. They are capable of managing millions of requests per second while maintaining extremely low latency. Classic LBs are the legacy ELB produce and they balance either on HTTP(S) or TCP, but not both. Even though they are the oldest LBs, they still support features like sticky sessions and X-Forwarded-For headers. If you need flexible application management and TLS termination then you should use the Application Load Balancer. If extreme performance and a static IP is needed for your application then you should use the Network Load Balancer. If your application is built within the EC2 Classic network then you should use the Classic Load Balancer. The lifecycle of a request to view a website behind an ELB: The browser requests the IP address for the load balancer from DNS. DNS provides the IP. With the IP at hand, your browser then makes an HTTP request for an HTML page from the Load Balancer. AWS perimeter devices checks and verifies your request before passing it onto the LB. The LB finds an active webserver to pass on the HTTP request. The webserver returns the requested HTML file. The browser receives the HTML file it requested and renders the graphical representation of it on the screen. Load balancers are a regional service. They do not balance load across different regions. You must provision a new ELB in each region that you operate out of. If your application stops responding, you’ll receive a 504 error when hitting your load balancer. This means the application is having issues and the error could have bubbled up to the load balancer from the services behind it. It does not necessarily mean there’s a problem with the LB itself.","elb-security#ELB Security:":"ELB supports SSL/TLS \u0026 HTTPS termination. Termination at load balancer is desired because decryption is resource and CPU intensive. Putting the decryption burden on the load balancer enables the EC2 instances to spend their processing power on application tasks, which helps improve overall performance. Elastic Load Balancers (along with CloudFront) support Perfect Forward Secrecy. This is a feature that provides additional safeguards against the eavesdropping of encrypted data in transit through the use of a uniquely random session key. This is done by ensuring that the in-use part of an encryption system automatically and frequently changes the keys it uses to encrypt and decrypt information. So if this latest key is compromised at all, it will only expose a small portion of the user’s recent data. Classic Load Balancers do not support Server Name Indication (SNI). SNI allows the server (the LB in this case) to safely host multiple TLS Certificates for multiple sites all under a single IP address (the Alias record or CName record in this case). To allow SNI, you have to use an Application Load Balancer instead or use it with a CloudFront web distribution.","elb-simplified#ELB Simplified:":"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, Docker containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault tolerant.","eni-key-details#ENI Key Details:":"ENI is used mainly for low-budget, high-availability network solutions However, if you suspect you need high network throughput then you can use Enhanced Networking ENI. Enhanced Networking ENI uses single root I/O virtualization to provide high-performance networking capabilities on supported instance types. SR-IOV provides higher I/O and lower throughput and it ensures higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. SR-IOV does this by dedicating the interface to a single instance and effectively bypassing parts of the Hypervisor which allows for better performance. Adding more ENIs won’t necessarily speed up your network throughput, but Enhanced Networking ENI will. There is no extra charge for using Enhanced Networking ENI and the better network performance it provides. The only downside is that Enhanced Networking ENI is not available on all EC2 instance families and types. You can attach a network interface to an EC2 instance in the following ways: When it’s running (hot attach) When it’s stopped (warm attach) When the instance is being launched (cold attach). If an EC2 instance fails with ENI properly configured, you (or more likely,the code running on your behalf) can attach the network interface to a hot standby instance. Because ENI interfaces maintain their own private IP addresses, Elastic IP addresses, and MAC address, network traffic will begin to flow to the standby instance as soon as you attach the network interface on the replacement instance. Users will experience a brief loss of connectivity between the time the instance fails and the time that the network interface is attached to the standby instance, but no changes to the VPC route table or your DNS server are required. For instances that work with Machine Learning and High Performance Computing, use EFA (Elastic Fabric Adaptor). EFAs accelerate the work required from the above use cases. EFA provides lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based High Performance Computing systems. EFA can also use OS-bypass (on linux only) that will enable ML and HPC applications to interface with the Elastic Fabric Adaptor directly, rather than be normally routed to it through the OS. This gives it a huge performance increase. You can enable a VPC flow log on your network interface to capture information about the IP traffic going to and from a network interface.","eni-simplified#ENI Simplified:":"An elastic network interface is a networking component that represents a virtual network card. When you provision a new instance, there will be an ENI attached automatically and you can create and configure additional network interfaces if desired. When you move a network interface from one instance to another, network traffic is redirected to the new instance.","exam-content-breakdown#Exam Content Breakdown:":"Domain 1: Design Resilient Architectures\n1.1 - Design a multi-tier architecture solution\n1.2 - Design highly available and/or fault-tolerant architectures\n1.3 - Design decoupling mechanisms using AWS services\n1.4 - Choose appropriate resilient storage\nDomain 2: Design High-Performing Architectures\n2.1 - Identify elastic and scalable compute solutions for a workload\n2.2 - Select high-performing and scalable storage solutions for a workload\n2.3 - Select high-performing networking solutions for a workload\n2.4 - Choose high-performing database solutions for a workload\nDomain 3: Design Secure Applications and Architectures\n3.1 - Design secure access to AWS resources\n3.2 - Design secure application tiers\n3.3 - Select appropriate data security options\nDomain 4: Design Cost-Optimized Architectures\n4.1 - Identify cost-effective storage solutions\n4.2 - Identify cost-effective compute and database services\n4.3 - Design cost-optimized network architectures","iam-entities#IAM Entities:":"Users - any individual end user such as an employee, system architect, CTO, etc.\nGroups - any collection of similar people with shared permissions such as system administrators, HR employees, finance teams, etc. Each user within their specified group will inherit the permissions set for the group.\nRoles - any software service that needs to be granted permissions to do its job, e.g- AWS Lambda needing write permissions to S3 or a fleet of EC2 instances needing read permissions from a RDS MySQL database.\nPolicies - the documented rule sets that are applied to grant or limit access. In order for users, groups, or roles to properly set permissions, they use policies. Policies are written in JSON and you can either use custom policies for your specific needs or use the default policies set by AWS.\nIAM Policies are separated from the other entities above because they are not an IAM Identity. Instead, they are attached to IAM Identities so that the IAM Identity in question can perform its necessary function.","iam-key-details#IAM Key Details:":"IAM is a global AWS services that is not limited by regions. Any user, group, role or policy is accessible globally.\nThe root account with complete admin access is the account used to sign up for AWS. Therefore, the email address used to create the AWS account for use should probably be the official company email address.\nNew users have no permissions when their accounts are first created. This is a secure way of delegating access as permissions must be intentionally granted.\nWhen joining the AWS ecosystem for the first time, new users are supplied an access key ID and a secret access key ID when you grant them programmatic access. These are created just once specifically for the new user to join, so if they are lost simply generate a new access key ID and a new secret access key ID. Access keys are only used for the AWS CLI and SDK so you cannot use them to access the console.\nWhen creating your AWS account, you may have an existing identity provider internal to your company that offers Single Sign On (SSO). If this is the case, it is useful, efficient, and entirely possible to reuse your existing identities on AWS. To do this, you let an IAM role be assumed by one of the Active Directories. This is because the IAM ID Federation feature allows an external service to have the ability to assume an IAM role.\nIAM Roles can be assigned to a service, such as an EC2 instance, prior to its first use/creation or after its been in used/created. You can change permissions as many times as you need. This can all be done by using both the AWS console and the AWS command line tools.\nYou cannot nest IAM Groups. Individual IAM users can belong to multiple groups, but creating subgroups so that one IAM Group is embedded inside of another IAM Group is not possible.\nWith IAM Policies, you can easily add tags that help define which resources are accessible by whom. These tags are then used to control access via a particular IAM policy. For example, production and development EC2 instances might be tagged as such. This would ensure that people who should only be able to access development instances cannot access production instances.","iam-simplified#IAM Simplified:":"IAM offers a centralized hub of control within AWS and integrates with all other AWS Services. IAM comes with the ability to share access at various levels of permission and it supports the ability to use identity federation (the process of delegating authentication to a trusted external party like Facebook or Google) for temporary or limited access. IAM comes with MFA support and allows you to set up custom password rotation policy across your entire organization. It is also PCI DSS compliant i.e. payment card industry data security standard. (passes government mandated credit card security regulations).","identity-access-management-iam#Identity Access Management (IAM)":"","internet-gateway#Internet Gateway:":"If the Internet Gateway is not attached to the VPC, which is the prerequisite for instances to be accessed from the internet, then naturally instances in your VPC will not be reachable. If you want all of your VPC to remain private (and not just some subnets), then do not attach an IGW. When a Public IP address is assigned to an EC2 instance, it is effectively registered by the Internet Gateway as a valid public endpoint. However, each instance is only aware of its private IP and not its public IP. Only the IGW knows of the public IPs that belong to instances. When an EC2 instance initiates a connection to the public internet, the request is sent using the public IP as its source even though the instance doesn’t know a thing about it. This works because the IGW performs its own NAT translation where private IPs are mapped to public IPs and vice versa for traffic flowing into and out of the VPC. So when traffic from the internet is destined for an instance’s public IP endpoint, the IGW receives it and forwards the traffic onto the EC2 instance using its internal private IP. You can only have one IGW per VPC. Summary: IGW connects your VPC with the internet.","introduction#Introduction":"The official AWS Solutions Architect - Associate (SAA-C02) exam guide","kinesis#Kinesis":"","kinesis-key-details#Kinesis Key Details:":"Amazon Kinesis makes it easy to load and analyze the large volumes of data entering AWS.\nKinesis is used for processing real-time data streams (data that is generated continuously) from devices constantly sending data into AWS so that said data can be collected and analyzed.\nIt is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.\nThere are three different types of Kinesis:\nKinesis Streams\nKinesis Streams works where the data producers stream their data into Kinesis Streams which can retain the data that enters it from one day up until 7 days. Once inside Kinesis Streams, the data is contained within shards. Kinesis Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events. For example: purchase requests from a large online store like Amazon, stock prices, Netflix content, Twitch content, online gaming data, Uber positioning and directions, etc. Kinesis Firehose\nAmazon Kinesis Firehose is the easiest way to load streaming data into data stores and analytics tools. When data is streamed into Kinesis Firehose, there is no persistent storage there to hold onto it. The data has to be analyzed as it comes in so it’s optional to have Lambda functions inside your Kinesis Firehose. Once processed, you send the data elsewhere. Kinesis Firehose can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. Kinesis Analytics\nKinesis Analytics works with both Kinesis Streams and Kinesis Firehose and can analyze data on the fly. The data within Kinesis Analytics also gets sent elsewhere once it is finished processing. It analyzes your data inside of the Kinesis service itself. Partition keys are used with Kinesis so you can organize data by shard. This way, input from a particular device can be assigned a key that will limit its destination to a specific shard.\nPartition keys are useful if you would like to maintain order within your shard.\nConsumers, or the EC2 instances that read from Kinesis Streams, can go inside the shards to analyze what is in there. Once finished analyzing or parsing the data, the consumers can then pass on the data to a number of places for storage like a DB or S3.\nThe total capacity of a Kinesis stream is the sum of data within its constituent shards.\nYou can always increase the write capacity assigned to your shard table.","kinesis-simplified#Kinesis Simplified:":"Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.","lambda#Lambda":"","lambda-key-details#Lambda Key Details:":"Lambda is a compute service where you upload your code as a function and AWS provisions the necessary details underneath the function so that the function executes successfully. AWS Lambda is the ultimate abstraction layer. You only worry about code, AWS does everything else. Lambda supports Go, Python, C#, PowerShell, Node.js, and Java Each Lambda function maps to one request. Lambda scales horizontally automatically. Lambda is priced on the number of requests and the first one million are free. Each million afterwards is $0.20. Lambda is also priced on the runtime of your code, rounded up to the nearest 100mb, and the amount of memory your code allocates. Lambda works globally. Lambda functions can trigger other Lambda functions. You can use Lambda as an event-driven service that executes based on changes in your AWS ecosystem. You can also use Lambda as a handler in response to HTTP events via API calls over the AWS SDK or API Gateway. When you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code.\nThe first time you create or update Lambda functions that use environment variables in a region, a default service key is created for you automatically within AWS KMS. This key is used to encrypt environment variables. However, if you wish to use encryption helpers and use KMS to encrypt environment variables after your Lambda function is created, you must create your own AWS KMS key and choose it instead of the default key.\nTo enable your Lambda function to access resources inside a private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within a private VPC.\nAWS X-Ray allows you to debug your Lambda function in case of unexpected behavior.","lambda-simplified#Lambda Simplified:":"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. You upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to be automatically triggered from other AWS services or be called directly from any web or mobile app.","lambdaedge#Lambda@Edge:":"You can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers. It adds compute capacity to your CloudFront edge locations and allows you to execute the functions in AWS locations closer to your application’s viewers. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points: After CloudFront receives a request from a viewer (viewer request) Before CloudFront forwards the request to the origin (origin request) After CloudFront receives the response from the origin (origin response) Before CloudFront forwards the response to the viewer (viewer response) You’d use Lambda@Edge to simplify and reduce origin infrastructure.","maximizing-s3-readwrite-performance#Maximizing S3 Read/Write Performance:":"If the request rate for reading and writing objects to S3 is extremely high, you can use sequential date-based naming for your prefixes to improve performance. Earlier versions of the AWS Docs also suggested to use hash keys or random strings to prefix the object’s name. In such cases, the partitions used to store the objects will be better distributed and therefore will allow better read/write performance on your objects. If your S3 data is receiving a high number of GET requests from users, you should consider using Amazon CloudFront for performance optimization. By integrating CloudFront with S3, you can distribute content via CloudFront’s cache to your users for lower latency and a higher data transfer rate. This also has the added bonus of sending fewer direct requests to S3 which will reduce costs. For example, suppose that you have a few objects that are very popular. CloudFront fetches those objects from S3 and caches them. CloudFront can then serve future requests for the objects from its cache, reducing the total number of GET requests it sends to Amazon S3. More information on how to ensure high performance in S3","miscellaneous#Miscellaneous":"The following section includes services, features, and techniques that may appear on the exam. They are also extremely useful to know as an engineer using AWS. If the following items do appear on the exam, they will not be tested in detail. You’ll just have to know what the meaning is behind the name. It is a great idea to learn each item in depth for your career’s benefit, but it is not necessary for the exam.","nat-instances-vs-nat-gateways#NAT Instances vs. NAT Gateways:":"Attaching an Internet Gateway to a VPC allows instances with public IPs to directly access the internet. NAT does a similar thing, however it is for instances that do not have a public IP. It serves as an intermediate step which allow private instances to first masked their own private IP as the NAT’s public IP before accessing the internet. You would want your private instances to access the internet so that they can have normal software updates. NAT prevents any initiating of a connection from the internet. NAT instances are individual EC2 instances that perform the function of providing private subnets a means to securely access the internet. Because they are individual instances, High Availability is not a built-in feature and they can become a choke point in your VPC. They are not fault-tolerant and serve as a single point of failure. While it is possible to use auto-scaling groups, scripts to automate failover, etc. to prevent bottlenecking, it is far better to use the NAT Gateway as an alternative for a scalable solution. NAT Gateway is a managed service that is composed of multiple instances linked together within an availability zone in order to achieve HA by default. To achieve further HA and a zone-independent architecture, create a NAT gateway for each Availability Zone and configure your routing to ensure that resources use the NAT gateway in their corresponding Availability Zone. NAT instances are deprecated, but still useable. NAT Gateways are the preferred means to achieve Network Address Translation. There is no need to patch NAT Gateways as the service is managed by AWS. You do need to patch NAT Instances though because they’re just individual EC2 instances. Because communication must always be initiated from your private instances, you need a route rule to route traffic from a private subnet to your NAT gateway. Your NAT instance/gateway will have to live in a public subnet as your public subnet is the subnet configured to have internet access. When creating NAT instances, it is important to remember that EC2 instances have source/destination checks on them by default. What these checks do is ensure that any traffic it comes across must be either generated by the instance or be the intended recipient of that traffic. Otherwise, the traffic is dropped because the EC2 instance is neither the source nor the destination. So because NAT instances act as a sort of proxy, you must disable source/destination checks when musing a NAT instance.","network-access-control-lists#Network Access Control Lists:":"Network Access Control Lists (or NACLs) are like security groups but for subnets rather than instances. The main difference between security groups and NACLs is that security groups are stateful, meaning you can perform both allow and deny rules that may be divergent, depending if traffic is inbound or outbound, for that rule. The following table highlights the differences between NACLs and Subnets. NACL Security Group Operates at the subnet level Operates at the instance level Supports allow rules and deny rules Supports allow rules only Is stateless: Return traffic must be explicitly allowed by rules Is stateful: Return traffic is automatically allowed, regardless of any rules We process rules in order, starting with the lowest numbered rule, when deciding whether to allow traffic We evaluate all rules before deciding whether to allow traffic Automatically applies to all instances in the subnets that it’s associated with (therefore, it provides an additional layer of defense if the security group rules are too permissive) Applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on Because NACLs are stateless, you must also ensure that outbound rules exist alongside the inbound rules so that ingress and egress can flow smoothly.\nThe default NACL that comes with a new VPC has a default rule to allow all inbounds and outbounds. This means that it exists, but doesn’t do anything as all traffic passes through it freely.\nHowever, when you create a new NACL (instead of using the default that comes with the VPC) the default rules will deny all inbounds and outbounds.\nIf you create a new NACL, you must associate whichever desired subnets to it manually so that they can inherit the NACL’s rule set. If you don’t explicitly assign a subnet to an NACL, AWS will associate it with your default NACL.\nNACLs are evaluated before security groups and you block malicious IPs with NACLs, not security groups.\nA subnet can only follow the rules listed by one NACL at a time. However, a NACL can describe the rules for any number of subnets. The rules will take effect immediately.\nNetwork ACL rules are evaluated by rule number, from lowest to highest, and executed immediately when a matching allow/deny rule is found. Because of this, order matters with your rule numbers.\nThe lower the number of a rule on the list, the more seniority that rule will have. List your rules accordingly.\nIf you are using NAT Gateway along with your NACL, you must ensure the availability of the NAT Gateway ephemeral port range within the rules of your NACL. Because NAT Gateway traffic can appear on any of range’s ports for the duration of its connection, you must ensure that all possible ports are accounted for and open.\nNACL can have a small impact on how EC2 instances in a private subnet will communicate with any service, including VPC Endpoints.","priority-levels-in-iam#Priority Levels in IAM:":"Explicit Deny: Denies access to a particular resource and this ruling cannot be overruled.\nExplicit Allow: Allows access to a particular resource so long as there is not an associated Explicit Deny.\nDefault Deny (or Implicit Deny): IAM identities start off with no resource access. Access instead must be granted.","rds-backups#RDS Backups:":"When it comes to RDS, there are two kinds of backups: automated backups database snapshots Automated backups allow you to recover your database to any point in time within a retention period (between one and 35 days). Automated backups will take a full daily snapshot and will also store transaction logs throughout the day. When you perform a DB recovery, RDS will first choose the most recent daily backup and apply the relevant transaction logs from that day. Within the set retention period, this gives you the ability to do a point in time recovery down to the precise second. Automated backups are enabled by default. The backup data is stored freely up to the size of your actual database (so for every GB saved in RDS, that same amount will freely be stored in S3 up until the GB limit of the DB). Backups are taken within a defined window so latency might go up as storage I/O is suspended in order for the data to be backed up. DB snapshots are done manually by the administrator. A key different from automated backups is that they are retained even after the original RDS instance is terminated. With automated backups, the backed up data in S3 is wiped clean along with the RDS engine. This is why you are asked if you want to take a final snapshot of your DB when you go to delete it. When you go to restore a DB via automated backups or DB snapshots, the result is the provisioning of an entirely new RDS instance with its own DB endpoint in order to be reached.","rds-enhanced-monitoring#RDS Enhanced Monitoring:":"RDS comes with an Enhanced Monitoring feature. Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console, or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the RDS OS Metrics log group in the CloudWatch console. Take note that there are key differences between CloudWatch and Enhanced Monitoring Metrics. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work that can be picked up and interpreted as part of the metric.","rds-key-details#RDS Key Details:":"RDS comes in six different flavors: SQL Server Oracle MySQL Server PostgreSQL MariaDB Aurora Think of RDS as the DB engine in which various DBs sit on top of. RDS has two key features when scaling out: Read replication for improved performance Multi-AZ for high availability In the database world, Online Transaction Processing (OLTP) differs from Online Analytical Processing (OLAP) in terms of the type of querying that you would do. OLTP serves up data for business logic that ultimately composes the core functioning of your platform or application. OLAP is to gain insights into the data that you have stored in order to make better strategic decisions as a company. RDS runs on virtual machines, but you do not have access to those machines. You cannot SSH into an RDS instance so therefore you cannot patch the OS. This means that AWS is responsible for the security and maintenance of RDS. You can provision an EC2 instance as a database if you need or want to manage the underlying server yourself, but not with an RDS engine. Just because you cannot access the VM directly, it does not mean that RDS is serverless. There is Aurora serverless however (explained below) which serves a niche purpose. SQS queues can be used to store pending database writes if your application is struggling under a high write load. These writes can then be added to the database when the database is ready to process them. Adding more IOPS will also help, but this alone will not wholly eliminate the chance of writes being lost. A queue however ensures that writes to the DB do not become lost.","rds-multi-az#RDS Multi-AZ:":"Disaster recovery in AWS always looks to ensure standby copies of resources are maintained in a separate geographical area. This way, if a disaster (natural disaster, political conflict, etc.) ever struck where your original resources are, the copies would be unaffected. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. With a Multi-AZ configuration, EC2 connects to its RDS data store using a DNS address masked as a connection string. If the primary DB fails, Multi-AZ is smart enough to detect that failure and automatically update the DNS address to point at the secondary. No manual intervention is required and AWS takes care of swapping the IP address in DNS. Multi-AZ is supported for all DB flavors except aurora. This is because Aurora is completely fault-tolerant on its own. Multi-AZ feature allows for high availability across availability zones and not regions. During a failover, the recovered former primary becomes the new secondary and the promoted secondary becomes primary. Once the original DB is recovered, there will be a sync process kicked off where the two DBs mirror each other once to sync up on the new data that the failed former primary might have missed out on. You can force a failover for a Multi-AZ setup by rebooting the primary instance With a Multi-AZ RDS configuration, backups are taken from the standby.","rds-read-replicas#RDS Read Replicas:":"Read Replication is exclusively used for performance enhancement. With a Read Replica configuration, EC2 connects to the RDS backend using a DNS address and every write that is received by the master database is also passed onto a DB secondary so that it becomes a perfect copy of the master. This has the overall effect of reducing the number of transactions on the master because the secondary DBs can be queried for the same data. However, if the master DB were to fail, there is no automatic failover. You would have to manually create a new connection string to sync with one of the read replicas so that it becomes a master on its own. Then you’d have to update your EC2 instances to point at the read replica. You can have up to five copies of your master DB with read replication. You can promote read replicas to be their very own production database if needed. Read replicas are supported for all six flavors of DB on top of RDS. Each Read Replica will have its own DNS endpoint. Automated backups must be enabled in order to use read replicas. You can have read replicas with Multi-AZ turned on or have the read replica in an entirely separate region. You can have even have read replicas of read replicas, but watch out for latency or replication lag. The caveat for Read Replicas is that they are subject to small amounts of replication lag. This is because they might be missing some of the latest transactions as they are not updated as quickly as primaries. Application designers need to consider which queries have tolerance to slightly stale data. Those queries should be executed on the read replica, while those demanding completely up-to-date data should run on the primary node.","rds-security#RDS Security:":"You can authenticate to your DB instance using IAM database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string that Amazon RDS generates on request. Authentication tokens have a lifetime of 15 minutes. You don’t need to store user credentials in the database because authentication is managed externally using IAM. IAM database authentication provides the following benefits: Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance. For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security Encryption at rest is supported for all six flavors of DB for RDS. Encryption is done using the AWS KMS service. Once the RDS instance is encryption enabled, the data in the DB becomes encrypted as well as all backups (automated or snapshots) and read replicas. After your data is encrypted, Amazon RDS handles authentication of access and decryption of your data transparently with a minimal impact on performance. You don’t need to modify your database client applications to use encryption. Amazon RDS encryption is currently available for all database engines and storage types. However, you need to ensure that the underlying instance type supports DB encryption. You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created and DB instances that are encrypted can’t be modified to disable encryption.","rds-simplified#RDS Simplified:":"RDS is a managed service that makes it easy to set up, operate, and scale a relational database in AWS. It provides cost-efficient and resizable capacity while automating or outsourcing time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.","recommended-reading#Recommended Reading:":"You can cover a lot of ground by skimming over what you already know or what you can infer to be true. In particular, read the first sentence of each paragraph and if you have no uncertainty about what is being said in that sentence, move on to the first sentence of the next paragraph. Take notes whenever necessary.\nAWS Well-Architected Framework\nAmazon VPC FAQs\nAWS Autoscaling FAQs\nAmazon EC2 FAQs\nAmazon EC2 Auto Scaling FAQs Amazon EBS FAQs\nElastic network interfaces\nAmazon S3 FAQs\nElastic Load Balancing FAQs\nAmazon Route 53 FAQs\nAWS Storage Gateway FAQs\nAmazon EFS FAQs\nAmazon FSx for Windows File Server FAQs\nAmazon FSx for Lustre FAQs\nAWS Organizations FAQs","redshift#Redshift":"","redshift-enhanced-vpc-routing#Redshift Enhanced VPC Routing:":"When you use Amazon Redshift Enhanced VPC Routing, Redshift forces all traffic (such as COPY and UNLOAD traffic) between your cluster and your data repositories through your Amazon VPC. If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network. By using Enhanced VPC Routing, you can use standard VPC features, such as VPC security groups, network access control lists (ACLs), VPC endpoints, VPC endpoint policies, internet gateways, and Domain Name System (DNS) servers.","redshift-key-details#Redshift Key Details:":"An Amazon Redshift cluster is a set of nodes which consists of a leader node and one or more compute nodes. The type and number of compute nodes that you need depends on the size of your data, the number of queries you will execute, and the query execution performance that you need. Redshift is used for business intelligence and pulls in very large and complex datasets to perform complex queries in order to gather insights from the data. It fits the use case of Online Analytical Processing (OLAP). Redshift is a powerful technology for data discovery including capabilities for almost limitless report viewing, complex analytical calculations, and predictive “what if” scenario (budget, forecast, etc.) planning. Depending on your data warehousing needs, you can start with a small, single-node cluster and easily scale up to a larger, multi-node cluster as your requirements change. You can add or remove compute nodes to the cluster without any interruption to the service. If you intend to keep your cluster running for a year or longer, you can save money by reserving compute nodes for a one-year or three-year period. Snapshots are point-in-time backups of a cluster. These backups are enabled by default with a 1 day retention period. The maximum retention period is 35 days. Redshift can also asynchronously replicate your snapshots to a different region if desired. A Highly Available Redshift cluster would require 3 copies of your data. One copy would be live in Redshift and the others would be standby in S3. Redshift can have up to 128 compute nodes in a multi-node cluster. The leader node always manages client connections and relays queries to the compute nodes which store the actual data and perform the queries. Redshift is able to achieve efficiency despite the many parts and pieces in its architecture through using columnar compression of data stores that contain similar data. In addition, Redshift does not require indexes or materialized views which means it can be relatively smaller in size compared to an OLTP database containing the same amount of information. Finally, when loading data into a Redshift table, Redshift will automatically down sample the data and pick the most appropriate compression scheme. Redshift also comes with Massive Parallel Processing (MPP) in order to take advantage of all the nodes in your multi-node cluster. This is done by evenly distributing data and query load across all nodes. Because of this, scaling out still retains great performance. Redshift is encrypted in transit using SSL and is encrypted at rest using AES-256. By default, Redshift will manage all keys, but you can do so too via AWS CloudHSM or AWS KMS. Redshift is billed for: Compute Node Hours (total hours your non-leader nodes spent querying for data) Backups Data transfer within a VPC (but not outside of it) Redshift is not multi-AZ, if you want multi-AZ you will need to spin up a separate cluster ingesting the same input. You can also manually restore snapshots to a new AZ in the event of an outage. When you provision an Amazon Redshift cluster, it is locked down by default so nobody has access to it. To grant other users inbound access to an Amazon Redshift cluster, you associate the cluster with a security group. Amazon Redshift provides free storage for snapshots that is equal to the storage capacity of your cluster until you delete the cluster. After you reach the free snapshot storage limit, you are charged for any additional storage at the normal rate. Because of this, you should evaluate how many days you need to keep automated snapshots and configure their retention period accordingly, and delete any manual snapshots that you no longer need. Regardless of whether you enable automated snapshots, you can take a manual snapshot whenever you want. Amazon Redshift will never automatically delete a manual snapshot. Manual snapshots are retained even after you delete your Redshift cluster. Because manual snapshots accrue storage charges, it’s important that you manually delete them if you no longer need them","redshift-simplified#Redshift Simplified:":"Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. The Amazon Redshift service manages all of the work of setting up, operating, and scaling a data warehouse. These tasks include provisioning capacity, monitoring and backing up the cluster, and applying patches and upgrades to the Amazon Redshift engine.","redshift-spectrum#Redshift Spectrum:":"Amazon Redshift Spectrum is used to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. Redshift Spectrum queries employ massive parallelism to execute very fast against large datasets. Much of the processing occurs in the Redshift Spectrum layer, and most of the data remains in Amazon S3. Redshift Spectrum queries use much less of your cluster’s processing capacity than other queries. The cluster and the data files in Amazon S3 must be in the same AWS Region. External S3 tables are read-only. You can’t perform insert, update, or delete operations on external tables.","relational-database-service-rds#Relational Database Service (RDS)":"","route-tables#Route Tables:":"Route tables are used to make sure that subnets can communicate with each other and that traffic knows where to go. Every subnet that you create is automatically associated with the main route table for the VPC. You can have multiple route tables. If you do not want your new subnet to be associated with the default route table, you must specify that you want it associated with a different route table. Because of this default behavior, there is a potential security concern to be aware of: if the default route table is public then the new subnets associated with it will also be public. The best practice is to ensure that the default route table where new subnets are associated with is private. This means you ensure that there is no route out to the internet for the default route table. Then, you can create a custom route table that is public instead. New subnets will automatically have no route out to the internet. If you want a new subnet to be publicly accessible, you can simply associate it with the custom route table. Route tables can be configured to access endpoints (public services accessed privately) and not just the internet.","route53#Route53":"","route53-key-details#Route53 Key Details:":"DNS is used to map human-readable domain names into an internet protocol address similarly to how phone books map company names with phone numbers. AWS has its own domain registrar. When you buy a domain name, every DNS address starts with an SOA (Start of Authority) record. The SOA record stores information about the name of the server that kicked off the transfer of ownership, the administrator who will now use the domain, the current metadata available, and the default number of seconds or TTL. NS records, or Name Server records, are used by the Top Level Domain hosts (.org, .com, .uk, etc.) to direct traffic to the Content servers. The Content DNS servers contain the authoritative DNS records. Browsers talk to the Top Level Domains whenever they are queried and encounter domain name that they do not recognize. Browsers will ask for the authoritative DNS records associated with the domain. Because the Top Level Domain contains NS records, the TLD can in turn queries the Name Servers for their own SOA. Within the SOA, there will be the requested information. Once this information is collected, it will then be returned all the way back to the original browser asking for it. In summary: Browser -\u003e TLD -\u003e NS -\u003e SOA -\u003e DNS record. The pipeline reverses when the correct DNS record is found. Authoritative name servers store DNS record information, usually a DNS hosting provider or domain registrar like GoDaddy that offers both DNS registration and hosting. There are a multitude of DNS records for Route53. Here are some of the more common ones: A records: These are the fundamental type of DNS record. The “A” in A records stands for “address”. These records are used by a computer to directly pair a domain name to an IP address. IPv4 and IPv6 are both supported with “AAAA” referring to the IPv6 version. A: URL -\u003e IPv4 and AAAA: URL -\u003e IPv6. CName records: Also referred to as the Canonical Name. These records are used to resolve one domain name to another domain name. For example, the domain of the mobile version of a website may be a CName from the domain of the browser version of that same website rather than a separate IP address. This would allow mobile users who visit the site and to receive the mobile version. CNAME: URL -\u003e URL. Alias records: These records are used to map your domains to AWS resources such as load balancers, CDN endpoints, and S3 buckets. Alias records function similarly to CNames in the sense that you map one domain to another. The key difference though is that by pointing your Alias record at a service rather than a domain name, you have the ability to freely change your domain names if needed and not have to worry about what records might be mapped to it. Alias records give you dynamic functionality. Alias: URL -\u003e AWS Resource. PTR records: These records are the opposite of an A record. PTR records map an IP to a domain and they are used in reverse DNS lookups as a way to obtain the domain name of an IP address. PTR: IPv4 -\u003e URL. One other major difference between CNames and Alias records is that a CName cannot be used for the naked domain name (the apex record in your entire DNS configuration / the primary record to be used). CNames must always be secondary records that can map to another secondary record or the apex record. The primary must always be of type Alias or A Record in order to work. Due to the dynamic nature of Alias records, they are often recommended for most use cases and should be used when it is possible to. TTL is the length that a DNS record is cached on either the resolving servers or the users own cache so that a fresher mapping of IP to domain can be retrieved. Time To Live is measured in seconds and the lower the TTL the faster DNS changes propagate across the internet. Most providers, for example, have a TTL that lasts 48 hours. You can create health checks to send you a Simple Notification if any issues arise with your DNS setup. Further, Route53 health checks can be used for any AWS endpoint that can be accessed via the Internet. This makes it an ideal option for monitoring the health of your AWS endpoints.","route53-routing-policies#Route53 Routing Policies:":"When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to DNS queries. The routing policies available are: Simple Routing Weighted Routing Latency-based Routing Failover Routing Geolocation Routing Geo-proximity Routing Multivalue Answer Routing Simple Routing is used when you just need a single record in your DNS with either one or more IP addresses behind the record in case you want to balance load. If you specify multiple values in a Simple Routing policy, Route53 returns a random IP from the options available. Weighted Routing is used when you want to split your traffic based on assigned weights. For example, if you want 80% of your traffic to go to one AZ and the rest to go to another, use Weighted Routing. This policy is very useful for testing feature changes and due to the traffic splitting characteristics, it can double as a means to perform blue-green deployments. When creating Weighted Routing, you need to specify a new record for each IP address. You cannot group the various IPs under one record like with Simple Routing. Latency-based Routing, as the name implies, is based on setting up routing based on what would be the lowest latency for a given user. To use latency-based routing, you must create a latency resource record set in the same region as the corresponding EC2 or ELB resource receiving the traffic. When Route53 receives a query for your site, it selects the record set that gives the user the quickest speed. When creating Latency-based Routing, you need to specify a new record for each IP. Failover Routing is used when you want to configure an active-passive failover set up. Route53 will monitor the health of your primary so that it can failover when needed. You can also manually set up health checks to monitor all endpoints if you want more detailed rules. Geolocation Routing lets you choose where traffic will be sent based on the geographic location of your users. Geo-proximity Routing lets you choose where traffic will be sent based on the geographic location of your users and your resources. You can choose to route more or less traffic based on a specified weight which is referred to as a bias. This bias either expands or shrinks the availability of a geographic region which makes it easy to shift traffic from resources in one location to resources in another. To use this routing method, you must enable Route53 traffic flow. If you want to control global traffic, use Geo-proximity routing. If you want traffic to stay in a local region, use Geolocation routing. Multivalue Routing is pretty much the same as Simple Routing, but Multivalue Routing allows you to put health checks on each record set. This ensures then that only a healthy IP will be randomly returned rather than any IP.","route53-simplified#Route53 Simplified:":"Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service. You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.","s3-and-elasticsearch#S3 and ElasticSearch:":"If you are using S3 to store log files, ElasticSearch provides full search capabilities for logs and can be used to search through data stored in an S3 bucket. You can integrate your ElasticSearch domain with S3 and Lambda. In this setup, any new logs received by S3 will trigger an event notification to Lambda, which in turn will then run your application code on the new log data. After your code finishes processing, the data will be streamed into your ElasticSearch domain and be available for observation.","s3-cross-region-replication#S3 Cross Region Replication:":"Cross region replication only work if versioning is enabled. When cross region replication is enabled, no pre-existing data is transferred. Only new uploads into the original bucket are replicated. All subsequent updates are replicated. When you replicate the contents of one bucket to another, you can actually change the ownership of the content if you want. You can also change the storage tier of the new bucket with the replicated content. When files are deleted in the original bucket (via a delete marker as versioning prevents true deletions), those deletes are not replicated. Cross Region Replication Overview What is and isn’t replicated such as encrypted objects, deletes, items in glacier, etc.","s3-encryption#S3 Encryption:":"S3 data can be encrypted both in transit and at rest.\nEncryption In Transit: When the traffic passing between one endpoint to another is indecipherable. Anyone eavesdropping between server A and server B won’t be able to make sense of the information passing by. Encryption in transit for S3 is always achieved by SSL/TLS.\nEncryption At Rest: When the immobile data sitting inside S3 is encrypted. If someone breaks into a server, they still won’t be able to access encrypted info within that server. Encryption at rest can be done either on the server-side or the client-side. The server-side is when S3 encrypts your data as it is being written to disk and decrypts it when you access it. The client-side is when you personally encrypt the object on your own and then upload it into S3 afterwards.\nYou can encrypted on the AWS supported server-side in the following ways:\nS3 Managed Keys / SSE - S3 (server side encryption S3 ) - when Amazon manages the encryption and decryption keys for you automatically. In this scenario, you concede a little control to Amazon in exchange for ease of use. AWS Key Management Service / SSE - KMS - when Amazon and you both manage the encryption and decryption keys together. Server Side Encryption w/ customer provided keys / SSE - C - when I give Amazon my own keys that I manage. In this scenario, you concede ease of use in exchange for more control.","s3-event-notications#S3 Event Notications:":"The Amazon S3 notification feature enables you to receive and send notifications when certain events happen in your bucket. To enable notifications, you must first configure the events you want Amazon S3 to publish (new object added, old object deleted, etc.) and the destinations where you want Amazon S3 to send the event notifications. Amazon S3 supports the following destinations where it can publish events:\nAmazon Simple Notification Service (Amazon SNS) - A web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. Amazon Simple Queue Service (Amazon SQS) - SQS offers reliable and scalable hosted queues for storing messages as they travel between computers. AWS Lambda - AWS Lambda is a compute service where you can upload your code and the service can run the code on your behalf using the AWS infrastructure. You package up and upload your custom code to AWS Lambda when you create a Lambda function. The S3 event triggering the Lambda function also can serve as the code’s input.","s3-key-details#S3 Key Details:":"Objects (regular files or directories) are stored in S3 with a key, value, version ID, and metadata. They can also contain torrents and sub resources for access control lists which are basically permissions for the object itself.\nThe data consistency model for S3 ensures immediate read access for new objects after the initial PUT requests. These new objects are introduced into AWS for the first time and thus do not need to be updated anywhere so they are available immediately.\nThe data consistency model for S3 also ensures immediate read access for PUTS and DELETES of already existing objects, since Decembre 2020.\nAmazon guarantees 99.999999999% (or 11 9s) durability for all S3 storage classes except its Reduced Redundancy Storage class.\nS3 comes with the following main features:\n1.) tiered storage and pricing variability\n2.) lifecycle management to expire older content\n3.) versioning for version control\n4.) encryption for privacy\n5.) MFA deletes to prevent accidental or malicious removal of content\n6.) access control lists \u0026 bucket policies to secure the data\nS3 charges by:\n1.) storage size\n2.) number of requests\n3.) storage management pricing (known as tiers)\n4.) data transfer pricing (objects leaving/entering AWS via the internet)\n5.) transfer acceleration (an optional speed increase for moving objects via Cloudfront)\n6.) cross region replication (more HA than offered by default\nBucket policies secure data at the bucket level while access control lists secure data at the more granular object level.\nBy default, all newly created buckets are private.\nS3 can be configured to create access logs which can be shipped into another bucket in the current account or even a separate account all together. This makes it easy to monitor who accesses what inside S3.\nThere are 3 different ways to share S3 buckets across AWS accounts:\n1.) For programmatic access only, use IAM \u0026 Bucket Policies to share entire buckets\n2.) For programmatic access only, use ACLs \u0026 Bucket Policies to share objects\n3.) For access via the console \u0026 the terminal, use cross-account IAM roles\nS3 is a great candidate for static website hosting. When you enable static website hosting for S3 you need both an index.html file and an error.html file. Static website hosting creates a website endpoint that can be accessed via the internet.\nWhen you upload new files and have versioning enabled, they will not inherit the properties of the previous version.","s3-lifecycle-management#S3 Lifecycle Management:":"Automates the moving of objects between the different storage tiers. Can be used in conjunction with versioning. Lifecycle rules can be applied to both current and previous versions of an object.","s3-multipart-upload#S3 Multipart Upload:":"Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object’s data. You can upload these object parts independently and in any order. Multipart uploads are recommended for files over 100 MB and is the only way to upload files over 5 GB. It achieves functionality by uploading your data in parallel to boost efficiency. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. Possible reasons for why you would want to use Multipart upload: Multipart upload delivers the ability to begin an upload before you know the final object size. Multipart upload delivers improved throughput. Multipart upload delivers the ability to pause and resume object uploads. Multipart upload delivers quick recovery from network issues. You can use an AWS SDK to upload an object in parts. Alternatively, you can perform the same action via the AWS CLI. You can also parallelize downloads from S3 using byte-range fetches. If there’s a failure during the download, the failure is localized just to the specific byte range and not the whole object.","s3-pre-signed-urls#S3 Pre-signed URLs:":"All S3 objects are private by default, however the object owner of a private bucket with private objects can optionally share those objects with without having to change the permissions of the bucket to be public.\nThis is done by creating a pre-signed URL. Using your own security credentials, you can grant time-limited permission to download or view your private S3 objects.\nWhen you create a pre-signed URL for your S3 object, you must do the following:\nProvide your security credentials. Specify a bucket. Specify an object key. Specify the HTTP method (GET to download the object). Specify the expiration date and time. The pre-signed URLs are valid only for the specified duration and anyone who receives the pre-signed URL within that duration can then access the object.\nThe following diagram highlights how Pre-signed URLs work:","s3-select#S3 Select:":"S3 Select is an Amazon S3 feature that is designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3. Most applications have to retrieve the entire object and then filter out only the required data for further analysis. S3 Select enables applications to offload the heavy lifting of filtering and accessing data inside objects to the Amazon S3 service. As an example, let’s imagine you’re a developer at a large retailer and you need to analyze the weekly sales data from a single store, but the data for all 200 stores is saved in a new GZIP-ed CSV every day. Without S3 Select, you would need to download, decompress and process the entire CSV to get the data you needed. With S3 Select, you can use a simple SQL expression to return only the data from the store you’re interested in, instead of retrieving the entire object. By reducing the volume of data that has to be loaded and processed by your applications, S3 Select can improve the performance of most applications that frequently access data from S3 by up to 400% because you’re dealing with significantly less data. You can also use S3 Select for Glacier.","s3-server-access-logging#S3 Server Access Logging:":"Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and better understand your Amazon S3 bill. By default, logging is disabled. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant. It works in the following way: S3 periodically collecting access log records of the bucket you want to monitor S3 then consolidates those records into log files S3 finally uploads the log files to your secondary monitoring bucket as log objects","s3-simplified#S3 Simplified:":"S3 provides developers and IT teams with secure, durable, and highly-scalable object storage. Object storage, as opposed to block storage, is a general term that refers to data composed of three things:\n1.) the data that you want to store\n2.) an expandable amount of metadata\n3.) a unique identifier so that the data can be retrieved\nThis makes it a perfect candidate to host files or directories and a poor candidate to host databases or operating systems. The following table highlights key differences between object and block storage:\nData uploaded into S3 is spread across multiple files and facilities. The files uploaded into S3 have an upper-bound of 5TB per file and the number of files that can be uploaded is virtually limitless. S3 buckets, which contain all files, are named in a universal namespace so uniqueness is required. All successful uploads will return an HTTP 200 response.","s3-storage-classes#S3 Storage Classes:":"S3 Standard - 99.99% availability and 11 9s durability. Data in this class is stored redundantly across multiple devices in multiple facilities and is designed to withstand the failure of 2 concurrent data centers.\nS3 Infrequently Accessed (IA) - For data that is needed less often, but when it is needed the data should be available quickly. The storage fee is cheaper, but you are charged for retrieval.\nS3 One Zone Infrequently Accessed (an improvement of the legacy RRS / Reduced Redundancy Storage) - For when you want the lower costs of IA, but do not require high availability. This is even cheaper because of the lack of HA.\nS3 Intelligent Tiering - Uses built-in ML/AI to determine the most cost-effective storage class and then automatically moves your data to the appropriate tier. It does this without operational overhead or performance impact.\nS3 Glacier - low-cost storage class for data archiving. This class is for pure storage purposes where retrieval isn’t needed often at all. Retrieval times range from minutes to hours. There are differing retrieval methods depending on how acceptable the default retrieval times are for you:\nExpedited: 1 - 5 minutes, but this option is the most expensive. Standard: 3 - 5 hours to restore. Bulk: 5 - 12 hours. This option has the lowest cost and is good for a large set of data. The Expedited duration listed above could possibly be longer during rare situations of unusually high demand across all of AWS. If it is absolutely critical to have quick access to your Glacier data under all circumstances, you must purchase Provisioned Capacity. Provisioned Capacity guarentees that Expedited retrievals always work within the time constraints of 1 to 5 minutes.\nS3 Deep Glacier - The lowest cost S3 storage where retrieval can take 12 hours.","s3-transfer-acceleration#S3 Transfer Acceleration:":"Transfer acceleration makes use of the CloudFront network by sending or receiving data at CDN points of presence (called edge locations) rather than slower uploads or downloads at the origin. This is accomplished by uploading to a distinct URL for the edge location instead of the bucket itself. This is then transferred over the AWS network backbone at a much faster speed. You can test transfer acceleration speed directly in comparison to regular uploads.","s3-versioning#S3 Versioning:":"When versioning is enabled, S3 stores all versions of an object including all writes and even deletes. It is a great feature for implicitly backing up content and for easy rollbacks in case of human error. It can be thought of as analogous to Git. Once versioning is enabled on a bucket, it cannot be disabled - only suspended. Versioning integrates w/ lifecycle rules so you can set rules to expire or migrate data based on their version. Versioning also has MFA delete capability to provide an additional layer of security.","security-groups#Security Groups":"","security-groups-key-details#Security Groups Key Details:":"Security groups control inbound and outbound traffic for your instances (they act as a Firewall for EC2 Instances) while NACLs control inbound and outbound traffic for your subnets (they act as a Firewall for Subnets). Security Groups usually control the list of ports that are allowed to be used by your EC2 instances and the NACLs control which network or list of IP addresses can connect to your whole VPC. Every time you make a change to a security group, that change occurs immediately Whenever you create an inbound rule, an outbound rule is created immediately. This is because Security Groups are stateful. This means that when you create an ingress rule for a security group, a corresponding egress rule is created to match it. This is in contrast with NACLs which are stateless and require manual intervention for creating both inbound and outbound rules. Security Group rules are based on ALLOWs and there is no concept of DENY when in comes to Security Groups. This means you cannot explicitly deny or blacklist specific ports via Security Groups, you can only implicitly deny them by excluding them in your ALLOWs list Because of the above detail, everything is blocked by default. You must go in and intentionally allow access for certain ports. Security groups are specific to a single VPC, so you can’t share a Security Group between multiple VPCs. However, you can copy a Security Group to create a new Security Group with the same rules in another VPC for the same AWS Account. Security Groups are regional and can span AZs, but can’t be cross-regional. Outbound rules exist if you need to connect your server to a different service such as an API endpoint or a DB backend. You need to enable the ALLOW rule for the correct port though so that traffic can leave EC2 and enter the other AWS service. You can attach multiple security groups to one EC2 instance and you can have multiple EC2 instances under the umbrella of one security group You can specify the source of your security group (basically who is allowed to bypass the virtual firewall) to be a single /32 IP address, an IP range, or even a separate security group. You cannot block specific IP addresses with Security Groups (use NACLs instead) You can increase your Security Group limit by submitting a request to AWS","security-groups-simplified#Security Groups Simplified:":"Security Groups are used to control access (SSH, HTTP, RDP, etc.) with EC2. They act as a virtual firewall for your instances to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance and security groups act at the instance level, not the subnet level.","simple-notification-service-sns#Simple Notification Service (SNS)":"","simple-queuing-service-sqs#Simple Queuing Service (SQS)":"","simple-storage-service-s3#Simple Storage Service (S3)":"","simple-workflow-service-swf#Simple Workflow Service (SWF)":"","snowball#Snowball":"","snowball-edge-and-snowmobile#Snowball Edge and Snowmobile:":"Snowball Edge is a specific type of Snowball that comes with both compute and storage capabilities via AWS Lambda and specific EC2 instance types. This means you can run code within your snowball while your data is en route to an Amazon data center. This enables support of local workloads in remote or offline locations and as a result, Snowball Edge does not need to be limited to a data transfer service. An interesting use case is with airliners. Planes sometimes fly with snowball edges onboard so they can store large amounts of flight data and compute necessary functions for the plane’s own systems. Snowball Edges can also be clustered locally for even better performance. Snowmobile is an exabyte-scale data transfer solution. It is a data transport solution for 100 petabytes of data and is contained within a 45-foot shipping container hauled by a semi-truck. This massive transfer makes sense if you want to move your entire data center with years of data into the cloud.","snowball-key-details#Snowball Key Details:":"Snowball is a strong choice for a data transfer job if you need a secure and quick data transfer ranging in the terabytes to many petabytes into AWS. Snowball can also be the right choice if you don’t want to make expensive upgrades to your existing network infrastructure, if you frequently experience large backlogs of data, if you’re located in a physically isolated environment, or if you’re in an area where high-speed internet connections are not available or cost-prohibitive. As a rule of thumb, if it takes more than one week to upload your data to AWS using the spare capacity of your existing internet connection, then you should consider using Snowball. For example, if you have a 100 Mb connection that you can solely dedicate to transferring your data and you need to transfer 100 TB of data in total, it will take more than 100 days for the transfer to complete over that connection. You can make the same transfer in about a week by using multiple Snowballs. Here is a reference for when Snowball should be considered based on the number of days it would take to make the same transfer over an internet connection:","snowball-simplified#Snowball Simplified:":"Snowball is a giant physical disk that is used for migrating high quantities of data into AWS. It is a peta-byte scale data transport solution. Using a large disk like Snowball helps to circumvent common large scale data transfer problems such as high network costs, long transfer times, and security concerns. Snowballs are extremely secure by design and once the data transfer is complete, the snowballs are wiped clean of your data.","sns-key-details#SNS Key Details:":"SNS is mainly used to send alarms or alerts. SNS provides topics for high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email. You can send these push notifications to Apple, Google, Fire OS, and Windows devices. SNS allows you to group multiple recipients using topics. A topic is an access point for allowing recipients to dynamically subscribe for identical copies of the same notification. One topic can support deliveries to multiple endpoint types. When you publish to a topic, SNS appropriately formats copies of that message to send to whichever kind of device. To prevent messages being lost, messages are stored redundantly across multiple AZs. There is no long or short polling involved with SNS due to the instantaneous pushing of messages SNS has flexible message delivery over multiple transport protocols and has a simple API.","sns-simplified#SNS Simplified:":"Simple Notification Service is a pushed-based messaging service that provides a highly scalable, flexible, and cost-effective method to publish a custom messages to subscribers who wish to be informed about a certain topic.","sqs-key-details#SQS Key Details:":"The point behind SQS is to decouple work across systems. This way, downstream services in a system can perform work when they are ready to rather than when upstream services feed them data. In a hypothetical AWS environment running without SQS, Application A would pass Application B data regardless if Application B was ready to receive the info. With SQS however, there is an intermediary step where the data is stored temporarily in a buffer. It waits there until Application B pulls the temporarily stored data. SQS is not a push-based service so it is necessary for SQS to work in tandem with another service that queries it for information. There are two types of SQS queues; standard and FIFO. Standard queues may be received out of order based on message size or however else the SQS queues decide to optimize. FIFO queues guarantees that the order of messages that went into the queue is the same as the order of messages that leave it. Standard SQS queues guarantee that a message is delivered at least once and because of this, it is possible on occasion that a message might be delivered more than once due to the asynchronous and highly distributed architecture. With standard queues, you have a nearly unlimited number of transactions per second. FIFO SQS queues guarantee exactly-once processing and is limited to 300 transactions per second. Messages in the queue can be kept there from one minute to 14 days and the default retention period is 4 days. Visibility timeouts in SQS are the mechanism in which messages marked for delivery from the queue are given a time frame to be fully received by a reader. This is done by temporarily making them invisible to other readers. If the message is not fully processed within the time limit, the message becomes visible again. This is another way in which messages can be duplicated. If you want to reduce the chance of duplication, increase the visibility timeout. The visibility timeout maximum is 12 hours. Always remember that the messages in the SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message. You have to ensure that you delete the message after processing to prevent the message from being received and processed again once the visibility timeout expires. An SQS queue can contain an unlimited number of messages. You cannot set a priority to the individual items in the SQS queue. If priority of messaging matters, create two separate SQS queues. The SQS queues for the priority message can be polled first by the EC2 Instances and once completed, the messages from the second queue can be processed next.","sqs-polling#SQS Polling:":"Polling is the means in which you query SQS for messages or work. Amazon SQS provides short-polling and long-polling to receive messages from a queue. By default, queues use short polling. SQS long-polling: This polling technique will only return from the queue once a message is there, regardless if the queue is currently full or empty. This way, the reader needs to wait either for the timeout set or for a message to finally arrive. SQS long polling doesn’t return a response until a message arrives in the queue, reducing your overall cost over time. SQS short-polling: This polling technique will return immediately with either a message that’s already stored in the queue or empty-handed. The ReceiveMessageWaitTimeSeconds is the queue attribute that determines whether you are using Short or Long polling. By default, its value is zero which means it is using short-polling. If it is set to a value greater than zero, then it is long-polling. Every time you poll the queue, you incur a charge. So thoughtfully deciding on a polling strategy that fits your use case is important.","sqs-simplified#SQS Simplified:":"SQS is a web-based service that gives you access to a message queue that can be used to store messages while waiting for a queue to process them. It helps in the decoupling of systems and the horizontal scaling of AWS resources.","ssd-vs-hdd#SSD vs. HDD:":"SSD-backed volumes are built for transactional workloads involving frequent read/write operations, where the dominant performance attribute is IOPS. Rule of thumb: Will your workload be IOPS heavy? Plan for SSD. HDD-backed volumes are built for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS. Rule of thumb: Will your workload be throughput heavy? Plan for HDD.","standard-reserved-vs-convertible-reserved-vs-scheduled-reserved#Standard Reserved vs. Convertible Reserved vs. Scheduled Reserved:":"Standard Reserved Instances have inflexible reservations that are discounted at 75% off of On-Demand instances. Standard Reserved Instances cannot be moved between regions. You can choose if a Reserved Instance applies to either a specific Availability Zone, or an Entire Region, but you cannot change the region. Convertible Reserved Instances are instances that are discounted at 54% off of On-Demand instances, but you can also modify the instance type at any point. For example, you suspect that after a few months your VM might need to change from general purpose to memory optimized, but you aren’t sure just yet. So if you think that in the future you might need to change your VM type or upgrade your VMs capacity, choose Convertible Reserved Instances. There is no downgrading instance type with this option though. Scheduled Reserved Instances are reserved according to a specified timeline that you set. For example, you might use Scheduled Reserved Instances if you run education software that only needs to be available during school hours. This option allows you to better match your needed capacity with a recurring schedule so that you can save money.","storage-gateway#Storage Gateway":"","storage-gateway-key-details#Storage Gateway Key Details:":"The Storage Gateway service can either be a physical device or a VM image downloaded onto a host in an on-prem data center. It acts as a bridge to send or receive data from AWS. Storage Gateway can sit on top of VMWare’s ESXi hypervisor for Linux machines and Microsoft’s Hyper-V hypervisor for Windows machines. The three types of Storage Gateways are below: File Gateway - Operates via NFS or SMB and is used to store files in S3 over a network filesystem mount point in the supplied virtual machine. Simply put, you can think of a File Gateway as a file system mount on S3. Volume Gateway - Operates via iSCSI and is used to store copies of hard disk drives or virtual hard disk drives in S3. These can be achieved via Stored Volumes or Cached Volumes. Simply put, you can think of Volume Gateway as a way of storing virtual hard disk drives in the cloud. Tape Gateway - Operates as a Virtual Tape Library Relevant file information passing through Storage Gateway like file ownership, permissions, timestamps, etc. are stored as metadata for the objects that they belong to. Once these file details are stored in S3, they can be managed natively. This mean all S3 features like versioning, lifecycle management, bucket policies, cross region replication, etc. can be applied as a part of Storage Gateway. Applications interfacing with AWS over the Volume Gateway is done over the iSCSI block protocol. Data written to these volumes can be asynchronously backed up into AWS Elastic Block Store (EBS) as point-in-time snapshots of the volumes’ content. These kind of snapshots act as incremental backups that capture only changed state similar to a pull request in Git. Further, all snapshots are compressed to reduce storage costs. Tape Gateway offers a durable, cost-effective way of archiving and replicating data into S3 while getting rid of tapes (old-school data storage). The Virtual Tape Library, or VTL, leverages existing tape-based backup infrastructure to store data on virtual tape cartridges that you create on the Tape Gateway. It’s a great way to modernize and move backups into the cloud.","storage-gateway-simplified#Storage Gateway Simplified:":"Storage Gateway is a service that connects on-premise environments with cloud-based storage in order to seamlessly and securely integrate an on-prem application with a cloud storage backend. and Volume Gateway as a way of storing virtual hard disk drives in the cloud.","stored-volumes-vs-cached-volumes#Stored Volumes vs. Cached Volumes:":"Volume Gateway’s Stored Volumes let you store data locally on-prem and backs the data up to AWS as a secondary data source. Stored Volumes allow low-latency access to entire datasets, while providing high availability over a hybrid cloud solution. Further, you can mount Stored Volumes on application infrastructure as iSCSI drives so when data is written to these volumes, the data is both written onto the on-prem hardware and asynchronously backed up as snapshots in AWS EBS or S3.\nIn the following diagram of a Stored Volume architecture, data is served to the user from the Storage Area Network, Network Attached, or Direct Attached Storage within your data center. S3 exists just as a secure and reliable backup. Volume Gateway’s Cached Volumes differ as they do not store the entire dataset locally like Stored Volumes. Instead, AWS is used as the primary data source and the local hardware is used as a caching layer. Only the most frequently used components are retained onto the on-prem infrastructure while the remaining data is served from AWS. This minimizes the need to scale on-prem infrastructure while still maintaining low-latency access to the most referenced data.\nIn the following diagram of a Cached Volume architecture, the most frequently accessed data is served to the user from the Storage Area Network, Network Attached, or Direct Attached Storage within your data center. S3 serves the rest of the data from AWS.","swf-key-details#SWF Key Details:":"SWF is a way of coordinating tasks between application and people. It is a service that combines digital and human-oriented workflows. An example of a human-oriented workflow is the process in which Amazon warehouse workers find and ship your item as part of your Amazon order. SWF provides a task-oriented API and ensures a task is assigned only once and is never duplicated. Using Amazon warehouse workers as an example again, this would make sense. Amazon wouldn’t want to send you the same item twice as they’d lose money. The SWF pipeline is composed of three different worker applications that help to bring a job to completion: SWF Actors are workers that trigger the beginning of a workflow. SWF Deciders are workers that control the flow of the workflow once it’s been started. SWF Activity Workers are the workers that actually carry out the task to completion. With SWF, workflow executions can last up to one year compared to the 14 day maximum retention period for SQS.","swf-simplified#SWF Simplified:":"SWF is a web service that makes it easy to coordinate work across distributed application components. SWF has a range of use cases including media processing, web app backend, business process workflows, and analytical pipelines.","table-of-contents#Table of Contents":"Introduction\nIdentity Access Management (IAM)\nSimple Storage Service (S3)\nCloudFront\nSnowball\nStorage Gateway\nElastic Compute Cloud (EC2)\nElastic Block Store (EBS)\nElastic Network Interfaces (ENI)\nSecurity Groups\nWeb Application Firewall (WAF)\nCloudWatch\nCloudTrail\nElastic File System (EFS)\nAmazon FSx for Windows\nAmazon FSx for Lustre\nRelational Database Service (RDS)\nAurora\nDynamoDB\nRedshift\nElastiCache\nRoute53\nElastic Load Balancers (ELB)\nAuto Scaling\nVirtual Private Cloud (VPC)\nSimple Queuing Service (SQS)\nSimple Workflow Service (SWF)\nSimple Notification Service (SNS)\nKinesis Lambda API Gateway CloudFormation ElasticBeanstalk\nAWS Organizations\nMiscellaneous","this-is-not-my-effort-original-content-is-from-keenan-romain-repository#This is not my effort. Original content is from Keenan Romain’s repository.":"This is not my effort. Original content is from Keenan Romain’s repository.I’d strongly recommend to use Keenan’s repository; I made a copy so that I can make notes and update as needed.","virtual-private-cloud-vpc#Virtual Private Cloud (VPC)":"","virtual-private-networks-vpns#Virtual Private Networks (VPNs):":"VPCs can also serve as a bridge between your corporate data center and the AWS cloud. With a VPC Virtual Private Network (VPN), your VPC becomes an extension of your on-prem environment. Naturally, your instances that you launch in your VPC can’t communicate with your own on-premise servers. You can allow the access by first: attaching a virtual private gateway to the VPC creating a custom route table for the connection updating your security group rules to allow traffic from the connection creating the managed VPN connection itself. To bring up VPN connection, you must also define a customer gateway resource in AWS, which provides AWS information about your customer gateway device. And you have to set up an Internet-routable IP address of the customer gateway’s external interface. A customer gateway is a physical device or software application on the on-premise side of the VPN connection. Although the term “VPN connection” is a general concept, a VPN connection for AWS always refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IPsec) VPN connections. The following diagram illustrates a single VPN connection. The above VPC has an attached virtual private gateway (note: not an internet gateway) and there is a remote network that includes a customer gateway which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway. Summary: VPNs connect your on-prem with your VPC over the internet.","vpc-endpoints#VPC Endpoints:":"VPC Endpoints ensure that you can connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect. Traffic between your VPC and other AWS services stay within the Amazon ecosystem and these Endpoints are virtual devices that are HA and without bandwidth constraints. These work basically by attaching an ENI to an EC2 instance that can easily communicate to a wide range of AWS services. Gateway Endpoints rely on creating entries in a route table and pointing them to private endpoints used for S3 or DynamoDB. Gateway Endpoints are mainly just a target that you set. Interface Endpoints use AWS PrivateLink and have a private IP address so they are their own entity and not just a target in a route table. Because of this, they cost $.01/hour. Gateway Endpoints are free as they’re just a new route in to set. Interface Endpoint provisions an Elastic Network interface or ENI (think network card) within your VPC. They serve as an entry and exit for traffic going to and from another supported AWS service. It uses a DNS record to direct your traffic to the private IP address of the interface. Gateway Endpoint uses route prefix in your route table to direct traffic meant for S3 or DynamoDB to the Gateway Endpoint (think 0.0.0.0/0 -\u003e igw). To secure your Interface Endpoint, use Security Groups. But to secure Gateway Endpoint, use VPC Endpoint Policies. Summary: VPC Endpoints connect your VPC with AWS services through a non-public tunnel.","vpc-flow-logs#VPC Flow Logs:":"VPC Flow Logs is a feature that captures the IP information for all traffic flowing into and out of your VPC. Flow log data is sent to an S3 bucket or CloudWatch where you can view, retrieve, and manipulate this data.\nYou can capture the traffic flow at various stages through its travel:\nTraffic flowing into and out of the VPC (like at the IGW) Traffic flowing into and out of the subnet Traffic flowing into and out of the network interface of the EC2 instance (eth0, eth1, etc.) VPS Flow Logs capture packet metadata and not packet contents. Things like:\nThe source IP The destination IP The packet size Anything which could be observed from outside of the packet. Your flow logs can be configured to log valid traffic, invalid traffic, or both\nYou can have flow logs sourced from a different VPC compared to the VPC where your Flow Logs are. However, the other VPC must be peered via VPC Peering and under your account via AWS Organizations.\nYou can customize your logs by tagging them.\nOnce you create a Flow Log, you cannot change its config. You must make a new one.\nNot all IP traffic is monitored under VPC Flow Logs. The following is a list of things that are ignored by Flow Logs:\nQuery requests for instance metadata DHCP traffic Query requests to the AWS DNS server","vpc-key-details#VPC Key Details:":"You can think of VPC as your own virtual data center in the cloud. You have complete control of your own network; including the IP range, the creation of sub-networks (subnets), the configuration of route tables and the network gateways used. You can then launch EC2 instances into a subnet of your choosing, select the IPs to be available for the instances, assign security groups for them, and create Network Access Control Lists (NACLs) for the subnets themselves as additional protection. This customization gives you much more control to specify and personalize your infrastructure setup. For example, you can have one public-facing subnet for your web servers to receive HTTP traffic and then a different private-facing subnet for your database server where internet access is forbidden. You use subnets to efficiently utilize networks that have a large number of hosts VPCs come with defense in depth by design. From the sub-network (NACLs) down to the individual server (security group) and further down to the application itself (secure coding practices), you can set up multiple levels of protection against malicious users and programs. The default VPC for your AWS environment permits all subnets to have a route out to the internet meaning all subnets in the default VPC are internet accessible. The default setting allows you to immediately deploy instances and each EC2 instance will have both a public and private IP address. There is one default VPC per region. However, you can have as many custom VPCs as you want and all are private by default. When you create a custom VPC, new subnets are not created by default. You must create them separately. The same is true for an internet gateway. If you want your VPC to have internet access, you need to also create the gateway so that the network can be reached publicly by the world. Because of this, when you create an IGW it will initially be in an detached state. You will need to manually assign it to the custom VPC. Once you create a custom VPC however, the following are created by default: a route table a NACL a security group These components, which will be explained in further depth in case they are not already known, actually correspond to the traffic flow for how data will reach your instances. Whether the traffic originates from outside of the VPC or from within it, it must first go through the route table by way of the router in order to know where the desired destination is. Once that is known, the traffic then passes through subnet level security as described by the NACL. If the NACL deems the traffic as valid, the traffic then passes through to the instance level security as described by the security group. If the traffic hasn’t been dropped at this point, only then will it reach its intended instance. The VPC Wizard is an automated tool that is useful for creating custom VPCs. You can have your VPC on dedicated hardware so that the network is exclusive at the physical level, but this option is extremely expensive. Fortunately, if a VPC is on dedicated hosting it can always be changed back to the default hosting. This can be done via the AWS CLI, SDK or API. However, existing hosts on the dedicated hardware must first be in a stopped state. When you create a VPC, you must assign it an IPv4 CIDR block. This CIDR block is a range of private IPv4 addresses that will be inherited by your instances when you create them. The IP range of a default VPC is always /16. When creating IP ranges for your subnets, the /16 CIDR block is the largest range of IPs that can be used. This is because subnets must have just as many IPs or fewer IPs than the VPC it belongs to. A /28 CIDR block is the smallest IP range available for subnets. With CIDR in general, a /32 denotes a single IP address and /0 refers to the entire network The higher you go in CIDR, the more narrow the IP range will be. The above information about IPs is in regards to both public and private IP addresses. Private IP addresses are not reachable over the Internet and instead are used for communication between the instances in your VPC. When you launch an instance into a VPC, a private IP address from the IPv4 address range of the subnet is assigned to the default network interface (eth0) of the instance. This means that all instances within a VPC has a private IP, but only those selected to communicate with the external world have a public IP. When you launch an instance into a subnet that has public access via an Internet Gateway, both a public IP address and a private IP address are created. The public IP address is instead assigned to the primary network interface (eth0) that’s created for the instance. Externally, the public IP address is mapped to the private IP address through network address translation (NAT). You can optionally associate an IPv6 CIDR block with your VPC and subnets, and assign IPv6 addresses from that block to the resources in your VPC. VPCs are region specific and you can have up to five VPCs per region. By default, AWS is configured to have one subnet in each AZ of the regions where your application is. In an ideal and secure VPC architecture, you launch the web servers or elastic load balancers in the public subnet and the database servers in the private subnet. Here is an example of a hypothetical application sitting behind a typical VPC setup: Security groups can span subnets, but do not span VPCs. ICMP ensures that instances from one security group can ping others in a different security group. It is IPv4 and IPv6 compatible.","vpc-peering#VPC Peering:":"VPC peering allows you to connect one VPC with another via a direct network route using the Private IPs belonging to both. With VPC peering, instances in different VPCs behave as if they were on the same network. You can create a VPC peering connection between your own VPCs, regardless if they are in the same region or not, and with a VPC in an entirely different AWS account. VPC Peering is usually done in such a way that there is one central VPC that peers with others. Only the central VPC can talk to the other VPCs. You cannot do transitive peering for non-central VPCs. Non-central VPCs cannot go through the central VPC to get to another non-central VPC. You must set up a new portal between non-central nodes if you need them to talk to each other. The following diagram highlights the above idea. VPC B is free to communicate with VPC A with VPC Peering enabled between both. However, VPC B cannot continue the conversation with VPC C. Only VPC A can communicate with VPC C. It is worth knowing what VPC peering configurations are not supported: Overlapping CIDR Blocks Transitive Peering Edge to Edge Routing through a gateway or connection device (VPN connection, Internet Gateway, AWS Direct Connect connection, etc.) You can peer across regions, but you cannot have one subnet stretched over multiple availability zones. However, you can have multiple subnets in the same availability zone. Summary: VPC Peering connects your VPC to another VPC through a non-public tunnel.","vpc-simplified#VPC Simplified:":"VPC lets you provision a logically isolated section of the AWS cloud where you can launch services and systems within a virtual network that you define. By having the option of selecting which AWS resources are public facing and which are not, VPC provides much more granular control over security.","vpc-subnets#VPC Subnets:":"If a network has a large number of hosts without logically grouped subdivisions, managing the many hosts can be a tedious job. Therefore you use subnets to divide a network so that management becomes easier. When you create a subnet, be sure to specify which VPC you want to place it in. You can assign both IPv4 and IPv6 ranges to your subnets. The main benefits of subnets: They improve traffic flow, and thus speed \u0026 performance of the entire network. An Internet gateway (IGW) receiving a packet and checking which of 5 subnets the packet should be delivered to is much faster than checking 100 instances individually. And if the destination of a packet is within the subnet from where it originates, the traffic stays inside the subnet and doesn’t clutter the rest of the VPC. Subnets function as logical groups to put your entities inside of. It makes it much easier to configure similar resources as a group instead of for every individual instance. Amazon always reserves five IP addresses within a subnet. The first four IP addresses and the last IP address of each subnet CIDR block will always be unavailable for use.","waf-key-details#WAF Key Details:":"As mentioned above, WAF operates as a Layer 7 firewall. This grants it the ability to monitor granular web-based conditions like URL query string parameters. This level of detail helps to detect both foul play and honest issues with the requests getting passed onto your AWS environment. With WAF, you can set conditions such as which IP addresses are allowed to make what kind of requests or access what kind of content. Based off of these conditions, the corresponding endpoint will either allow the request by serving the requested content or return an HTTP 403 Forbidden status. At the simplest level, AWS WAF lets you choose one of the following behaviors: Allow all requests except the ones that you specify: This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers. Block all requests except the ones that you specify: This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website. Count the requests that match the properties that you specify: When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn’t accidentally configure AWS WAF to block all the traffic to your website. When you’re confident that you specified the correct properties, you can change the behavior to allow or block requests.","waf-protection-capabilities#WAF Protection Capabilities:":"The different request characteristics that can be used to limit access: The IP address that a request originates from The country that a request originates from The values found in the request headers Any strings that appear in the request (either specific strings or strings that match a regex pattern) The length of the request Any presence of SQL code (likely a SQL injection attempt) Any presence of a script (likely a cross-site scripting attempt) You can also use NACLs to block malicious IP addresses, prevent SQL injections / XSS, and block requests from specific countries. However, it is good form to practice defense in depth. Denying or blocking malicious users at the WAF level has the added advantage of protecting your AWS ecosystem at its outermost border.","waf-simplified#WAF Simplified:":"AWS WAF is a web application that lets you allow or block the HTTP(s) requests that are bound for CloudFront, API Gateway, Application Load Balancers, EC2, and other Layer 7 entry points into your AWS environment. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns that you can define. WAF’s default rule-set addresses issues like the OWASP Top 10 security risks and is regularly updated whenever new vulnerabilities are discovered.","web-application-firewall-waf#Web Application Firewall (WAF)":"","what-are-blue-green-deployments#What are Blue-Green deployments?":"One of the challenges with automating deployments is the cut-over from the final stage of testing to live production. You usually need to do this quickly in order to minimize downtime. The Blue-Green deployment approach does this by ensuring you have two production environments, as identical as possible. At any time one of them, let’s say blue for the example, is live. As you prepare a new release of your software you do your final stage of testing in the green environment. Once the software is working in the green environment, you switch the router so that all incoming requests go to the green environment - the blue one is now idle. Blue-green deployment also gives you a rapid way to rollback - if anything goes wrong you switch the router back to your blue environment. CloudFormation and CodeDeploy (AWS’s version of Jenkins) both support this deployment technique.","what-does-pilot-light-mean#What does pilot light mean?":"The term pilot light is often used to describe a disaster recovery scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that’s always on and can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario. For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core that has always been running.","what-is-amazon-data-lifecycle-manager#What is Amazon Data Lifecycle Manager?":"You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. Automating snapshot management helps you to: Protect valuable data by enforcing a regular backup schedule. Retain backups as required by auditors or internal compliance. Reduce storage costs by deleting outdated backups. Using Amazon DLM means that you no longer need to remember to take your EBS snapshots, thus reducing cognitive load on engineers.","what-is-amazon-elastic-container-service#What is Amazon Elastic Container Service?":"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Amazon ECS eliminates the need for you to install, operate, and scale your own cluster management infrastructure. With simple API calls, you can launch and stop container-enabled applications, query the complete state of your cluster, and access many familiar features like security groups, Elastic Load Balancing, EBS volumes and IAM roles. You can use Amazon ECS to schedule the placement of containers across your cluster based on your resource needs and availability requirements. You can also integrate your own scheduler or third-party schedulers to meet business or application specific requirements. You can choose to run your ECS clusters using AWS Fargate, which is serverless compute for containers. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.","what-is-amazon-elastic-kubernetes-service#What is Amazon Elastic Kubernetes Service?":"Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. EKS runs upstream Kubernetes and is certified Kubernetes conformant so you can leverage all benefits of open source tooling from the community. You can also easily migrate any standard Kubernetes application to EKS without needing to refactor your code. Kubernetes is open source software that allows you to deploy and manage containerized applications at scale. Kubernetes groups containers into logical groupings for management and discoverability, then launches them onto clusters of EC2 instances. Using Kubernetes you can run containerized applications including microservices, batch processing workers, and platforms as a service (PaaS) using the same tool set on premises and in the cloud. Amazon EKS provisions and scales the Kubernetes control plane, including the API servers and backend persistence layer, across multiple AWS availability zones for high availability and fault tolerance. Amazon EKS automatically detects and replaces unhealthy control plane nodes and provides patching for the control plane. Without Amazon EKS, you have to run both the Kubernetes control plane and the cluster of worker nodes yourself. With Amazon EKS, you provision your worker nodes using a single command in the EKS console, CLI, or API, and AWS handles provisioning, scaling, and managing the Kubernetes control plane in a highly available and secure configuration. This removes a significant operational burden for running Kubernetes and allows you to focus on building applications instead of managing AWS infrastructure. You can run EKS using AWS Fargate, which is serverless compute for containers. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design. Amazon EKS is integrated with many AWS services to provide scalability and security for your applications. These services include Elastic Load Balancing for load distribution, IAM for authentication, Amazon VPC for isolation, and AWS CloudTrail for logging.","what-is-amazon-mq#What is Amazon MQ?":"Amazon MQ is a managed message broker service that makes it easy to set up and operate message brokers in the cloud. The service is used when migrating services and apps into the cloud from your on-prem which is how it differs from Amazon SQS. Amazon MQ supports durability-optimized brokers backed by Amazon EFS to support high availability and message durability, and throughput-optimized brokers backed by Amazon EBS to support high-volume applications that require low latency and high throughput. You can easily move from any message broker to Amazon MQ because you don’t have to rewrite any messaging code in your applications. Amazon MQ is suitable for enterprise IT pros, developers, and architects who are managing a message broker themselves–whether on-premises or in the cloud–and want to move to a fully managed cloud service without rewriting the messaging code in their applications.","what-is-athena#What is Athena?":"Athena is an interactive query service which allows you to interact and query data from S3 using standard SQL commands. This is beneficial for programmatic querying for the average developer. It is serverless, requires no provisioning, and you pay per query and per TB scanned. You basically turn S3 into a SQL supported database by using Athena. Example use cases: Query logs that are dumped into S3 buckets as an alternative or supplement to the ELK stack Setting queries to run business reports based off of the data regularly entering S3 Running queries on click-stream data to have further insight of customer behavior","what-is-aws-config#What is AWS Config?":"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. AWS Config allows you to do the following: · Evaluate your AWS resource configurations for desired settings. · Get a snapshot of the current configurations of the supported resources that are associated with your AWS account. · Retrieve configurations of one or more resources that exist in your account. · Retrieve historical configurations of one or more resources. · Receive a notification whenever a resource is created, modified, or deleted. View relationships between resources. For example, you might want to find all resources that use a particular security group.","what-is-aws-directory-service#What is AWS Directory Service?":"AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)–aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access.","what-is-aws-fargate#What is AWS Fargate?":"AWS Fargate is a serverless compute engine for containers. The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. Just register your task definition and Fargate launches the container for you. It works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. It removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.","what-is-aws-kms#What is AWS KMS?":"AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. The master keys that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules. AWS KMS is integrated with most other AWS services that encrypt your data with encryption keys that you manage. AWS KMS is also integrated with AWS CloudTrail to provide encryption key usage logs to help meet your auditing, regulatory and compliance needs. You can configure your application to use the KMS API to encrypt all data before saving it to disk.","what-is-aws-macie#What is AWS Macie?":"To understand Macie, it is important to understand PII or Personally Identifiable Information: Personal data used to establish an individual’s identity which can be exploited Examples: Social Security number, phone number, home address, email address, D.O.B, passport number, etc. Amazon Macie is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization. You can be informed of detections via the Macie dashboards, alerts, or reporting. Macie can also analyze CloudTrail logs to see who might have interacted with sensitive data. Macie continuously monitors data access activity for anomalies, and delivers alerts when it detects risk of unauthorized access or inadvertent data leaks. Macie has ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.","what-is-aws-resource-access-manager#What is AWS Resource Access Manager?":"AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. Many organizations use multiple accounts to create administrative or billing isolation, and to limit the impact of errors as part of the AWS Organizations service. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available at no additional charge.","what-is-aws-secrets-manager#What is AWS Secrets Manager?":"AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs. In the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another (functionality vs. security). Secrets Manager enables you to replace hard-coded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can’t be compromised by someone examining your code, because the secret simply isn’t there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise.","what-is-aws-sts#What is AWS STS?":"AWS Security Token Service (AWS STS) is the service that you can use to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use. Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.","what-is-aws-workspaces#What is AWS WorkSpaces?":"Amazon WorkSpaces is a managed, secure Desktop-as-a-Service (DaaS) solution. You can use Amazon WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe. Amazon WorkSpaces helps you eliminate the complexity in managing hardware inventory, OS versions and patches, and Virtual Desktop Infrastructure (VDI), which helps simplify your desktop delivery strategy. With Amazon WorkSpaces, your users get a fast, responsive desktop of their choice that they can access anywhere, anytime, from any supported device.","what-is-elastic-transcoder#What is Elastic Transcoder?":"A media transcoder in the cloud. Basically, it is a service that converts media files from their original format to the media format specified whether for phones, tablets, PCs, etc. Because of the built-in support for different media types, you can trust that the resulting quality will be good. With Elastic Transcoder, you pay per minute of the transcode job and the resolution of the finished work.","what-is-iot-core#What is IoT Core?":"AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core provides secure communication and data processing across different kinds of connected devices and locations so you can easily build IoT applications.","what-is-opsworks#What is OpsWorks?":"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. OpsWorks has three offerings - AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks. AWS OpsWorks Stacks lets you manage applications and servers on AWS and on-premises. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. OpsWorks Stacks is complex enough for you to deploy and configure Amazon EC2 instances in each layer or connect to other resources such as Amazon RDS databases.","what-is-route-origin-authorization#What is Route Origin Authorization?":"You can bring part or all of your public IPv4 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the Internet. After you bring the address range to AWS, it appears in your account as an address pool. You can then create an Elastic IP address from your address pool and use it with your AWS resources, such as EC2 instances, NAT gateways, and Network Load Balancers. This is also called “Bring Your Own IP Addresses (BYOIP)”. To ensure that only you can bring your address range to your AWS account, you must authorize Amazon to advertise the address range and provide proof that you own the address range. The benefit of ROA is that you can migrate pre-existing applications to AWS without requiring your partners and customers to change their IP address whitelists.","what-is-the-amazon-cognito#What is the Amazon Cognito?":"Before discussing Amazon Cognito, it is first important to understand what Web Identity Federation is. Web Identity Federation lets you give your users access to AWS resources after they have successfully authenticated into a web-based identity provider such as Facebook, Google, Amazon, etc. Following a successful login into these services, the user is provided an auth code from the identity provider which can be used to gain temporary AWS credentials. Amazon Cognito is the Amazon service that provides Web Identity Federation. You don’t need to write the code that tells users to sign in for Facebook or sign in for Google on your application. Cognito does that already for you out of the box. Once authenticated into an identity provider (say with Facebook as an example), the provider supplies an auth token. This auth token is then supplied to cognito which responds with limited access to your AWS environment. You dictate how limited you would like this access to be in the IAM role. Cognito’s job is broker between your app and legitimate authenticators. Cognito User Pools are user directories that are used for sign-up and sign-in functionality on your application. Successful authentication generates a JSON web token. Remember user pools to be user based. It handles registration, recovery, and authentication. Cognito Identity Pools are used to allow users temp access to direct AWS Services like S3 or DynamoDB. Identity pools actually go in and grant you the IAM role. SAML-based authentication can be used to allow AWS Management Console login for non-IAM users. In particular, you can use Microsoft Active Directory which implements Security Assertion Markup Language (SAML) as well. You can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources. Amazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier (identity ID) for your end user immediately if you’re allowing unauthenticated users or after you’ve set the login tokens in the credentials provider if you’re authenticating users. When you need to easily add authentication to your mobile and desktop app, think Amazon Cognito."},"title":"AWS SAA C02 Study Guide"},"/SridharBlog/posts/aws-systems-manager/":{"data":{"":"","sample-code#Sample code":"using Amazon.Extensions.NETCore.Setup; using Amazon.Runtime; using Microsoft.Extensions.Configuration; using Microsoft.Extensions.DependencyInjection; namespace CloudConsole.Services; public static class ServiceBuilder { private static IConfiguration? Configuration; internal static IServiceCollection ConfigureServices(string applicationName) { IServiceCollection services = new ServiceCollection(); Configuration = BuildConfiguration(applicationName); services.AddSingleton(Configuration); return services; } private static IConfiguration BuildConfiguration(string applicationName) { var allEnvVariables = Environment.GetEnvironmentVariables(); //var region = allEnvVariables[\"AWS_DEFAULT_REGION\"]?.ToString() ?? string.Empty; var accessKey = allEnvVariables[\"AWS_ACCESS_KEY_ID\"]?.ToString() ?? string.Empty; var secretKey = allEnvVariables[\"AWS_SECRET_ACCESS_KEY\"]?.ToString() ?? string.Empty; var awsCredentials = new BasicAWSCredentials(accessKey, secretKey); AWSOptions aWSOptions = new() { Region = Amazon.RegionEndpoint.USEast1, Credentials = awsCredentials }; IConfigurationBuilder builder = new ConfigurationBuilder(); builder.SetBasePath(Directory.GetCurrentDirectory()) .AddJsonFile(\"appsettings.json\", optional: true, reloadOnChange: true) .AddJsonFile($\"appsettings.{Environment.GetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\") ?? \"Production\"}.json\" , optional: true, reloadOnChange: true) .AddEnvironmentVariables() .AddSystemsManager(@\"/\" + applicationName + @\"/\", aWSOptions, TimeSpan.FromMinutes(5)) .AddSystemsManager(\"\"\"/PortfolioManager/\"\"\", aWSOptions); Configuration = builder.Build(); return Configuration; } } Create an AWSOptions instance and use this instance when you call AddSystemsManager.\nAnd when I run my image I issue the following command:\ndocker run --name test_container1 -e AWS_ACCESS_KEY_ID=AccessKey -e AWS_SECRET_ACCESS_KEY=SecretKey test_image3","what-is-this#What is this?":"I had issues using AWS Systems Manager in Docker, and I didn’t see any examples of how people use Systems Manager within a Docker container; I wanted to pass my login credentials via environment variables. Systems Manager does not use environment variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc. and I didn’t see any examples of how people use Systems Manager within a docker image.\nThis is the way I solved it, and yes, it works!"},"title":"Using AWS Systems Manager in a docker image"},"/SridharBlog/posts/blazor-on-rp/":{"data":{"":"","aws-credentials#AWS credentials":"Blazing Blog is a web application that uses AWS S3 and System Manager’s parameter store to manage its data. It stores the database connection string in the parameter store and uses it to connect to the database. It also uploads and downloads images from S3 buckets. To ensure security and limit the scope of access, a new IAM user BlazingBlogUser was created with permissions to only access S3 and System Manager. The access keys of this user were saved in a secure location and used by the application to authenticate with AWS.","enviromnet-variables#Enviromnet variables":"One of the common problems that developers face when deploying their applications on Raspberry Pi devices is the configuration of environment variables. Environment variables are key-value pairs that store information such as paths, credentials, settings, etc. that are specific to the operating system or the application. When we develop our applications in Visual Studio, we often rely on its built-in features to manage the environment variables for us, without paying much attention to how they are set or where they are stored. However, when we try to run our applications on a new Raspberry Pi device, we may encounter errors or unexpected behaviors because the device does not have the same environment variables that we used in Visual Studio. Therefore, it is important to understand how to properly configure the environment variables for our applications on Raspberry Pi devices, and how to troubleshoot any issues that may arise from them. Here are a few challenges and how I resolved it:","how-are-the-executables-built#How are the executable(s) built?":"I attempted to use DuckDNS as a dynamic DNS service, but it required me to configure my router settings, which I was not comfortable with. Therefore, I decided to look for another solution for hosting my Blazing Blog website. I wanted to deploy my blog as a docker container, but I faced a challenge in creating a Dockerfile that would work on both Intel and Raspberry Pi architectures. As a result, I opted to install .Net SDK on Raspberry Pi and compile my application using the following commands:\ndotnet build release","introduction#Introduction":"I have always been interested in creating a blog using Blazor, a web framework that allows me to write C# code for both the client and the server side. I also wanted to host it on a Raspberry Pi, a low-cost and portable device that can run Linux and .NET applications. In this post, I will share how I configured my Blazing Blog, the name I gave to my project, on my Raspberry Pi.","kestrel#Kestrel":"Kestrel is a lightweight web server that can run on its own without a reverse proxy like Nginx. I chose to use Kestrel for my Blazing Blog project, which is hosted on a Raspberry Pi and has a very low traffic volume (no more than a handful concurrent users). To make Kestrel accessible from other machines, I had set enviroment variable ASPNETCORE_URLS as follows:\nexport ASPNETCORE_URLS=\"http://0.0.0.0:5001;https://0.0.0.0:5002\" One way to configure the ASP.NET Core application to listen on all network interfaces is to set the ASPNETCORE_URLS environment variable to the value of 0.0.0.0. This means that the application will accept requests from any IP address.","putting-all-together#Putting all together":"export ASPNETCORE_URLS=\"http://0.0.0.0:5001;https://0.0.0.0:5002\" export ASPNETCORE_Kestrel__Certificates__Default__Password=MySecretPassword export ASPNETCORE_Kestrel__Certificates__Default__Path=/home/sridh/.aspnet/https/aspnetapp.pfx cd ~/dotnetApp/bin/Release/net8.0/publish ./BlazingBlog \u0026","self-signed-ssl#Self signed SSL":"As Blazing Blog is a intranet application we are going to use a Self signed SSL certificate to make browsers happy! Self signed SSL certificates are a type of digital certificates that are created and signed by the owner of the website or server, rather than by a trusted third-party authority. They are often used for testing or development purposes, or for internal networks where trust is not an issue. However, self signed SSL certificates are not recommended for public-facing websites or services, as they can trigger security warnings in browsers and clients, and expose users to potential man-in-the-middle attacks.\nThe SSL was created using the following commands:\ndotnet dev-certs https -ep $env:USERPROFILE\\.aspnet\\https\\aspnetapp.pfx -p crypticpassword dotnet dev-certs https --trust This creates a file $HOME/.aspnet/https/aspnetapp.pfx and we set the following two envirornment variables to use this certificate.\nexport ASPNETCORE_Kestrel__Certificates__Default__Password=MySecretPassword export ASPNETCORE_Kestrel__Certificates__Default__Path=/home/sridh/.aspnet/https/aspnetapp.pfx"},"title":"Blazor on RP"},"/SridharBlog/posts/boonton/":{"data":{"":"On April 27th, 2025, I found myself drawn to a small, unassuming getaway—Boonton, NJ. It wasn’t one of those days marked by a whirlwind of plans or urgent appointments. Instead, I craved change from the everyday rhythms of life and wanted to immerse myself in a tranquil, unhurried environment, even if only for a few precious hours. What unfolded was a day embroidered with moments of quiet introspection, unexpected photographic revelations, and slow, deliberate pleasure in nature’s simple wonders.","a-photographers-unexpected-realizations#A Photographer’s Unexpected Realizations":"Photography has always been a way for me to capture and reflect upon the beauty of a moment. I arrived equipped with my trusted mirrorless camera—an apparatus I normally consider indispensable for high-quality shots and meticulous control over every composition. My plan was to document the scenic charm of the park and, particularly, the dynamic presence of the waterfall. However, fate had a playful twist in store.\nAs I set out to compose my first shots, I found myself instinctively reaching for my smartphone to capture what immediately caught my eye. There was a subtle magic at work. Despite all the promises and technicalities heralded by my mirrorless camera, the images snapped on my phone possessed an unexpected clarity and vibrancy. They carried a raw, unfiltered quality that echoed the spontaneity of the moment far more effectively than the carefully calibrated shots from my primary device.\nI experimented further—alternating between the mirrorless camera and my phone, angle by angle, light by light. Each time I reviewed the results, I was surprised by the phone’s ability to pick out details that seemed to come alive with a natural ease. The play of light on the water’s surface, the fine mist caught in mid-air, and even the subtle hues dancing along the rocks all spoke with a simplicity that was enchanting. What began as a technical comparison quickly evolved into a reflective meditation on the art of capturing life’s transient moments. It was a clear reminder that while advanced tools offer precision, sometimes it’s the immediacy and authenticity of a simple device that best preserves the spirit of an experience.","continuing-the-journey-of-discovery#Continuing the Journey of Discovery":"In an age saturated with digital imagery and rapid exchanges, it is easy to overlook the power of slowing down and truly absorbing the world around us. My visit to Boonton reminded me that every journey holds the potential for renewal and insight if we are willing to see beyond the surface. Whether it’s the serene flow of water at a modest levee, the gentle dance of sunlight and shadow, or the subtle interplay of technology and nature, something extraordinary is always waiting to be discovered.","discovering-boontons-scenic-side#Discovering Boonton’s Scenic Side":"Boonton isn’t typically on the radar for grandiose tourist attractions, but it has a charm that speaks to those who love to explore at a slower pace. Nestled on the banks of the Rockaway River, this small town possesses a unique blend of rustic appeal and understated beauty. One of its local parks, a modest yet inviting green space, quickly captured my attention. Here, a small levee gracefully guided the water into forming a waterfall—a cascade that, while not towering in height, commanded the scene with a ceaseless flow of water and an audible, soothing murmur.\nAs I meandered along the park’s winding paths, I found time to ponder the marvel of nature working quietly in the background. The steady yet unpretentious waterfall reminded me that beauty doesn’t always come with fanfare. Sometimes, it is the deliberate, unassuming presence of nature—a constant, soothing rhythm in the midst of our hurried lives—that holds the most profound allure.","embracing-the-chill-and-the-sun#Embracing the Chill and the Sun":"That morning, the air carried a brisk, chilly quality—a reminder that early spring was still mingling with hints of winter’s retreat. Despite the nipping cold, the sun emerged in all its gentle, life-affirming brilliance. Its rays, soft yet determined to warm the day, infused the landscape with an almost ethereal glow. As I stepped out, the contrast between the cool air and the sun’s warm caress provided an invigorating sensation, heightening every detail of my surroundings. This interplay between weather elements helped set the tone for what was destined to be an unexpectedly memorable outing.","lessons-learned-from-nature-and-technology#Lessons Learned from Nature and Technology":"This day in Boonton became much more than a simple escape—it evolved into a lesson about perspective and creativity. In today’s digital age, we are often conditioned to believe that better, more sophisticated gear equates to better art. However, my experience that day challenged that notion. The spontaneous charm of the phone’s lens revealed that the essence of photography, or indeed art in general, is not solely defined by technical superiority, but by the authenticity of the moment captured.\nIt spurred a reflective inquiry: Are we so enamored with modern gadgets that we sometimes overlook the artistry embedded in everyday simplicity? In an era where photographic perfection is marketed as the ultimate goal, the clear and compelling images produced by my phone became a personal manifesto. They affirmed the idea that every device, no matter how simple, holds the potential to document beauty when used with genuine intent and openness to the moment.\nI took away an invaluable truth that day—a truth that many photographers and creatives might resonate with. In the interplay between advanced technology and uncomplicated spontaneity, there is a balance that can redefine what it means to capture life. It is indeed a challenge worth embracing: learning to see beyond the constraints of our tools and instead focusing on the magic that unfolds naturally around us.","the-enduring-impact-of-a-simple-outing#The Enduring Impact of a Simple Outing":"Leaving Boonton that day, I carried with me more than just a collection of photographs; I carried memories of quiet moments that spoke profoundly to the soul. The experience was a gentle reminder that even on seemingly unremarkable days, nature offers endless opportunities for reflection and joy. The unhurried pace of the day allowed for encounters with both external beauty and internal revelations, highlighting that sometimes, the simplest journeys are the ones that change us the most.","the-magic-of-a-slow-day#The Magic of a Slow Day":"There is a unique kind of richness that emerges on days with little clutter—days when the schedule slows, allowing the mind to wander and observe. In Boonton, that slow pace was a gift. Without the pressure of rushing from one destination to the next, I was free to pause and immerse myself fully in the present. Every detail—the gentle ripple of the Rockaway River, the soft murmur of cascading water, and the interplay of shadow and sunshine—beckoned me to take a closer look, to slow down and appreciate the nuance of the moment.\nStrolling through the park, I noticed how the environment seemed to breathe with its own time. The natural sounds offered a quiet soundtrack that enriched the reflective ambiance of the day. In those moments, I felt both an intimate connection with nature and a gentle nudge to live more presently. The waterfall, even in its modesty, became a metaphor for life—a reminder that our most meaningful experiences often lie in the simplicity of constant, persistent presence."},"title":"A Day in Boonton: A Journey into Quiet Beauty"},"/SridharBlog/posts/california-2025/":{"data":{"":"","bridalveil-fall#Bridalveil Fall":"Next we headed towards Yosemite Falls and on the way we came across the Bridalveil Fall. Bridalveil Fall is one of Yosemite National Park’s most iconic and accessible waterfalls, plunging 620 feet (189 meters) from a hanging valley into Yosemite Valley below. It’s often the first waterfall visitors see when entering the park, especially from the famous Tunnel View overlook.\nThe fall flows throughout the year, reaching its peak in spring when snowmelt nourishes Bridalveil Creek. On windy days, the water is blown sideways, forming a misty veil that gave it its name. The Ahwahneechee people named it Pohono, meaning “Spirit of the Puffing Wind,” due to its ethereal look. Our photos taken there were geo-tagged as Sierra Nevada.\nSierra Nevada","trip-to-yosemite-national-park#Trip to Yosemite National Park":"In June 2025, we had a fantastic opportunity to explore Yosemite National Park. It was a delightful two-day adventure, and we’re excited to share some notes about our journey and all the fun things we did.\nLet’s begin with day one. On June 19th, Veena and I had the pleasure of visiting this lovely park together with Chandrasekar Vemula. We started our day early from his house and arrived at the park around 10:30 or 11:00 A.M., eager for a wonderful day ahead.\nEven though I did some homework and made a list of things to do at the park, I didn’t realize just how big it was. I was surprised to find out that most of the attractions are more than an hour’s drive from the entrance!","tuolumne-grove#Tuolumne Grove":"Tuolumne Grove is a serene pocket of Yosemite National Park where you can walk among giant sequoias, some of the largest and oldest living trees on Earth. Nestled near Crane Flat along Tioga Road, this grove is home to about two dozen mature sequoias, including the famous Dead Giant Tunnel Tree, which was tunneled through in 1878 to attract tourists.\nThe hike to the grove is a 2.5-mile round-trip that descends approximately 500 feet through a forest of sugar pines and white firs. It’s a moderately challenging trail—easy on the way down, but a bit of a workout coming back up. Unlike the more crowded Mariposa Grove, Tuolumne offers a quieter, more contemplative experience.\nPicture time: Let’s take a look at some of the photos we’ve taken."},"title":"California 2025"},"/SridharBlog/posts/darjeeling/":{"data":{"visiting-darjeeling-after-40-years#Visiting Darjeeling after 40 years":"Visiting Darjeeling after 40 yearsA long time ago, when I lived in Salem, India, I often visited Yercaud, a hill station town about 30 minutes away by car. Yercaud was a small town where you could walk everywhere. You didn’t need a vehicle to get around. My childhood mind made me think that all hill stations would be just small, sleepy towns with nothing much to do.\nWhen I was in 10th grade, my school organized a trip to Nepal, and during that visit, we also went to Darjeeling. Darjeeling was a pleasant surprise. I never imagined a hill station could have so many businesses and visitors.\nThe above picture was shot in 2025. No pictures from the 1970s 😭\nLet me talk a little more about the 70s. Darjeeling is about 6,000 feet high, and we took a train up the hill. The trip was pretty long — it took almost the whole day! However, in 2025, we took the road and the trip took only about 3 hours. On our way up, we saw a train climb up the hill, which brought back some old memories!\nOn our way up the hill, we happened to see one of Darjeeling’s main attractions, the steam engine. Yes, people walk, stand, talk, cross, and even ride on it. It’s okay, and it’s a part of life in Darjeeling! Perhaps it’s the same engine that was used 40 years ago. Who can say?\nIt’s been over 40 years, and I still fondly remember Darjeeling tea. Over time, I’ve heard many people praise it as some of the best black tea around, and naturally, the first thing I tried was their tea, which truly lived up to the reputation.\nGolden yellow first-flush tea We arrived in Darjeeling late in the afternoon. After checking into our hotel and having lunch, we strolled to Market Street, the main commercial hub and the popular hangout spot for tourists. If I were to describe it as crowded, that would be an understatement given the sheer number of people there. It was swamped; the only thing visible around us was a sea of heads. We happened to step into Glenary’s, which is a bakery on Market Street.\nPeople waiting in line to pay for their purchase at Glenary's.\nThat night, a storm triggered many landslides, making transportation in Darjeeling nearly impossible. While not officially suspended, driving was effectively impossible the following day.\nTo make the most of our time, we decided to explore on foot and visited the Happy Valley tea estate. The walk was quite interesting, with narrow, winding roads—if I recall correctly, we crossed at least 10 hairpin bends. Did I mention how narrow it was? If two motorcycles met on the road, one had to wait for the other to pass. That narrow. We walked for around 45 minutes to an hour, stopping multiple times for pictures and snacks. Finally, we reached Happy Valley and enjoyed some tea tasting.\nSince we spent most of the day walking, we observed a few interesting things. First, the process of water distribution to homes. It’s easy to imagine how water is supplied from the treatment plant to our houses. As shown in the image below, water flows through a main pipe, from which individual house taps are connected.\nHowever, in Darjeeling, it is a different story. Each house is independently connected to the water treatment plant, and you will see hundreds of pipes laid above ground throughout the town!\nI am truly amazed by the reasoning and economics behind running thousands of pipes throughout the town. It must be an enormous maintenance challenge!\nOnly in Darjeeling is it quite common to see encroachments on sidewalks in India, like street hawkers and panhandlers. But have you noticed how law enforcement sometimes encroaches as well? Interestingly, they were the only encroachers in this area, and I find it puzzling that they built a booth where no police officer was stationed, taking up the entire sidewalk! Let’s talk about what makes Darjeeling such a wonderful place to visit. I had a lovely three-night stay there. My room had a window that overlooked a beautiful valley, which was often shrouded in dense fog. It was truly breathtaking to watch the mountains and tea estates gently emerge and disappear as the fog moved. However, on one of the three mornings I was there, I experienced this:\nThese pictures are of Kanchenjunga, also known as Kangchenjunga, which is the third-highest mountain in the world, standing at an elevation of 8,586 meters (28,169 feet). It is located in the eastern Himalayas, straddling the border between India and Nepal.\nWe visited a Japanese Temple; Like most of the other Peace Pagodas, it was built under the guidance of Nichidatsu Fujii (1885–1985), a Buddhist monk from Japan and founder of the Nipponzan-Myōhōji Buddhist Order.\nWe did take a ride on the toy train; it is real train that transports people! May be this was the engine that brought up and down the hill 40 years ago! Who knows?\nBefore we wind up let me mention about Darjeeling momos! Mouth watering!\nSee you soon!"},"title":"Darjeeling"},"/SridharBlog/posts/hisense-freezer/":{"data":{"":"I recently purchased a Hisense freezer from Costco that can double as a refrigerator. Switching modes isn’t very intuitive, and I don’t do it frequently. Rather than referencing the manual every time, I thought extracting the key information and sharing it on my blog would be helpful.","controlling-temperature#Controlling temperature":"","display-controls#Display Controls":"Controlling the Hisense freezer/refrigerator involves understanding the standby mode. To enter or exit this mode, simply press and hold the power button for at least 3 seconds. When in standby mode, the temperature area will display ‘OF’, other icons will be extinguished, and other key functions will be disabled. Pressing the button again will restore the original control state, exiting the standby mode.","door-alarm#Door Alarm":"If the door remains open for over two minutes, the alarm will activate thrice every minute. This audio alert will cease after eight minutes.","energy-saving#Energy Saving":"This function enables the refrigerator to operate in a power-saving mode, which is beneficial for reducing energy consumption when you’re away. To activate energy-saving mode, press the “Energy Saving” button for three seconds until it lights up. In this mode, the temperature is set to 43°F in the refrigerator and 1°F in the freezer.","super-freezecool#Super Freeze/Cool":"I’m not going to use this mode.","switching-between-freezerfridge#Switching between Freezer/Fridge":"The Freezer/Fridge conversion button is the key to switching between the two modes. By pressing and holding it for 3 seconds, you can easily switch from freezer to fridge and vice versa. After the switch, the lights will briefly go out, then turn on or flash once. This is a normal indication that the control system has successfully executed the switch.","temperature-control#Temperature Control":"Press the TEMP button to adjust the temperature to your preferred level. In freezer mode, you can modify the temperature between 7°F and 11°F. In refrigerator mode, you can set the temperature from 36°F to 46°F."},"title":"Hisense Freezer"},"/SridharBlog/posts/install-ollama/":{"data":{"":"This is an urgent note about how I resolved a pressing issue installing Ollama on my Raspberry Pi.\nA few months ago, I successfully installed Ollama on my Raspberry Pi, and it performed flawlessly. I made some configuration tweaks to ensure everything worked smoothly at the time, but I couldn’t recall the changes I had implemented. Recently, after upgrading the Ollama server, disaster struck—all the applications reliant on this Raspberry Pi began to fail. I wrote this blog to prevent a repeat of this frustrating experience during future upgrades, documenting how I resolved the issue and restored functionality.","fix#Fix":"To expose Ollama on your network, you must change the default bind address from 127.0.0.1 to 0.0.0.0. This allows Ollama to accept connections from any IP address and makes it accessible from other devices on the same network. The bind address can be modified using the OLLAMA_HOST environment variable.\nI run Raspberry Pi OS, which is Debian-based, on my Raspberry Pi. The required change needs to be done in ollama.service file, and the way one updates this file is as follows:\nsystemctl edit ollama.service In the editor, add the following line under the [Service] section:\n[Service] Environment=\"OLLAMA_HOST=0.0.0.0:11434\" Save and exit the editor, and reload systemd configuration and restart Ollama as follows:\nsystemctl daemon-reload systemctl restart ollama Ollama should be accessible from a remote machine. If not, update this document after you resolve the issue 😔.","what-happened#What happened?":"I’ve worked with models like Pi3 and Gemma3 on my Raspberry Pi, and while they performed adequately, I discovered a newer version of Gemma3 was available—an upgrade I couldn’t resist. At the time, I was running Ollama version 0.3.x, but the latest Gemma3 model required Ollama version 0.6.0 or higher. As someone prioritizing staying up-to-date with the newest software, I promptly upgraded my installation. This upgrade occurred on a Friday, and as fate would have it, I overlooked the change and didn’t use the applications reliant on my local Ollama installation for weeks. When I finally reconnected, every application hit me with exceptions, indicating that the upgrade needed further attention.\nThe challenge I faced was rooted in confusion—I couldn’t pinpoint the exact cause of the problem. Was the issue introduced by the Ollama server upgrade itself, or was it connected to the change in my network setup? Around the same time, I had replaced my router, switching from one that primarily used IPV4 to a newer model that emphasized IPV6. This shift introduced the possibility of a firewall misconfiguration or connectivity hiccups that I hadn’t anticipated. Alternatively, could the problem have stemmed from the Windows update that occurred in parallel with these changes? The timing of these events made troubleshooting even more complex, as I had to consider multiple variables and their potential interactions.\nOn the surface, each factor seemed plausible, but I lacked the clarity to determine which one—or combination of them—was to blame. The Ollama upgrade introduced compatibility concerns with my applications, while the router change could have affected network communication protocols. Simultaneously, the Windows update may have brought unexpected changes to system settings, firewall rules, or application permissions. The overlapping nature of these modifications created a perfect storm of uncertainty, leaving me scrambling to identify the root cause of the failure.\nThis experience served as a stark reminder of the importance of thorough testing and documentation when implementing multiple changes to a system. It underscored how seemingly unrelated upgrades and modifications could combine to create unforeseen issues, leaving critical applications in limbo. As I delved deeper into the problem, I realized that unraveling this tangled web of possibilities would require methodical investigation and careful analysis of each component involved. My goal became not just resolving the immediate issue but ensuring that I could learn from this situation and mitigate similar risks in the future."},"title":"Installing (Upgrading) Ollama"},"/SridharBlog/posts/kolkata/":{"data":{"":"","durga-puja-2025-a-kaleidoscope-of-creativity-in-kolkata#Durga Puja 2025: A Kaleidoscope of Creativity in Kolkata":"","introduction#Introduction":"Veena and I found ourselves in India during Durga Pooja, a perfect opportunity to revisit Kolkata, a city close to my heart. I was excited to witness every street transformed into an open gallery, a blend of creativity and tradition. This post is a visual journey, so I’ll let my pictures tell the story!","pandals#Pandals":"Let us start our picture journey:\nLocation Picture Lake Town Lake Town Dum Dum Park Dum Dum Park Dum Dum Tarun Dal Dum Dum Tarun Dal Dum Dum Tarun Dal Arjurpur Amara sabai club Arjurpur Amara sabai club New Town mela ground New Town mela ground I have quite a collection of images to share from this experience, but I want to focus only a little on Location 5. This particular place stood out as one of the most beautifully constructed and thoughtfully designed areas I have ever seen. The atmosphere there was absolutely electric, charged with an energy that was almost palpable. As I stepped inside, I was immediately captivated by the stunning mirrors that seemed to multiply the space endlessly, creating a mesmerizing effect. The lighting was specially curated to enhance the ambiance, casting a warm, inviting glow that complemented the lively music playing in the background. Every element in this place combined to create an environment that was vibrant, exciting, and unforgettable. Words truly can't do justice to the sheer sense of wonder and exhilaration I felt in that moment! Back to pictures.\nLocation Picture Salt Lake IB Block Salt Lake IB Block Salt Lake IB Block Salt Lake EC Block Salt Lake EC Block Salt Lake EC Block The locations tend to be crowded in the evenings and into the night, while they are generally easier to access before noon. However, we chose to visit some sites after sunset. Below are some of the photographs we took following sunset.\nLocation Picture Salt Lake FD Block Salt Lake FD Block Salt Lake FD Block Salt Lake FD Block Bakul Bagan Bakul Bagan North Park North Park North Park Mahanayak UTtam I have many more pictures taken, but I believe these are the ones worth sharing. While there were hundreds of Pandals in Calcutta, the humid weather and visiting countless Pandals left me exhausted, so I decided not to visit them all.","start#Start":"Kolkata is in West Bengal, and whenever I think about West Bengal, the first thing that flashes in my head is: After a hiatus of almost forty years, I was thrilled to return to Kolkata. And the first thing on my agenda? To relish their tea in a traditional clay cup, a taste of nostalgia and a journey of rediscovery!\nNow that we’ve covered that, let’s begin exploring the pandals I visited. I’ve mentioned the word ‘pandal’ several times—what does it mean? A pandal is a temporary structure used for festivals, religious ceremonies, or social events. It is a temporary structure, usually in place for about a month. All structures and deities you see below don’t exist when you read this."},"title":"Where Art Meets Devotion: Exploring Kolkata`s Pandals"},"/SridharBlog/posts/momentum-trading/":{"data":{"":"At first glance, momentum investing appears more like a reflexive response to market trends than a deliberate investing strategy. The concept of selling underperforming assets and purchasing those on the rise is enticing, but it directly challenges the timeless Wall Street principle of “buy low, sell high.”\nThis article explores the concept of momentum investing, examining its advantages and drawbacks.\nWhile Richard Driehaus wasn’t the pioneer of momentum investing, he refined the approach and implemented it as the cornerstone of his fund management strategy.\nHe believed that greater profits could be achieved by “buying high and selling higher,” rather than purchasing undervalued stocks and waiting for the market to recognize their worth.","advantages-of-momentum-investing#Advantages of Momentum Investing":"Momentum investing can yield significant returns for traders who possess the right temperament, can manage the associated risks, and remain committed to the strategy.","building-blocks-for-momentum-investing#Building blocks for Momentum Investing":"Navigating momentum markets demands advanced risk management strategies to tackle challenges such as volatility, overcrowding, and unforeseen pitfalls that can erode profits. However, traders often disregard these principles, driven by a deep-seated fear of missing out on rallies or sell-offs while others secure substantial gains. These guidelines can be summarized into five key components:\nOpt for liquid securities. Carefully manage risks during trade entries and exits. Aim to enter positions early. Select your holding period judiciously. Execute your exit with precision.","capitalizing-on-emotional-market-behavior#Capitalizing on Emotional Market Behavior:":"Instead of being swayed by emotions like many investors, momentum traders profit from price changes driven by emotional decisions, relying on discipline and predefined entry and exit points.","carefully-manage-risks-during-trade-entries-and-exits#Carefully manage risks during trade entries and exits":"To ensure the success of a momentum strategy, it’s crucial to thoroughly address the risk factors involved. Common pitfalls in momentum trading include:\nEntering a position prematurely, before the momentum move is fully confirmed. Exiting a position too late, after the market has reached saturation. Neglecting to monitor the market, leading to missed shifts in trends, reversals, or unexpected news events. Holding a position overnight, exposing it to external influences after market hours that can cause significant price and pattern changes the next day. Delaying action on a losing position, inadvertently following the momentum in the wrong direction.","disadvantages-of-momentum-investing#Disadvantages of Momentum Investing":"However, momentum investing comes with its challenges, as the risk-return trade-off inherent to all strategies applies here as well.","etfs#ETFs":"There are several alternatives that enable retail investors to access momentum strategies without requiring hands-on management.","exploiting-market-volatility#Exploiting Market Volatility:":"Momentum investors thrive on volatile market trends, targeting stocks on the rise and selling before prices decline. Staying ahead of the curve allows for maximum return on investment.","high-turnover-costs#High Turnover Costs:":"Frequent trading can lead to substantial fees, which remain a concern for new investors despite low-cost brokerage options.","managing-your-position#Managing Your Position":"Mastering position management takes practice, as securities well-suited for momentum trades often have wide bid/ask spreads. These spreads require more substantial price movements to generate profits, and the large intraday fluctuations can trigger stop-losses, even if the overall trend remains intact. Carefully select your holding period, as risk escalates with longer positions.","market-sensitivity#Market Sensitivity:":"Momentum strategies are more effective in bull markets, where investor herding behavior is more pronounced. In bear markets, increased caution among investors reduces profit margins.","momentum-etfs#Momentum ETFs:":"A Passive Solution Momentum-focused ETFs apply rules-based methodologies to identify and invest in stocks with strong price momentum, effectively streamlining and automating the momentum investment approach.\nInvestopedia has built a chart listing some of the ETFs that people can consider.\nSymbol Description MTUM Tracks an index of large/mid cap U.S. stocks with higher price momentum. MMTM The fund invests at least 80% of its assets in the index, which reweights the S\u0026P Composite 1500 Index to overweight stocks with strong momentum and underweight those with weak momentum. The fund is non-diversified. PDP The fund invests at least 90% of its assets in the underlying index, which includes about 100 securities selected from the largest 1,000 by market capitalization in the NASDAQ U.S. benchmark index. SPMO The fund invests at least 90% of its assets in the underlying index, which includes around 100 S\u0026P 500® stocks with the highest “momentum score.” Momentum investing focuses on securities with strong recent performance. The fund is non-diversified. I recommend SPMO!","opt-for-liquid-securities#Opt for liquid securities":"When employing momentum strategies, prioritize liquid securities. Avoid leveraged or inverse ETFs, as their price movements often fail to accurately reflect underlying indices or futures markets due to their intricate construction. While regular funds are reliable trading options, they typically deliver smaller percentage gains and losses compared to individual stocks.\nFocus on securities with a daily trading dollar volume is high, as many well-known stocks meet this standard. Occasionally, low-float stocks can become highly liquid when significant news breaks, driving heightened volatility and attracting diverse market participants through strong emotional reactions.\nKeep an eye out for the “flavor of the day”—new products, divisions, or concepts that captivate public interest, compelling analysts to discard traditional models and recalculate profit projections. Biotech firms and small- to mid-sized tech companies are particularly abundant sources of such compelling story stocks.","risk-of-poor-timing#Risk of Poor Timing:":"Like riding ocean waves, a mistimed purchase can leave an investor stranded. Most momentum traders accept this risk in pursuit of higher returns.","securing-profits#Securing Profits":"Take profits when the price surges into an overextended technical state. This is often indicated by a sequence of vertical bars on a 60-minute chart or when the price breaches the third or fourth standard deviation of a top or bottom 20-day Bollinger Band.\nConsider tightening stop-losses or executing a blind exit upon reaching key technical levels, such as a major trendline or a previous high/low. Partial profit-taking or exiting altogether is prudent when indicators suggest potential trend reversals.","short-term-high-profit-potential#Short-Term High Profit Potential:":"Momentum investing offers the chance for substantial gains over short periods. For example, buying a stock at $50 and selling at $75 based on an overoptimistic analyst report could generate a 50% return before the stock corrects. Over time, the cumulative profit potential can be immense.","time-intensive-approach#Time-Intensive Approach:":"Momentum investing requires constant monitoring of market updates, often on a daily or hourly basis, to quickly respond to news or trends that could influence stock prices.","what-is-momentum-investing#What is Momentum investing?":"Momentum investing capitalizes on market volatility by entering short-term positions in rising stocks and selling them at the first signs of decline. The investor then reallocates the capital to fresh opportunities. In this analogy, market volatility resembles ocean waves, with a momentum investor riding the crest of one wave and swiftly transitioning to the next before the first wave crashes.\nMomentum investors also exploit the phenomenon of investor herding, positioning themselves at the forefront to profit and exit before the crowd. Below, you’ll find a table outlining how momentum investing compares with other popular strategies."},"title":"Momentum Trading"},"/SridharBlog/posts/monkeycachesettings/":{"data":{"":"What is Monkey Cache? MonkeyCache is a lightweight caching library for .NET applications, created. It’s designed to make caching flexible and straightforward—especially useful when you want to store data temporarily without setting up a full-blown database or complex infrastructure.\nKey Features Simple API: Uses a concept called a “Barrel” to store and retrieve cached items.\nTime-based expiration: Cached items can specify how long an item should stay in the cache.\nMultiple storage backends:\nMonkeyCahce.FileStore: Store data in local files.\nMonkeyCache.SQLite: Uses SQLite for structured caching.\nMonkeyCache.LiteDB: Uses LiteDB for document-style caching.\nCross-platform support: Can be used with all applications built on .NET 8+.\nWhy use it? Perfect for scenarios like:\nCaching API responses\nStoring user preferences or session data\nReducing redundant network calls in mobile apps.\nIt’s convenient in mobile or desktop apps where you want quick, local caching without spinning up a whole database.\nHow do I use it:","how-do-i-use-it#How do I use it:":"","initialization#Initialization:":"I usually have a multi-project solution and multiple projects running when I launch the application. Hence, to limit my code repetition, I initialize MonkeyCache using the following code:\npublic static class CacheInitialize { public static void Initialize(string applicationName) { Barrel.ApplicationId = applicationName; var cachePath = Environment.GetEnvironmentVariable(\"HOME\"); if (RuntimeInformation.IsOSPlatform(OSPlatform.Linux)) { cachePath += $\"/cache/DbData/{applicationName}\"; DirectoryInfo di; if (!Directory.Exists(cachePath)) { di = Directory.CreateDirectory(cachePath); } else { di = new DirectoryInfo(cachePath); } if (di.Exists) { Barrel.Create(cachePath); } } else { cachePath = Path.GetTempPath(); cachePath = Path.Combine(cachePath, applicationName); Barrel.Create(cachePath); } } } The code is designed to operate seamlessly on both Linux and Windows platforms. It detects the operating system and creates the database file accordingly. Please note that the application establishes the cache within a directory named after the application.","key-features#Key Features":"","storing-and-data-retrieval#Storing and data retrieval:":"The following code is the one I use for data storage and retrieval.\npublic static class HandleCache { private static Lock? lockingObj; private static IBarrel? currentBarrel; public static string? GetString(string key) { currentBarrel ??= Barrel.Current; lockingObj ??= new Lock(); lock (lockingObj) { if (!currentBarrel.Exists(key) || currentBarrel.IsExpired(key)) { currentBarrel.EmptyExpired(); return null; } return currentBarrel.Get\u003cstring\u003e(key); } } public static void SaveString(string key, string value, TimeSpan timeSpan) { currentBarrel ??= Barrel.Current; lockingObj ??= new Lock(); lock (lockingObj) { currentBarrel.Add(key, value, timeSpan); } } } I’m using a simple object locking to ensure that it can be used in a multi-process application. I had issues in the past with multi-process applications and still maintain the lock.\nThis code saves and retrieves only a string object. I use JsonSerializer.Deserialize and JsonSerializer.Serialize to convert my objects into string objects.\nIn some of the applications, I use Templates, but the code is a little complex. Sharing the Template code below:\npublic class HandleCache : IHandleCache { private static object? lockingObj; private readonly IBarrel currentBarrel; private readonly HttpClient httpClient; private readonly ILogger logger; public HandleCache(HttpClient httpClient, ILogger logger) { this.httpClient = httpClient; httpClient.Timeout = TimeSpan.FromSeconds(5); this.logger = logger; currentBarrel = Barrel.Current; } public async Task GetAsync(string url, CacheDuration cacheDuration = CacheDuration.Minutes, int duration = 60, bool forceRefresh = false, string nullReplace = \"\") { string json = await GetStringAsync(url, cacheDuration, duration, forceRefresh); string pattern = @\"Date\\\"\": \\\"\"None\\\"\"\"; string substitution = \"Date\\\": \\\"1900-01-01\\\"\"; RegexOptions options = RegexOptions.Multiline; Regex regex = new Regex(pattern, options); json = regex.Replace(json, substitution); json = JsonNode.Parse(json)!.ToString(); if (!string.IsNullOrWhiteSpace(nullReplace)) { json = Regex.Replace(json, nullReplace, \"\\\"0\\\"\"); } try { if (!string.IsNullOrWhiteSpace(json)) { T? retObj = JsonSerializer.Deserialize(json); if (retObj != null) { return retObj; } } } catch (Exception ex) { logger.LogError($\"Error while processing data from {url}\\n\"); logger.LogError(ex.Message); } return (T)Activator.CreateInstance(typeof(T), Array.Empty\u003cobject\u003e())!; } public async Task\u003cstring\u003e GetStringAsync(string url, CacheDuration cacheDuration = CacheDuration.Minutes, int duration = 60, bool forceRefresh = false) { string json = string.Empty; try { lockingObj ??= new object(); lock (lockingObj) { currentBarrel.EmptyExpired(); } if (!forceRefresh \u0026\u0026 !currentBarrel.IsExpired(url)) { json = currentBarrel.Get\u003cstring\u003e(url); } if (string.IsNullOrWhiteSpace(json)) { json = await httpClient.GetStringAsync(url); lock (lockingObj) { var expiring = cacheDuration switch { CacheDuration.Seconds =\u003e TimeSpan.FromSeconds(duration), CacheDuration.Minutes =\u003e TimeSpan.FromSeconds(duration), CacheDuration.Hours =\u003e TimeSpan.FromHours(duration), CacheDuration.Days =\u003e TimeSpan.FromDays(duration), _ =\u003e TimeSpan.FromMinutes(duration), }; currentBarrel.Add(url, json, expiring); } } json = json.Replace(\":\\\"N/A\\\"\", \":null\"); return json; } catch (Exception ex) { logger.LogError($\"Error while processing data from {url}\", ex); return string.Empty; } } } And the interface I use to inject:\npublic interface IHandleCache { Task GetAsync(string url, CacheDuration cacheDuration = CacheDuration.Minutes, int duration = 60, bool forceRefresh = false, string nullReplace = \"\"); Task\u003cstring\u003e GetStringAsync(string url, CacheDuration cacheDuration = CacheDuration.Minutes, int duration = 60, bool forceRefresh = false); } public enum CacheDuration { Seconds, Minutes, Hours, Days } Let’s not go over the details of what this code does, but I can confirm it works. If you’re interested, feel free to use it.","what-is-monkey-cache#What is Monkey Cache?":"","why-use-it#Why use it?":""},"title":"Monkey Cache Settings"},"/SridharBlog/posts/nginxdotnetcore/":{"data":{"":"","almost-done#Almost done":"Let’s make sure our configuration changes are good by issuing the command\nsudo nginx -t And if nginx states you’ve done an excellent job then let us restart the server by issuing the command\nsudo systemctl restart nginx Navigate to your site using the https link (e.g. https://stockfilter.de) and if things are good then its time for your coffee break :coffee:.\nElse Stackoverflow! :anguished:","build-your-application#Build your application":"By now you should have your application fully tested and ready for deployment. Build your application with the command\ndotnet publish -c Release Once successfully built the executables will be in\n........./bin/Release/netX.X/ folder. We want this application to start on startup as a service and the best way to start the application and restart it if needed will be to run it as a Daemon application. To do that we will create a file in\n/etc/systemd/system/ folder. Though you can create the file with any name I like the naming convention\nApplicationName.service The content of this file will be as follows (adjust it as per your needs)\n[Unit] Description= StockSelector [Service] User=srvean Group=srvean WorkingDirectory=/home/srvean/DotNet/Stock-Selector-UI/StockSelector/Server/ ExecStart=/usr/bin/dotnet run --configuration Release Restart=always # Restart service after 60 seconds if the dotnet service crashes: RestartSec=60 SyslogIdentifier=StockSelector Environment=ASPNETCORE_ENVIRONMENT=Production [Install] WantedBy=multi-user.target Most of the values above should be self-explanatory. The working directory could have been the bin/Release/netx/ folder but didn’t try dotnet run xxxx.dll command. Going forward we’ll assume that the application listens on port 5000 for HTTP requests. Now let us enable the server and run it:\nsudo systemctl enable ApplicationName.service sudo systemctl restart ApplicationName.service sudo systemctl status ApplicationName.service Well, you know what we are doing above.","configure-nginx#Configure Nginx":"To configure Nginx as a reverse proxy to forward HTTP requests to your ASP.NET Core app, modify /etc/nginx/sites-available/default. Open it in a text editor, and replace the contents with the following snippet:\nserver { listen 80; server_name stockfilter.de; location / { proxy_pass http://127.0.0.1:5000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection keep-alive; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } You can use the above code as is; only change the value for server_name. After updating the file you can run the command:\nsudo nginx -t to test if the configuration is free of manual errors. Now let us restart the server with command\nsudo systemctl restart nginx Now if you navigate to your site (http not https) your Dot Net application should be running. If you do not get the expected results =\u003e Stackoverflow!","dns-resolve#DNS Resolve":"Click on the name of the new domain you created in Google Domain and on the left menu click on DNS. Select Default name servers and Manage custom records. Here we are going to create an A Record and a CNAME Record. (A record is domain name to IP address mapping and CNAME points to another name usually the value on A record).","how-was-stock-filter-installed#How was Stock filter installed?":"","install-required-software#Install required software":"This is simple; we are going to issue the following commands.\nsudo apt install nginx sudo apt install certbot sudo apt install python3-certbot-nginx Lets assume you worked on the CNAME and A Record(s); if not take a walk. This might be good time to ensure your installation is working fine until now. You should be greeted with the default Nginx web page and if you get see any errors then Stackoverflow!","introduction#Introduction":"Stockfilter is a Dot Net application that uses the inbuilt webserver Kestrel to host the application. Kestrel is a light weight web server for hosting ASP.NET Core applications on really any platform. Kestrel lacks a lot of the things that an ASP.NET web developer might have come to expect from a web server like IIS. For instance you cannot do SSL termination with Kestrel or URL rewrites or GZip compression.\nNginix is a full-fledged web server. You can use it as a reverse proxy; in this configuration it takes load off your actual web server by preserving a cache of data which it serves before calling back to your web server. As a proxy it can also sit in front of multiple end points on your server and make them appear to be a single end point. This is useful for hiding a number of microservices behind a single end point. It can do SSL termination which makes it easy to add SSL to your site without having to modify a single line of code.\nThis document was prepared using the procedures mentioned in Microsoft docs and following a YouTube by Ayden’s Developer Channel.","proxy#Proxy":"Add the /etc/nginx/proxy.conf configuration file:\nproxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffers 32 4k; Replace the contents of the /etc/nginx/nginx.conf configuration file with the following file. The example contains both http and server sections in one configuration file.\nhttp { include /etc/nginx/proxy.conf; limit_req_zone $binary_remote_addr zone=one:10m rate=5r/s; server_tokens off; sendfile on; # Adjust keepalive_timeout to the lowest possible value that makes sense # for your use case. keepalive_timeout 29; client_body_timeout 10; client_header_timeout 10; send_timeout 10; upstream helloapp{ server 127.0.0.1:5000; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name example.com *.example.com; ssl_certificate /etc/ssl/certs/testCert.crt; ssl_certificate_key /etc/ssl/certs/testCert.key; ssl_session_timeout 1d; ssl_protocols TLSv1.2 TLSv1.3; ssl_prefer_server_ciphers off; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_stapling off; add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; #Redirects all traffic location / { proxy_pass http://helloapp; limit_req zone=one burst=10 nodelay; } } } In the above file you need to update ssl_certificate and ssl_certificate_key. We can get its values from /etc/nginx/sites-available/default file that we updated earlier. After we updated when we ran Certbot it would have added a few lines after our edit and in the inserts that Certbot inserted you would see entries for ssl_certificate and ssl_certificate_key. Copy those values and replace it in the above code. Also replace example.com in the above code to your domain (e.g. stockfilter.de).","register-your-domain#Register your domain":"For this application I used Google domain to register the domain name Stock Filter.","securing-our-site#Securing our site.":"To automate the process of obtaining, installing, and updating TLS/SSL certificates on a web server, Let’s Encrypt is an extremely useful tool. It is a certificate authority (CA) that comes packaged with a corresponding software client, Certbot, that will automatically install TLS/SSL certificates. This means that we can run encrypted HTTPS on a web server without having to worry about constant maintenance.\nWe’ll relate Certbot and our instance of Nginx using the following command:\nsudo certbot --nginx -d stockfilter.de Obviously, the value for -d in the above command will be for our domain name. Certbot will ask a few straightforward questions which need to be answered."},"title":"NginxDotNetCore"},"/SridharBlog/posts/ollamatocs/":{"data":{"":"","cleint-application#Cleint Application":"We are going to use a Nuget package, OllamaSharp, to talk to our local LLM server. I wanted to use Microsoft’s Semantic Kernel to connect to Ollama, but I need to do more work to make it happen. For now, let us be happy with this package, and when we hit a wall with it, we’ll try Semantic Kernel once again. In Visual Studio, create a console application and add the following packages:\nInstall-Package Lyndychivs.Fibonacci Install-Package OllamaSharp Fibonacci is more of a gimmick, but why not? It makes the terminal behave a little differently from regular chat applications. The following code does not require much explanation; once you read it, you’ll know exactly what it is doing.\nusing Fibonacci; using OllamaSharp; using System.Numerics; using System.Text; var endpoint = new Uri(\"http://rbpi-5-1:11434\"); var modelId = \"gemma2:2b\"; var Fibonacci = new FibonacciRecursive(); List sequence = Fibonacci.GetFibonacciSequence(15); int sequenceCount = sequence.Count(); Random rand = new Random(DateTime.Now.Second); var ollama = new OllamaApiClient(endpoint, modelId); var chat = new Chat(ollama); Console.ForegroundColor = ConsoleColor.Green; Console.WriteLine(\"AI: Now How can I help?\"); Console.ForegroundColor = ConsoleColor.Yellow; Console.Write(\"User: \"); var helpTopic = Console.ReadLine(); var outMsg = new StringBuilder(); while (!string.IsNullOrEmpty(helpTopic)) { outMsg.Clear(); Console.ForegroundColor = ConsoleColor.Green; Console.Write(\"AI: \"); await foreach (var answerToken in chat.Send(helpTopic)) { await Task.Delay(50); outMsg.Append(answerToken); if (outMsg.Length \u003e= sequence[rand.Next(0, sequenceCount)]) { Console.Write(outMsg.ToString()); outMsg.Clear(); } } Console.WriteLine(outMsg.ToString()); Console.ForegroundColor = ConsoleColor.Yellow; Console.Write(\"\\nUser: (bye or nothing to exit chat)\"); helpTopic = Console.ReadLine() ?? string.Empty; if (helpTopic.Equals(\"bye\", StringComparison.OrdinalIgnoreCase)) { break; } } Console.ForegroundColor = ConsoleColor.White;","installation#Installation":"Let us start by installing Ollama on your Raspberry Pi (RbP). I’ve named my RbP rbpi-5-1, and if you see “rbpi-5-1” in the rest of the document, I’m indicating my RbP.\nWe’ll start by visiting Ollama’s website and copy the curl script provided for Linux installation.\ncurl -fsSL https://ollama.com/install.sh | sh Run the above curl command on your RbP, and it takes care of installing Ollama on your RbP. The first step is completed! It is time to check out the installation. We invoke Ollama with the command:\nollama run {model_name} Several models that are available, each specializing in a different subject. For example, codellama focuses more on coding, while wizard-math focuses on math and logic problems.\nWith my vast experience using Ollama, which is 3 hours at the time of this writing, gemma2 is the best for me. It has a wide range of applications across various industries and domains. It is good in\nText Generation Chatbots and conversational AI Text summarization These are my needs as of now, and hence Heil gemma2!","prerequests#Prerequests":"Here are the instructions for connecting to Ollama running on my Raspberry Pi from my C# application. I will provide step-by-step instructions for installing Ollama and connecting it to a C# application. Please ensure that your Raspberry Pi is already running some version of Linux and that you have Visual Studio installed on your computer.","service-fix#Service fix":"Ollama is now running as a systemd service. Hence, the environment variables should be using systemctl: we need to allow access to other machines to use your ollama instance, and hence, we need to open the port for external requests. I tried to follow Ollama’s FAQ verbatim, and I failed. So I suggest you follow my suggestion.\nsudo systemctl stop ollama.service sudo systemctl disable ollama.service cd /etc/systemd/system Now, in this folder, we are going to edit the file ollama.service. In this file, under section Service, add the following line:\nEnvironment=\"OLLAMA_HOST=0.0.0.0\" After editing, my file looked as follows: yours will be different, especially in the path, but it gives you an idea of how it should look.\n[Unit] Description=Ollama Service After=network-online.target [Service] ExecStart=/usr/local/bin/ollama serve Environment=\"OLLAMA_HOST=0.0.0.0\" User=ollama Group=ollama Restart=always RestartSec=3 Environment=\"PATH=/home/sridh/bin:./:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/home/sridh/bin:/home/sridh/.dotnet/tools\" [Install] WantedBy=default.target Restart ollama as follows:\nsudo systemctl enable ollama.service sudo systemctl restart ollama.service I prefer using the restart command over the start command. Feel free to choose whichever option you prefer. I won’t judge you! The last thing we’ll be doing when we are still logged in your RbP will be checking if you can access your RbP from your computer. In my case, in my browser, I navigated to my rbpi-5 using the following link:\nhttp://rbpi-5-1:11434/ I got a beautiful welcome message:\nOllama is running We can now ignore the terminal window for now and work with Visual Studio.","testing#Testing":"Having installed Ollama, let us do some basic testing. Invoke Ollama’s CLI frontend with the command:\n# Let's use the 2b model - the smallest available! ollama run gemma2:2b If this is your first time running this command, it is time to make a cup of coffee. It has to download the model, parse it, and eventully you will get the prompt.\n\u003e\u003e\u003e Send a message (/? for help) Ask your first question: if you go blank like me, here are some examples.\nWho was the first person to land on the moon? Why is the blood red? Why is the sky blue? If your new chatty friend answers correctly, you are good for the next step. If not abort!"},"title":"Connecting to Ollama running on Raspberry Pi"},"/SridharBlog/posts/openvpndockerubuntu/":{"data":{"":"The following instructions are based on this Youtube video","install-openvpn-and-configure-it#Install OpenVPN and configure it":"Your server should have rebooted by now; if not time for a coffee break. Okay, let’s roll up our sleeves!\nHold on! Not so fast. Let’s confirm we are ready.\ndocker run hello-world Make sure we get no errors and now let us prepare to use openVPN.\ndocker pull kylemanna/openvpn This should pull the necessary image to run openvpn. Make a note of the servers’ public IP. We’ll be using it in one of the following commands. To keep all VPN content in one folder create a folder, say openVPN, and cd to that folder: now let’s run the following commands:\n# Create and initialize openvpn docker run --rm -v $PWD:/etc/openvpn kylemanna/openvpn ovpn_genconfig -u udp:// # Create certificate docker run --rm -v $PWD:/etc/openvpn -it kylemanna/openvpn ovpn_initpki # it might ask you to provide phrases to encrypt the key; use a strong key and save it; you'll need it later. # Create a client account docker run --rm -v $PWD:/etc/openvpn -it kylemanna/openvpn easyrsa build-client-full # Copy client certificate (ovpn file) from container docker run --rm -v $PWD:/etc/openvpn kylemanna/openvpn ovpn_getclient \u003e .ovpn At this stage, we should have a file .ovpn in the openVPN folder. Transfer this file to your client’s machine. Finally, start the Open VPN server:\ndocker run --name openvpn -v $PWD:/etc/openvpn -d -p 1194:1194/udp --cap-add=NET_ADMIN --restart always kylemanna/openvpn # Verify it is running docker ps All done; check if you can use the VPN server!","introduction#Introduction":"When I want to virtually travel to a different country I need to create a VPN server in that region and I often land up watching Youtube videos or installing the software wrong. So I’m creating this gist so that I will not be lost when I install my next OpenVPN server on a VM.","legacy-write-up#Legacy write-up":"","platform#Platform":"Though we can use any VM, I prefer using LightSail offering from AWS to running my OpenVPN. Necessary changes need to be done (only in IP Firewall settings) based on the selected platform. I usually select the smallest server, (512 MB RAM, 1 vCPU, 20 GB SSD) to run my OpenVPN as it is going to serve only one client me.","prepare-the-server#Prepare the server":"The first change I do on any server is to edit the .bashrc file. I had the following two lines at the bottom of the file\nPATH=./:~/bin/:$PATH set -o vi Log out and log in to the server to load the changes to the environment.","security#Security":"Anyone accessing your IP address can download the ovpn file and use your VPN server. So, for security, it would be better to shut down the server or block port 80 when not in use.","start-installing-packages#Start installing packages":"It’s time to install the necessary packages; I like to use nala instead of apt so I start with installing nala.\nsudo apt install nala Next, let us install docker:\nnala install docker.io After installing docker the system might request you reboot but let us issue one more command and then reboot the server; we’ll add the current user to the user group docker and then reboot the server.\nsudo usermod -aG docker $USER sudo reboot now While the server is rebooting we need to open port 1194 so that your PC can communicate with the VPN server. Select your server in the Lightsail console as shown in the following image. In the following image, the name of my Server is OpenVPN and I’ll click on it. You will be presented with your server’s console and on this screen click on the Network menu item: Update firewall rules so that we can use port 1194 using UDP protocol. In the image below I’m opening ports 1190 to 1199 (don’t ask me why; not necessary but I like to do it this way). Once you’ve installed docker, let us use a different docker image, which automates most of the manual work mentioned above. Install the alekslitvinenk/openvpn as follows:\ndocker run -itd --rm --cap-add=NET_ADMIN \\ -p 1194:1194/udp -p 80:8080/tcp \\ -e HOST_ADDR=$(curl -s https://api.ipify.org) \\ --name dockovpn alekslitvinenk/openvpn Make sure the VM has port 80 open for HTTP (default on with LightSail) and 1194 (details above) open. On your laptop/desktop, navigate to http://Your servers IP address, and your client profile will be downloaded to your local machine (the ovpn file). Use the profile to connect to your VPN server as mentioned above.","system-update#System Update":"Let us bring the latest updates to the operating system; I, almost always, use Ubuntu and I run the following commands:\nsudo apt update \u0026\u0026 sudo apt upgrade -y sudo reboot now Of course, Ubuntu will ask you a couple of questions during the update and reboot requests; answer accordingly. Once the server reboots:\nsudo do-release-upgrade sudo reboot now As usual, the OS will ask for your confirmations every now and then; answer them to ensure the installation completes. It depends on the server location and available resources but until now you should have spent around 30 to 45 minutes bringing up the server!","windows-machine#Windows Machine":"If your router supports IPV6, you need to do one more setup. Your VPN will mask your IPV4 but not IPV6. Further browsers like Microsoft Edge happily share your location, and hence, in short, “Operation successful patient died”!\nHow do we address this issue?\nRun the following command on your Windows machine:\nncpa.cpl This opens the classic Network Connections window directly.\nFrom there, right-click your active adapter-\u003eProperties-\u003eUncheck IPv6 and click OK.\nUse a privacy-focused browser like Brave and ensure your IP address and location are masked.\nOnce you disconnect from your VPN, turn on IPv6!"},"title":"Installing open VPN in an VM"},"/SridharBlog/posts/portfolio-analyzer/":{"data":{"":"","1-project-overview#1. Project Overview":"The goal of this project is to build a web application using C# and Blazor that allows users to easily track the performance of their stock portfolio. Users will be able to input their stock holdings and purchase history, and the application will then fetch real-time (or near real-time) stock data to calculate portfolio value, returns, and present insightful visualizations. The application aims to provide a clear and straightforward view of investment performance without overwhelming the user with excessive complexity.","2-technical-details-c-with-blazor#2. Technical Details (C# with Blazor)":"You’ve chosen a fantastic tech stack! Here’s how the application could be structured:\nBlazor Frontend (Client-Side or Server-Side): Components: You’ll likely create several Blazor components for different parts of the application: PortfolioInput.razor: For users to add or edit their stock holdings and purchase details. PortfolioDisplay.razor: To show the current portfolio holdings with real-time data. PerformanceCharts.razor: To render visualizations of portfolio performance over time. TransactionHistory.razor: To display a log of the user’s buy and sell transactions. StockDetails.razor: Potentially to show more detailed information about individual stocks. Layout components for the overall structure of the application. State Management: Consider how you’ll manage the application’s state, especially the user’s portfolio data and fetched stock quotes. Options include: Simple State Management: For a medium-complexity project, you might start with passing state down through component parameters and using EventCallback for child-to-parent communication. Flux/Redux-like patterns: Libraries like Fluxor or Blazor State offer more structured approaches for larger applications but might be overkill initially. Scoped Services: Using scoped services to hold and manage user-specific data can be a good middle ground. UI Framework: Blazor leverages standard HTML and CSS. You can enhance the styling and responsiveness with CSS frameworks like Bootstrap or Tailwind CSS, or Blazor UI component libraries like MudBlazor or Radzen Blazor Components. Backend (ASP.NET Core Web API): Controllers: You’ll need API controllers to handle requests from the Blazor frontend: PortfolioController: Endpoints for adding, retrieving, updating, and deleting portfolio holdings. StockDataController: Endpoint(s) to fetch real-time stock quotes and potentially historical data. Services: Create services to encapsulate business logic: PortfolioService: To manage portfolio data (calculations, persistence). StockDataService: To interact with the chosen stock data API. Data Access: Use Entity Framework Core (EF Core) to interact with your database. Define your data models (e.g., PortfolioItem, Transaction). Authentication and Authorization: Implement user authentication to secure user data. ASP.NET Core Identity is a robust choice. Database: Choose a relational database like PostgreSQL, MySQL, or SQL Server to store user data, portfolio holdings, and transaction history.","3-data-source#3. Data Source":"For fetching real-time or near real-time stock data, you’ll need to integrate with a financial data API. Here are some popular options:\nFree/Limited Free Options: Yahoo Finance API (Unofficial Libraries): While Yahoo Finance doesn’t have an official public API, there are several community-maintained libraries in C# that can scrape or access its data (be mindful of terms of service and potential instability). Alpha Vantage: Offers a free tier with limitations on API calls and data frequency. Provides real-time and historical stock data. Financial Modeling Prep: Offers a free tier with limited API calls and data. Paid Options (More Reliable and Feature-Rich): IEX Cloud: A popular option with various data plans and a well-documented API. Polygon.io: Offers real-time and historical market data with different pricing tiers. Refinitiv Eikon API: A professional-grade API with comprehensive financial data (typically higher cost). Bloomberg API: Another professional-grade option (also typically higher cost). Considerations for Data Source:\nReal-time vs. Delayed Data: Decide if you need truly real-time data or if a slight delay (e.g., 15-minute delayed data offered by some free APIs) is acceptable for your “simplified” tracker. API Limits: Be aware of rate limits on free APIs and design your application to handle them gracefully (e.g., implement caching, backoff strategies). Data Coverage: Ensure the API covers the stock exchanges and instruments your target users are likely to trade. Historical Data: If you plan to implement charting of past performance, you’ll need an API that provides historical data.","4-portfolio-details#4. Portfolio Details":"","a-user-provided-details-input#a. User-Provided Details (Input)":"When a user adds a stock to their portfolio, they will likely need to provide the following information:\nInstrument Details: Ticker Symbol (Required): The unique identifier of the stock (e.g., AAPL, MSFT, GOOGL). Company Name (Optional, but helpful for display): The full name of the company. You might consider fetching this automatically based on the ticker symbol using the data API. Exchange (Optional, but can be important for disambiguation): The stock exchange where the stock is traded (e.g., NASDAQ, NYSE). Some APIs might require this for accurate data retrieval. Procurement Details (Transaction History): Transaction Type (Buy/Sell): To track purchases and sales. Purchase Date (for Buy transactions): The date the stock was acquired. Purchase Price (per share, for Buy transactions): The price at which the stock was bought. Quantity (Number of shares): The number of shares bought or sold. Transaction Costs/Fees (Optional): Any brokerage fees associated with the transaction. You’ll need a user interface that allows users to input this information easily, potentially with validation to ensure data integrity. You might also consider allowing users to edit or delete their transactions.","b-details-to-be-presented-to-the-user-output#b. Details to be Presented to the User (Output)":"Based on the input data and fetched stock information, here are some details you can present to the user:\nCurrent Holdings:\nTicker Symbol: The stock ticker. Company Name: The name of the company. Current Price: The latest stock price fetched from the data API. Quantity Held: The total number of shares currently owned. Current Value: The current price multiplied by the quantity held. Average Cost Basis (per share): The average price paid for all shares of that stock (total cost of purchases divided by total shares bought). Total Cost Basis: The total amount invested in that stock. Unrealized Gain/Loss (Dollar Amount): Current Value minus Total Cost Basis. Unrealized Gain/Loss (Percentage): ((Current Value - Total Cost Basis) / Total Cost Basis) * 100. Portfolio Summary:\nTotal Portfolio Value: The sum of the current value of all holdings. Total Investment: The sum of the cost basis of all holdings. Overall Gain/Loss (Dollar Amount): Total Portfolio Value minus Total Investment. Overall Gain/Loss (Percentage): ((Total Portfolio Value - Total Investment) / Total Investment) * 100. Charts:\nPortfolio Value Over Time: A line chart showing how the total portfolio value has changed over a selected period (e.g., 1 month, 6 months, 1 year, YTD, All Time). This will require fetching historical stock data. Asset Allocation (Optional): A pie chart showing the percentage of the portfolio allocated to different stocks. Investment Returns:\nDaily/Weekly/Monthly Returns (Percentage or Dollar Amount): Showing the change in portfolio value over specific periods. Time-Weighted Return (Optional, more complex): A more accurate measure of investment performance that removes the impact of cash flows (deposits and withdrawals). This is generally more advanced. Analyst Estimates (Optional, adds complexity):\nTarget Price: Analyst consensus on the expected future price of the stock. Buy/Hold/Sell Ratings: Summary of analyst recommendations. Note: This data is often available through paid APIs and can be less reliable or consistent across different sources. Consider if this is crucial for your “simplified” version. Transaction History:\nA table displaying all buy and sell transactions with details like date, ticker, quantity, price, and total cost/proceeds. Further Considerations:\nUser Interface (UI) and User Experience (UX): Focus on creating a clean, intuitive, and responsive interface using Blazor components and styling. Error Handling: Implement proper error handling for API calls, data parsing, and user input. Data Persistence: Use EF Core to save and retrieve user portfolio data from your chosen database. Asynchronous Operations: Use async and await effectively in Blazor to handle API calls and prevent blocking the UI thread. Code Organization: Structure your C# code logically with services, controllers, and data models to maintain readability and scalability.","detailed-specification#Detailed Specification":"","preamble#Preamble":"This high-level requirement was generated using my prompts, with the AI providing its interpretations of my bullet points. My implementation of the Portfolio Tracker may diverge from the original specifications. However, I must establish a target, and I hope that the final product will incorporate most of the defined objectives. Once I execute my project, I will enumerate the changes and articulate how they differ from the initial goals."},"title":"Simplified Stock Portfolio Tracker and Analyzer"},"/SridharBlog/posts/semantickernelrag/":{"data":{"":"","sematic-kernel-with-rag#Sematic Kernel with RAG":"Okay, let’s break down how to use Semantic Kernel with Retrieval Augmented Generation (RAG) in a Blazor application. It can be a bit tricky to get all the pieces working together, so we’ll go step by step.","understanding-the-core-concepts#Understanding the Core Concepts":"Semantic Kernel: This is a framework that helps you build intelligent applications by integrating Large Language Models (LLMs) like OpenAI, Azure OpenAI, etc. It provides abstractions for things like plugins, memory, and planners. RAG (Retrieval Augmented Generation): This approach improves LLM responses by first retrieving relevant information from a data source (like a vector database) and then using that information as context when generating the final output. This helps the LLM ground its responses in factual data and reduce hallucinations. Blazor: This is a framework for building interactive web UIs with C# instead of JavaScript. We’ll use it to create the front-end for your RAG-powered application. High-Level Steps\nHere’s a general outline of what we’ll need to do:\nSet up your environment: Install the necessary NuGet packages and configure your API keys. Prepare your data source: Choose a vector database (or similar) and index your data. Create Semantic Kernel components: Build your skills (plugins), memory, and kernel. Develop your Blazor UI: Create a simple interface for users to ask questions. Connect Semantic Kernel with Blazor: Send user input to the kernel, get the RAG-enhanced response, and display it in the UI. Let’s Get Started with the Code\nI’ll provide code snippets with explanations and comments. This will be for a simple Blazor Server app for demonstration purposes.\n1. Setting up your Environment\nCreate a New Blazor Server Project:\nIn Visual Studio or using the .NET CLI:\ndotnet new blazorserver -o MyRAGApp cd MyRAGApp Install NuGet Packages:\ndotnet add package Microsoft.SemanticKernel dotnet add package Microsoft.SemanticKernel.Connectors.Memory.Qdrant dotnet add package Microsoft.SemanticKernel.Connectors.OpenAI (Choose Qdrant or a memory connector of your preference)\nMake sure you have System.Text.Json package included as well. 2. Configure App Settings (appsettings.json)\n{ \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } }, \"AllowedHosts\": \"*\", \"SemanticKernel\": { \"OpenAI\": { \"ApiKey\": \"YOUR_OPENAI_API_KEY\", \"OrgId\": \"YOUR_OPTIONAL_OPENAI_ORG_ID\", \"ModelId\": \"gpt-3.5-turbo\", \"EmbeddingModelId\": \"text-embedding-ada-002\" }, \"Qdrant\":{ \"Endpoint\":\"http://localhost:6333\", \"VectorSize\":1536, \"CollectionName\":\"ragcollection\" } } } Replace the placeholder with the api key of your preferred LLM API, e.g., OpenAI. Make sure you also replace the placeholder with the correct values of your memory vector database. 3. Create Data Loader (optional)\nFor simplicity, let’s assume you have a simple text file of data to be indexed, say knowledge_base.txt. Create a DataLoader service (Services/DataLoader.cs): using System.Text; using Microsoft.SemanticKernel.Memory; using Microsoft.Extensions.Options; using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Embeddings; using Microsoft.SemanticKernel.Connectors.Memory.Qdrant; namespace MyRAGApp.Services { public class DataLoader { private readonly IOptions _options; private readonly ILogger _logger; private readonly ITextEmbeddingGeneration _textEmbeddingGeneration; public DataLoader( IOptions options, ILogger logger, ITextEmbeddingGeneration textEmbeddingGeneration ) { _options = options; _logger = logger; _textEmbeddingGeneration = textEmbeddingGeneration; } public async Task LoadKnowledge(string filePath, IKernel kernel) { try { // Read content from text file var content = await File.ReadAllTextAsync(filePath, Encoding.UTF8); var lines = content.Split(Environment.NewLine, StringSplitOptions.RemoveEmptyEntries); var memoryStore = GetMemoryStore(); if(memoryStore != null) { foreach (var line in lines) { var id = Guid.NewGuid().ToString(); await kernel.Memory.SaveInformationAsync(_options.Value.Qdrant.CollectionName, line, id, line, null, _textEmbeddingGeneration); } _logger.LogInformation(\"Knowledge base loaded successfully\"); } else { _logger.LogError(\"Memory store is null. Cannot load the knowledge base\"); } } catch (Exception ex) { _logger.LogError($\"Error reading or loading data: {ex.Message}\"); } } private ISemanticTextMemory? GetMemoryStore() { if (_options.Value.Qdrant is not null) { return new QdrantMemoryStore( _options.Value.Qdrant.Endpoint, _options.Value.Qdrant.VectorSize ); } return null; } } } Add the corresponding SemanticKernelOptions class:\nusing System.ComponentModel.DataAnnotations; namespace MyRAGApp { public class SemanticKernelOptions { [Required] public OpenAIConfig? OpenAI { get; set; }\npublic QdrantConfig? Qdrant { get; set; } } public class OpenAIConfig { [Required] public string? ApiKey { get; set; } public string? OrgId { get; set; } [Required] public string? ModelId { get; set; } [Required] public string? EmbeddingModelId { get; set; } } public class QdrantConfig { [Required] public string? Endpoint { get; set; } [Required] public int? VectorSize { get; set; } [Required] public string? CollectionName { get; set; } } }\n* Make sure to add your knowledge base content in the `knowledge_base.txt` in the `wwwroot/` directory or other preferred location. **4. Kernel Setup** * **Create a Kernel service (`Services/KernelService.cs`):** ```csharp using Microsoft.SemanticKernel; using Microsoft.SemanticKernel.Connectors.OpenAI; using Microsoft.Extensions.Options; using Microsoft.SemanticKernel.Memory; using Microsoft.SemanticKernel.Embeddings; using Microsoft.SemanticKernel.Connectors.Memory.Qdrant; namespace MyRAGApp.Services { public class KernelService { private readonly IOptions _options; private readonly ILogger _logger; public KernelService(IOptions options, ILogger logger) { _options = options; _logger = logger; } public IKernel GetKernel() { try { var builder = Kernel.Builder; var apiKey = _options.Value.OpenAI?.ApiKey; var orgId = _options.Value.OpenAI?.OrgId; var modelId = _options.Value.OpenAI?.ModelId; var embeddingModelId = _options.Value.OpenAI?.EmbeddingModelId; if (string.IsNullOrEmpty(apiKey) || string.IsNullOrEmpty(modelId) || string.IsNullOrEmpty(embeddingModelId)) { _logger.LogError(\"OpenAI API settings are not correctly configured.\"); return null; } builder.AddAzureOpenAIChatCompletion(modelId, apiKey,orgId); builder.AddAzureOpenAITextEmbeddingGeneration(embeddingModelId, apiKey, orgId); builder.Services.AddTransient((sp) =\u003e GetMemoryStore()); return builder.Build(); } catch (Exception ex) { _logger.LogError($\"Error creating Kernel: {ex.Message}\"); return null; } } private ISemanticTextMemory? GetMemoryStore() { if (_options.Value.Qdrant is not null) { return new QdrantMemoryStore( _options.Value.Qdrant.Endpoint, _options.Value.Qdrant.VectorSize ); } return null; } } } 5. Register Services in Program.cs\nusing MyRAGApp; using MyRAGApp.Services; var builder = WebApplication.CreateBuilder(args); builder.Services.Configure(builder.Configuration.GetSection(\"SemanticKernel\")); // Add services to the container. builder.Services.AddRazorPages(); builder.Services.AddServerSideBlazor(); builder.Services.AddSingleton(); builder.Services.AddSingleton(); var app = builder.Build(); // Configure the HTTP request pipeline. if (!app.Environment.IsDevelopment()) { app.UseExceptionHandler(\"/Error\"); app.UseHsts(); } app.UseHttpsRedirection(); app.UseStaticFiles(); app.UseRouting(); app.MapBlazorHub(); app.MapFallbackToPage(\"/_Host\"); using (var scope = app.Services.CreateScope()) { var dataLoader = scope.ServiceProvider.GetRequiredService(); var kernelService = scope.ServiceProvider.GetRequiredService(); var kernel = kernelService.GetKernel(); var filePath = Path.Combine(app.Environment.WebRootPath, \"knowledge_base.txt\"); if (kernel is not null) { await dataLoader.LoadKnowledge(filePath, kernel); } } app.Run(); 6. Blazor UI\nModify Pages/Index.razor: @page \"/\" @using MyRAGApp.Services @inject KernelService KernelService @inject ILogger Logger @inject DataLoader DataLoader RAG with Semantic Kernel \"input-group mb-3\"\u003e \"text\" class=\"form-control\" placeholder=\"Ask a question\" @bind=\"UserQuestion\" /\u003e \"btn btn-primary\" @onclick=\"AskQuestion\"\u003eAsk @if (!string.IsNullOrEmpty(Response)) { \"alert alert-secondary\"\u003e @Response } @code { private string UserQuestion { get; set; } = string.Empty; private string Response { get; set; } = string.Empty; private async Task AskQuestion() { if (string.IsNullOrEmpty(UserQuestion)) return; var kernel = KernelService.GetKernel(); if (kernel is null) { Logger.LogError(\"Kernel was not properly initialized\"); Response = \"Error initializing the kernel.\"; return; } try { var result = await kernel.Memory.SearchAsync(\"ragcollection\", UserQuestion, limit:1, minRelevanceScore:0.7); var context = result.FirstOrDefault()?.Text; if(context != null) { var prompt = @$\"You are an AI assistant. Use the provided context to answer the question. If the context is not relevant for the answer, say you don't know. Context: {context} Question: {UserQuestion} \"; var promptTemplateFactory = new Microsoft.SemanticKernel.PromptTemplate.PromptTemplateFactory(); var promptTemplate = promptTemplateFactory.Create(prompt); var chatFunction = kernel.CreateSemanticFunction(promptTemplate); var answer = await kernel.RunAsync(new KernelArguments(), chatFunction); Response = answer.ToString(); } else { Response = \"No relevant knowledge found. Please ask a question related to the content of knowledge base file\"; } } catch (Exception ex) { Logger.LogError($\"Error processing question: {ex.Message}\"); Response = $\"An error occurred: {ex.Message}\"; } } } Explanation:\nDependency Injection: We inject KernelService and DataLoader to use them in the page. UI Elements: A text input (UserQuestion) for the user to type their question. A button to trigger the question processing. A div (Response) to display the result. AskQuestion Method: Gets the kernel from the KernelService. Uses Semantic Kernel’s memory to search for relevant data. Constructs an advanced prompt with retrieved context. Uses Semantic Kernel to run an LLM completion and get the response. Updates the Response variable, which triggers a UI update. How to Run\nMake sure that your Qdrant instance is running. Set your API keys in appsettings.json. Add your knowledge base content in the knowledge_base.txt file. Build and run your project (dotnet run). Navigate to the application in your browser (https://localhost:7085 by default). Type your question in the input and hit “Ask.” Important Considerations\nError Handling: Implement better error handling and logging throughout your application. Memory Store Selection: Explore other vector databases or memory stores (like Azure Cognitive Search). Qdrant is a good starting point. Prompt Engineering: Fine-tune your prompts to get the best results. Performance: Vector search and LLM completions can be slow. Optimize based on your application’s needs. Security: Be mindful of storing your API keys securely. Use environment variables or secrets management. Let me know if you would like to dive deeper into specific parts of this process, like more advanced prompt engineering or using a different memory store. I’m here to help you get your RAG application working!"},"title":"SemanticKernelRag"}}